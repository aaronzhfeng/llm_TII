# LLM Training Infrastructure & Analysis

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/CUDA-11.8+-76B900?logo=nvidia&logoColor=white" alt="CUDA">
  <img src="https://img.shields.io/badge/Qwen3-1.8B-FF6F00" alt="Qwen3">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

A **complete end-to-end infrastructure** for training Large Language Models from scratch. This repository provides everything needed to build, train, evaluate, and serve production-ready LLMs, with **Qwen3-1.8B** as our flagship model architecture.

---

## ğŸ¯ Pipeline Overview: Building an LLM from Scratch

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM TRAINING PIPELINE (End-to-End)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. PLANNING     â”‚     â”‚  2. DATA PREP    â”‚     â”‚  3. TRAINING     â”‚
  â”‚                  â”‚     â”‚                  â”‚     â”‚                  â”‚
  â”‚  â€¢ Scaling Laws  â”‚ â”€â”€â–¶ â”‚  â€¢ SlimPajama    â”‚ â”€â”€â–¶ â”‚  â€¢ Qwen3-1.8B    â”‚
  â”‚  â€¢ FLOPs Budget  â”‚     â”‚    627B tokens   â”‚     â”‚  â€¢ B200 GPUs     â”‚
  â”‚  â€¢ Model Size    â”‚     â”‚  â€¢ Tokenization  â”‚     â”‚  â€¢ MFU Tracking  â”‚
  â”‚  â€¢ Token Count   â”‚     â”‚  â€¢ Qwen3 Vocab   â”‚     â”‚  â€¢ ZeRO-1/FSDP   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                        â”‚                        â”‚
           â”‚   flops_parameter_     â”‚   enhanced_training_   â”‚   enhanced_training_
           â”‚   counting/            â”‚   system/data/         â”‚   system/train.py
           â”‚                        â”‚                        â”‚
           â–¼                        â–¼                        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                                              â”‚
  â”‚    ğŸ“Š Compute Budget    â†’    ğŸ“¦ Tokenized Data    â†’    ğŸ§  Trained Model     â”‚
  â”‚    C = 9.22 ZFLOPs           627B tokens              1.8B parameters        â”‚
  â”‚    D = 64B optimal           Qwen3 tokenizer          115B tokens trained    â”‚
  â”‚                              151,643 vocab                                   â”‚
  â”‚                                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                        â”‚                        â”‚
           â”‚                        â”‚                        â”‚
           â–¼                        â–¼                        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  4. EVALUATION   â”‚     â”‚  5. SERVING      â”‚     â”‚  6. ANALYSIS     â”‚
  â”‚                  â”‚     â”‚                  â”‚     â”‚                  â”‚
  â”‚  â€¢ ARC-E/C       â”‚ â”€â”€â–¶â”‚  â€¢ FastAPI       â”‚ â”€â”€â–¶ â”‚  â€¢ MFU Compute   â”‚
  â”‚  â€¢ OpenBookQA    â”‚     â”‚  â€¢ Chat UI       â”‚     â”‚  â€¢ Loss Curves   â”‚
  â”‚  â€¢ Log-prob      â”‚     â”‚  â€¢ REST API      â”‚     â”‚  â€¢ Scaling Fit   â”‚
  â”‚  â€¢ Generation    â”‚     â”‚  â€¢ Production    â”‚     â”‚                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                        â”‚                        â”‚
           â”‚   evaluation_system/   â”‚   serving_system/      â”‚   MFU_compute/
           â”‚                        â”‚                        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Repository Structure

```
llm_TII/
â”‚
â”œâ”€â”€ ğŸš€ enhanced_training_system/     # [CORE] Complete LLM training framework
â”‚   â”œâ”€â”€ train.py                     # Main training script
â”‚   â”œâ”€â”€ model_builder.py             # Modular model construction
â”‚   â”œâ”€â”€ model_components.py          # Architecture components (RoPE, SwiGLU, RMSNorm)
â”‚   â”œâ”€â”€ model_config.py              # Configuration system with presets
â”‚   â”œâ”€â”€ training_logger.py           # Detailed JSON logging
â”‚   â”œâ”€â”€ config/                      # Configuration files
â”‚   â”‚   â”œâ”€â”€ full_qwen3_1.8b_b200_optimal.py  # ğŸŒŸ Flagship config
â”‚   â”‚   â”œâ”€â”€ full_llama2_1.36b_b200_optimal.py
â”‚   â”‚   â””â”€â”€ archived/                # GPT-2, LLaMA variants
â”‚   â”œâ”€â”€ data/                        # Dataset preparation
â”‚   â”‚   â”œâ”€â”€ slimpajama_627b_qwen3/   # ğŸŒŸ Production dataset (627B tokens)
â”‚   â”‚   â”œâ”€â”€ slimpajama_6b_qwen3/     # Quick testing subset
â”‚   â”‚   â””â”€â”€ shakespeare/             # Debugging dataset
â”‚   â”œâ”€â”€ docs/                        # Detailed documentation (50+ docs)
â”‚   â””â”€â”€ plots/                       # Training visualization
â”‚
â”œâ”€â”€ ğŸ“Š flops_parameter_counting/     # [ANALYSIS] FLOPs, Parameters & Scaling Laws
â”‚   â”œâ”€â”€ detailed_cost_analysis.py    # ğŸŒŸ Main analysis tool
â”‚   â”‚   â”œâ”€â”€ Forward analysis         # Model â†’ FLOPs/params
â”‚   â”‚   â””â”€â”€ Backward scaling         # Compute budget â†’ Optimal (N, D)
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ models/                  # LLaMA, DeepSeek V3 MoE configs
â”‚   â”‚   â””â”€â”€ scaling_laws/            # Chinchilla, Kaplan parameters
â”‚   â””â”€â”€ docs/                        # Academic formulas & references
â”‚
â”œâ”€â”€ âš¡ MFU_compute/                   # MFU calculation tools
â”‚   â”œâ”€â”€ mfu_analysis.py              # Detailed MFU analysis
â”‚   â”œâ”€â”€ simple_mfu_analysis.py       # Quick MFU estimation
â”‚   â””â”€â”€ *_config.json                # Hardware configurations (B200/H200/H100/A100)
â”‚
â”œâ”€â”€ ğŸ§ª evaluation_system/            # Model evaluation
â”‚   â”œâ”€â”€ eval_benchmarks.py           # Benchmark runner (ARC, OpenBookQA)
â”‚   â”œâ”€â”€ eval_qwen3_official.py       # Official Qwen3 comparison
â”‚   â””â”€â”€ plot_comparison.py           # Results visualization
â”‚
â”œâ”€â”€ ğŸŒ serving_system/               # Production deployment
â”‚   â”œâ”€â”€ serve_qwen3.py               # FastAPI server with Chat UI
â”‚   â”œâ”€â”€ static/index.html            # Modern chat interface
â”‚   â””â”€â”€ deploy/                      # Docker, Nginx configs
â”‚
â”œâ”€â”€ ğŸ”¬ system_implementation/        # [ARCHIVE] Early-stage experiments
â”‚   â”œâ”€â”€ nanoGPT/                     # Base reference implementation
â”‚   â”œâ”€â”€ phase1_zero1/                # ZeRO-1 prototypes
â”‚   â”œâ”€â”€ phase2_triton/               # Triton kernel experiments
â”‚   â””â”€â”€ phase3_fsdp/                 # FSDP prototypes
â”‚
â””â”€â”€ ğŸ“š legacy/                       # Deprecated implementations
```

---

## ğŸŒŸ Flagship: Qwen3-1.8B Architecture

Our production model uses the **Qwen3 architecture** with modern optimizations:

| Component | Choice | Benefit |
|-----------|--------|---------|
| **Normalization** | RMSNorm | 25% faster than LayerNorm |
| **Position Encoding** | RoPE (Î¸=1M) | Better length extrapolation |
| **FFN** | SwiGLU (8/3x) | Better quality per FLOP |
| **Attention** | GQA (16 heads, 8 KV) | 50% KV cache reduction |
| **Activation** | SiLU | Smoother gradients |
| **Vocabulary** | 151,643 (BBPE) | Multilingual support |

### Model Specifications

```python
# Qwen3-1.8B Configuration
n_layer = 24           # Transformer layers
n_head = 16            # Query heads
n_embd = 2048          # Hidden dimension
d_ff = 6144            # FFN intermediate (SwiGLU)
num_key_value_heads = 8  # GQA key-value heads
block_size = 2048      # Context length
vocab_size = 151643    # Qwen3 vocabulary
```

**Total Parameters**: ~1.8B

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
cd llm_TII/enhanced_training_system
pip install -r requirements.txt
```

### 2. Data Preparation (SlimPajama-627B)

```bash
cd data/slimpajama_627b_qwen3

# Download and tokenize (high-scale workflow)
python build_manifest.py --output manifests/slimpajama_manifest.jsonl
python tokenize_from_manifest.py \
  --manifest manifests/slimpajama_manifest.jsonl \
  --split train \
  --tokenizer ../../qwen3_tokenizer \
  --output-dir tokenized \
  --spawn-workers -1  # Use all CPU cores
```

### 3. Training (8Ã— B200 GPUs)

```bash
# Production training
torchrun --standalone --nproc_per_node=8 train.py \
  config/full_qwen3_1.8b_b200_optimal.py

# Quick test (Shakespeare)
python train.py config/preset_quick_test.py --max_iters=100
```

### 4. Evaluation

```bash
cd ../evaluation_system
python eval_benchmarks.py \
  --checkpoint /path/to/ckpt_160000.pt \
  --tokenizer ../enhanced_training_system/qwen3_tokenizer \
  --mode logprob
```

### 5. Serving

```bash
cd ../serving_system
uvicorn serve_qwen3:app --host 0.0.0.0 --port 8000
# Open http://localhost:8000 for Chat UI
```

---

## ğŸ“Š Compute Planning with Scaling Laws

The `flops_parameter_counting/` module provides **detailed academic formulas** (not simplified 6ND) for:

### Forward Analysis: Model â†’ FLOPs

```bash
python detailed_cost_analysis.py --model_config configs/models/llama_7b_config.json
```

Output includes:
- **FLOPs per token** (forward & training)
- **Component breakdown** (Attention vs FFN)
- **Memory requirements** (weights, gradients, optimizer, activations)

### Backward Scaling: Compute Budget â†’ Optimal (N, D)

```bash
python detailed_cost_analysis.py --backward_config configs/scaling_laws/backward_scaling_config.jsonc
```

Solves for **optimal training tokens D** given:
- GPU setup (8Ã— H100, 8Ã— B200, etc.)
- Training time (hours/days)
- Expected MFU
- Dataset constraints

```
================================================================================
BACKWARD SCALING LAW: Training Setup â†’ Optimal (N, D)
================================================================================

Step 1: Calculate N from architecture
  Model parameters (N): 6.74B

Step 2: Calculate available compute (C)
  GPU setup: 8Ã— H100 @ 989 TFLOPS
  Training time: 720 hours (30 days)
  Compute budget (C): 9.22e+21 FLOPs

Step 3: Calculate FLOPs per token (detailed formula)
  Training FLOPs/token: 144.00 GFLOPs

Step 4: Solve for D
  D_optimal: 64.03B tokens

Step 5: Predicted loss (Chinchilla)
  L(6.74B, 64.03B) = 2.1590
================================================================================
```

---

## âš¡ MFU (Model FLOPs Utilization)

Architecture-aware MFU calculation:

```python
# Forward pass FLOPs per layer:
attention_flops = 8*HÂ² + 2*a*SÂ²*H    # QKV + scores + output
ffn_flops = 6*H*D_ff                 # SwiGLU: 3 projections

# Training FLOPs = 3Ã— Forward (1 forward + 2 backward)
# MFU = Achieved FLOPs / Hardware Peak FLOPs
```

### Supported Hardware

| GPU | BF16 Peak | Memory | Typical MFU |
|-----|-----------|--------|-------------|
| **B200** | 4,500 TF | 192 GB | 45-55% |
| H200 | 1,979 TF | 141 GB | 40-50% |
| H100 | 989 TF | 80 GB | 40-50% |
| A100 | 312 TF | 40/80 GB | 35-45% |

---

## ğŸ¨ Modular Architecture System

Mix and match components without code changes:

```bash
# Qwen3-style (flagship)
python train.py config/full_qwen3_1.8b_b200_optimal.py

# LLaMA-style
python train.py config/full_llama2_1.36b_b200_optimal.py

# Custom mix
python train.py config/full_custom.py \
  --normalization=rmsnorm \
  --position_encoding=rope \
  --ffn_type=swiglu \
  --attention_backend=flash_attn_2
```

### Component Options

| Component | Options | Default (Qwen3) |
|-----------|---------|-----------------|
| **Normalization** | LayerNorm, RMSNorm | RMSNorm |
| **Position** | Learned, RoPE | RoPE (Î¸=1M) |
| **FFN** | Standard (4x), SwiGLU (8/3x) | SwiGLU |
| **Attention** | MHA, GQA | GQA |
| **Backend** | SDPA, FlashAttention-2 | FA2 |

---

## ğŸ“ˆ Training Output

```
================================================================================
ğŸš€ TRAINING INITIALIZATION
================================================================================

ğŸ“Š MODEL ARCHITECTURE:
  Architecture:          Qwen3-1.8B
  Total parameters:      1,831,845,888 (1.83B)
  Layers:                24
  Hidden size:           2048
  Attention heads:       16 (8 KV heads)
  FFN size:              6144 (SwiGLU)
  Sequence length:       2048

âš™ï¸  TRAINING CONFIGURATION:
  Dataset:               SlimPajama-627B (Qwen3 tokenizer)
  Batch size (micro):    64
  Gradient accum:        4 (global: 32)
  Tokens per iteration:  4,194,304
  Total iterations:      25,000
  Total tokens:          ~105B

ğŸ–¥ï¸  HARDWARE:
  Device:                8Ã— NVIDIA B200
  Peak FLOPs:            36,000 TFLOPS
  Precision:             bfloat16
  Parallelism:           DDP + ZeRO-1

ğŸ“ˆ PERFORMANCE:
  Expected MFU:          45-50%
  Expected tokens/s:     ~160,000
================================================================================
```

---

## ğŸ§ª Evaluation Results

| Benchmark | Random | Qwen3-1.8B (ours) | Notes |
|-----------|--------|-------------------|-------|
| **ARC-Easy** | 25% | 45-50% | Grade-school science |
| **ARC-Challenge** | 25% | 28-32% | Harder reasoning |
| **OpenBookQA** | 25% | 30-35% | World knowledge |

*Base model results. Instruction-tuning typically adds +10-20%.*

---

## ğŸ“š Documentation

### Core Guides
- `enhanced_training_system/README.md` - Full training guide
- `enhanced_training_system/docs/` - 50+ detailed docs
- `flops_parameter_counting/README.md` - Scaling law analysis
- `flops_parameter_counting/docs/ACADEMIC_FORMULAS_README.md` - FLOPs formulas

### Quick References
- `evaluation_system/README.md` - Benchmark evaluation
- `serving_system/README.md` - Deployment guide
- `MFU_compute/README.md` - MFU calculation

---

## ğŸ”¬ Historical: system_implementation/

The `system_implementation/` folder contains **early-stage prototypes** that were later refined into the production `enhanced_training_system`:

- `phase1_zero1/` â†’ ZeRO-1 optimizer sharding (now in train.py)
- `phase2_triton/` â†’ Triton kernel experiments (replaced by FlashAttention-2)
- `phase3_fsdp/` â†’ FSDP prototypes (now optional in train.py)

These are preserved for reference but not actively maintained.

---

## ğŸ“š References

1. [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathy
2. [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671) - Alibaba
3. [Chinchilla Scaling Laws](https://arxiv.org/abs/2203.15556) - Hoffmann et al.
4. [LLaMA Paper](https://arxiv.org/abs/2302.13971) - Touvron et al.
5. [SlimPajama Dataset](https://huggingface.co/datasets/cerebras/SlimPajama-627B) - Cerebras
6. [FlashAttention-2](https://arxiv.org/abs/2307.08691) - Dao et al.

---

## ğŸ“ License

MIT License (same as nanoGPT)

## ğŸ™ Acknowledgments

- Andrej Karpathy for [nanoGPT](https://github.com/karpathy/nanoGPT)
- Alibaba for the Qwen architecture and tokenizer
- Cerebras for SlimPajama-627B dataset
- PyTorch team for distributed training infrastructure

---

*Complete infrastructure for building, training, evaluating, and serving production LLMs from scratch.*
