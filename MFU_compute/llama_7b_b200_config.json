{
  "_comment": "MFU configuration for LLaMA 7B on B200 hardware",
  "model_name": "llama_7b",
  "hardware": "B200",
  "gpus": 8,
  "precision": "FP8",

  "model_config": {
    "hidden_size": 4096,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "num_key_value_heads": 32,
    "vocab_size": 32000,
    "tie_word_embeddings": false,
    "max_position_embeddings": 2048
  },

  "hardware_specs": {
    "peak_tflops_fp8": 2000,
    "peak_tflops_fp16": 1000,
    "peak_tflops_bf16": 1000,
    "peak_tflops_fp32": 500,
    "peak_tflops_int8": 4000,
    "memory_gb": 192,
    "memory_bandwidth_gb_s": 8000,
    "interconnect_bandwidth_gb_s": 1600
  },

  "training_config": {
    "batch_size": 64,
    "sequence_length": 2048,
    "gradient_accumulation_steps": 1,
    "mixed_precision": true,
    "optimizer": "AdamW",
    "learning_rate": 3e-4
  },

  "performance_measurements": {
    "_comment": "These should be measured during actual training",
    "tokens_per_second_achieved": 0,
    "throughput_tokens_per_sec_per_gpu": 0,
    "wall_clock_time_per_iteration_ms": 0,
    "peak_memory_used_gb": 0,
    "communication_overhead_percent": 0
  },

  "mfu_targets": {
    "target_mfu_percent": 70,
    "realistic_mfu_percent": 55,
    "minimum_acceptable_mfu_percent": 40
  },

  "references": {
    "paper": "NVIDIA B200 GPU Specifications",
    "authors": "NVIDIA",
    "year": 2024,
    "url": "https://www.nvidia.com/en-us/data-center/b200/"
  }
}
