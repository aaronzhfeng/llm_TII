{
  "_comment": "MFU configuration for LLaMA 13B on V100 hardware",
  "model_name": "llama_13b",
  "hardware": "V100",
  "gpus": 16,
  "precision": "FP16",

  "model_config": {
    "hidden_size": 5120,
    "intermediate_size": 13824,
    "num_hidden_layers": 40,
    "num_attention_heads": 40,
    "num_key_value_heads": 40,
    "vocab_size": 32000,
    "tie_word_embeddings": false,
    "max_position_embeddings": 2048
  },

  "hardware_specs": {
    "peak_tflops_fp16": 125,
    "peak_tflops_fp32": 62.5,
    "peak_tflops_int8": 250,
    "memory_gb": 32,
    "memory_bandwidth_gb_s": 900,
    "interconnect_bandwidth_gb_s": 300
  },

  "training_config": {
    "batch_size": 16,
    "sequence_length": 2048,
    "gradient_accumulation_steps": 2,
    "mixed_precision": true,
    "optimizer": "AdamW",
    "learning_rate": 2e-4
  },

  "performance_measurements": {
    "_comment": "These should be measured during actual training",
    "tokens_per_second_achieved": 0,
    "throughput_tokens_per_sec_per_gpu": 0,
    "wall_clock_time_per_iteration_ms": 0,
    "peak_memory_used_gb": 0,
    "communication_overhead_percent": 0
  },

  "mfu_targets": {
    "target_mfu_percent": 35,
    "realistic_mfu_percent": 25,
    "minimum_acceptable_mfu_percent": 15
  },

  "references": {
    "paper": "LLaMA: Open and Efficient Foundation Language Models",
    "authors": "Touvron et al.",
    "year": 2023,
    "url": "https://arxiv.org/abs/2302.13971"
  }
}
