{
  "run_name": "run_20251112_122613",
  "start_time": "2025-11-12T12:26:13.104141",
  "config": {
    "MODULAR_ARCH_AVAILABLE": true,
    "LOGGER_AVAILABLE": true,
    "out_dir": "out-llama3-2.2b-chinchilla",
    "eval_interval": 1000,
    "log_interval": 10,
    "eval_iters": 50,
    "eval_only": false,
    "eval_at_start": false,
    "always_save_checkpoint": true,
    "init_from": "scratch",
    "save_log_to_json": true,
    "log_save_interval": 10,
    "gradient_log_interval": 10,
    "wandb_log": false,
    "wandb_project": "llama3-2.2b-chinchilla",
    "wandb_run_name": "run-1",
    "dataset": "slimpajama_6b_llama3",
    "gradient_accumulation_steps": 64,
    "batch_size": 2,
    "block_size": 2048,
    "n_layer": 30,
    "n_head": 16,
    "n_embd": 2048,
    "dropout": 0.0,
    "bias": false,
    "arch_preset": "llama3",
    "normalization": "layernorm_nobias",
    "activation": "gelu",
    "attention_backend": "flash_attn_2",
    "position_encoding": "learned_absolute",
    "norm_position": "post",
    "ffn_type": "standard",
    "weight_tying": true,
    "rope_theta": 500000.0,
    "d_ff": 7168,
    "intermediate_size": 7168,
    "learning_rate": 0.0003,
    "max_iters": 2000,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.95,
    "grad_clip": 1.0,
    "decay_lr": true,
    "warmup_iters": 2000,
    "lr_decay_iters": 30000,
    "min_lr": 3e-05,
    "backend": "nccl",
    "use_zero1": true,
    "use_fsdp": false,
    "fsdp_min_num_params": 1000000.0,
    "fsdp_activation_checkpointing": false,
    "device": "cuda",
    "dtype": "bfloat16",
    "compile": false
  },
  "startup_info": {
    "timestamp": "2025-11-12T12:26:13.106682",
    "model": {
      "total_params": 2223106048,
      "trainable_params": 2223106048,
      "non_embedding_params": 2223106048
    },
    "optimizer": {
      "type": "ZeroRedundancyOptimizer",
      "param_groups": 2
    },
    "config": {
      "MODULAR_ARCH_AVAILABLE": true,
      "LOGGER_AVAILABLE": true,
      "out_dir": "out-llama3-2.2b-chinchilla",
      "eval_interval": 1000,
      "log_interval": 10,
      "eval_iters": 50,
      "eval_only": false,
      "eval_at_start": false,
      "always_save_checkpoint": true,
      "init_from": "scratch",
      "save_log_to_json": true,
      "log_save_interval": 10,
      "gradient_log_interval": 10,
      "wandb_log": false,
      "wandb_project": "llama3-2.2b-chinchilla",
      "wandb_run_name": "run-1",
      "dataset": "slimpajama_6b_llama3",
      "gradient_accumulation_steps": 64,
      "batch_size": 2,
      "block_size": 2048,
      "n_layer": 30,
      "n_head": 16,
      "n_embd": 2048,
      "dropout": 0.0,
      "bias": false,
      "arch_preset": "llama3",
      "normalization": "layernorm_nobias",
      "activation": "gelu",
      "attention_backend": "flash_attn_2",
      "position_encoding": "learned_absolute",
      "norm_position": "post",
      "ffn_type": "standard",
      "weight_tying": true,
      "rope_theta": 500000.0,
      "d_ff": 7168,
      "intermediate_size": 7168,
      "learning_rate": 0.0003,
      "max_iters": 2000,
      "weight_decay": 0.1,
      "beta1": 0.9,
      "beta2": 0.95,
      "grad_clip": 1.0,
      "decay_lr": true,
      "warmup_iters": 2000,
      "lr_decay_iters": 30000,
      "min_lr": 3e-05,
      "backend": "nccl",
      "use_zero1": true,
      "use_fsdp": false,
      "fsdp_min_num_params": 1000000.0,
      "fsdp_activation_checkpointing": false,
      "device": "cuda",
      "dtype": "bfloat16",
      "compile": false
    },
    "hardware": {
      "gpu_name": "NVIDIA RTX A6000",
      "num_gpus": 2,
      "gpu_memory_gb": 51.033931776,
      "precision": "bfloat16",
      "parallelism": "DDP+ZeRO-1"
    }
  },
  "training_iterations": [
    {
      "iter": 0,
      "loss": 12.518844604492188,
      "time_ms": 25073.543071746826,
      "mfu": -100.0
    },
    {
      "iter": 10,
      "loss": 9.017982482910156,
      "time_ms": 25312.61920928955,
      "mfu": {
        "mfu": 0.2480254448796397,
        "mfu_percent": 24.80254448796397,
        "flops_achieved": 76887887912688.31,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5178.128700008157,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.88788791268831,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 20,
      "loss": 6.0272369384765625,
      "time_ms": 25342.562913894653,
      "mfu": {
        "mfu": 0.2477323884637887,
        "mfu_percent": 24.77323884637887,
        "flops_achieved": 76797040423774.5,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5172.01044130137,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7970404237745,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 30,
      "loss": 5.194859027862549,
      "time_ms": 25352.30302810669,
      "mfu": {
        "mfu": 0.24763721203129724,
        "mfu_percent": 24.763721203129723,
        "flops_achieved": 76767535729702.14,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5170.023403975873,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.76753572970215,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 40,
      "loss": 5.052191257476807,
      "time_ms": 25351.26304626465,
      "mfu": {
        "mfu": 0.24764737082312768,
        "mfu_percent": 24.764737082312767,
        "flops_achieved": 76770684955169.58,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5170.235493229701,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.77068495516957,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 50,
      "loss": 5.1065568923950195,
      "time_ms": 25336.462020874023,
      "mfu": {
        "mfu": 0.24779204118083015,
        "mfu_percent": 24.779204118083015,
        "flops_achieved": 76815532766057.34,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5173.255835483792,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.81553276605734,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 60,
      "loss": 4.853307247161865,
      "time_ms": 25353.766918182373,
      "mfu": {
        "mfu": 0.24762291381445897,
        "mfu_percent": 24.7622913814459,
        "flops_achieved": 76763103282482.28,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5169.724894252386,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.76310328248228,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 70,
      "loss": 5.035944938659668,
      "time_ms": 25366.222143173218,
      "mfu": {
        "mfu": 0.24750132696218588,
        "mfu_percent": 24.75013269621859,
        "flops_achieved": 76725411358277.62,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.186475786472,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.72541135827763,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 80,
      "loss": 4.834681034088135,
      "time_ms": 25361.165285110474,
      "mfu": {
        "mfu": 0.24755067718196952,
        "mfu_percent": 24.75506771819695,
        "flops_achieved": 76740709926410.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5168.216780518059,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.74070992641055,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 90,
      "loss": 4.881685256958008,
      "time_ms": 25365.620851516724,
      "mfu": {
        "mfu": 0.24750719397737728,
        "mfu_percent": 24.75071939773773,
        "flops_achieved": 76727230132986.95,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.308963863292,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.72723013298695,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 100,
      "loss": 5.0616455078125,
      "time_ms": 25371.95348739624,
      "mfu": {
        "mfu": 0.24744541816899113,
        "mfu_percent": 24.744541816899112,
        "flops_achieved": 76708079632387.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5166.019245034138,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.70807963238725,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 110,
      "loss": 5.006844520568848,
      "time_ms": 25338.963747024536,
      "mfu": {
        "mfu": 0.24776757657227313,
        "mfu_percent": 24.776757657227314,
        "flops_achieved": 76807948737404.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5172.745077840498,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.80794873740467,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 120,
      "loss": 4.694549083709717,
      "time_ms": 25336.07006072998,
      "mfu": {
        "mfu": 0.24779587463266073,
        "mfu_percent": 24.779587463266072,
        "flops_achieved": 76816721136124.83,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5173.335868026234,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.81672113612483,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 130,
      "loss": 4.742898464202881,
      "time_ms": 25378.64661216736,
      "mfu": {
        "mfu": 0.2473801592494295,
        "mfu_percent": 24.73801592494295,
        "flops_achieved": 76687849367323.14,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.656807867752,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.68784936732314,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 140,
      "loss": 4.659245491027832,
      "time_ms": 25377.407550811768,
      "mfu": {
        "mfu": 0.24739223767764762,
        "mfu_percent": 24.739223767764763,
        "flops_achieved": 76691593680070.77,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.908974155924,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.69159368007077,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 150,
      "loss": 4.4369306564331055,
      "time_ms": 25336.066722869873,
      "mfu": {
        "mfu": 0.247795907278137,
        "mfu_percent": 24.779590727813698,
        "flops_achieved": 76816731256222.47,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5173.3365495791995,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.81673125622247,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 160,
      "loss": 4.408073902130127,
      "time_ms": 25350.741386413574,
      "mfu": {
        "mfu": 0.24765246683545383,
        "mfu_percent": 24.765246683545385,
        "flops_achieved": 76772264718990.69,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5170.341884764225,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.77226471899068,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 170,
      "loss": 4.489499092102051,
      "time_ms": 25365.31400680542,
      "mfu": {
        "mfu": 0.2475101880768574,
        "mfu_percent": 24.75101880768574,
        "flops_achieved": 76728158303825.8,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.371472903267,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7281583038258,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 180,
      "loss": 4.690978527069092,
      "time_ms": 25360.6698513031,
      "mfu": {
        "mfu": 0.2475555132125331,
        "mfu_percent": 24.75555132125331,
        "flops_achieved": 76742209095885.27,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5168.317744306945,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.74220909588527,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 190,
      "loss": 4.445441246032715,
      "time_ms": 25361.367225646973,
      "mfu": {
        "mfu": 0.24754870605335821,
        "mfu_percent": 24.75487060533582,
        "flops_achieved": 76740098876541.05,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5168.175628459492,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.74009887654104,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 200,
      "loss": 4.128906726837158,
      "time_ms": 25368.59440803528,
      "mfu": {
        "mfu": 0.24747818264871618,
        "mfu_percent": 24.74781826487162,
        "flops_achieved": 76718236621102.02,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5166.703282483956,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.71823662110202,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 210,
      "loss": 4.10780143737793,
      "time_ms": 25379.86660003662,
      "mfu": {
        "mfu": 0.24736826790271216,
        "mfu_percent": 24.736826790271216,
        "flops_achieved": 76684163049840.77,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.408547356623,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.68416304984076,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 220,
      "loss": 4.130568027496338,
      "time_ms": 25354.875087738037,
      "mfu": {
        "mfu": 0.24761209111573043,
        "mfu_percent": 24.76120911157304,
        "flops_achieved": 76759748245876.44,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5169.498944342589,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.75974824587644,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 230,
      "loss": 4.122347354888916,
      "time_ms": 25334.34271812439,
      "mfu": {
        "mfu": 0.24781276981626602,
        "mfu_percent": 24.7812769816266,
        "flops_achieved": 76821958643042.47,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5173.688595687547,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.82195864304246,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 240,
      "loss": 3.968841791152954,
      "time_ms": 25361.958980560303,
      "mfu": {
        "mfu": 0.24754293015240322,
        "mfu_percent": 24.75429301524032,
        "flops_achieved": 76738308347245.0,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5168.055042611867,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.738308347245,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 250,
      "loss": 3.976512908935547,
      "time_ms": 25372.70498275757,
      "mfu": {
        "mfu": 0.2474380892663746,
        "mfu_percent": 24.74380892663746,
        "flops_achieved": 76705807672576.12,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.8662365353675,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.70580767257613,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 260,
      "loss": 3.4198617935180664,
      "time_ms": 25365.04292488098,
      "mfu": {
        "mfu": 0.24751283327396195,
        "mfu_percent": 24.751283327396195,
        "flops_achieved": 76728978314928.2,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.426697765583,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7289783149282,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 270,
      "loss": 3.8814704418182373,
      "time_ms": 25345.669507980347,
      "mfu": {
        "mfu": 0.24770202414563194,
        "mfu_percent": 24.770202414563194,
        "flops_achieved": 76787627485145.9,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5171.376513006714,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7876274851459,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 280,
      "loss": 3.8185205459594727,
      "time_ms": 25376.635551452637,
      "mfu": {
        "mfu": 0.24739976375999823,
        "mfu_percent": 24.739976375999824,
        "flops_achieved": 76693926765599.45,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.066099256685,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.69392676559946,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 290,
      "loss": 3.7920403480529785,
      "time_ms": 25364.398956298828,
      "mfu": {
        "mfu": 0.2475191172978248,
        "mfu_percent": 24.75191172978248,
        "flops_achieved": 76730926362325.69,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.5578919030695,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.73092636232569,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 300,
      "loss": 3.466641426086426,
      "time_ms": 25370.075464248657,
      "mfu": {
        "mfu": 0.24746373534836805,
        "mfu_percent": 24.746373534836806,
        "flops_achieved": 76713757957994.1,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5166.401660283029,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.71375795799409,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 310,
      "loss": 3.5158421993255615,
      "time_ms": 25768.310546875,
      "mfu": {
        "mfu": 0.24363931927288604,
        "mfu_percent": 24.363931927288604,
        "flops_achieved": 75528188974594.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5086.557760997471,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.52818897459467,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 320,
      "loss": 3.5609943866729736,
      "time_ms": 25541.151523590088,
      "mfu": {
        "mfu": 0.24580620942851245,
        "mfu_percent": 24.580620942851244,
        "flops_achieved": 76199924922838.86,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5131.796813426382,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.19992492283886,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 330,
      "loss": 3.616806745529175,
      "time_ms": 25727.832317352295,
      "mfu": {
        "mfu": 0.2440226429887994,
        "mfu_percent": 24.40226429887994,
        "flops_achieved": 75647019326527.81,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5094.560567063308,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.64701932652781,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 340,
      "loss": 3.1223933696746826,
      "time_ms": 25426.872491836548,
      "mfu": {
        "mfu": 0.2469109656513439,
        "mfu_percent": 24.69109656513439,
        "flops_achieved": 76542399351916.61,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5154.861261135496,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.54239935191661,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 350,
      "loss": 3.5398244857788086,
      "time_ms": 25428.577423095703,
      "mfu": {
        "mfu": 0.2468944108037579,
        "mfu_percent": 24.68944108037579,
        "flops_achieved": 76537267349164.95,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5154.515638808518,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.53726734916495,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 360,
      "loss": 2.928910970687866,
      "time_ms": 25406.949758529663,
      "mfu": {
        "mfu": 0.2471045796571954,
        "mfu_percent": 24.71045796571954,
        "flops_achieved": 76602419693730.58,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.903419958797,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.60241969373058,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 370,
      "loss": 3.4443721771240234,
      "time_ms": 25517.952919006348,
      "mfu": {
        "mfu": 0.2460296741035536,
        "mfu_percent": 24.602967410355358,
        "flops_achieved": 76269198972101.61,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5136.4621768846755,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.2691989721016,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 380,
      "loss": 3.4963481426239014,
      "time_ms": 25401.73602104187,
      "mfu": {
        "mfu": 0.2471552981753824,
        "mfu_percent": 24.71552981753824,
        "flops_achieved": 76618142434368.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5159.962291216031,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.61814243436855,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 390,
      "loss": 3.2666261196136475,
      "time_ms": 25396.833181381226,
      "mfu": {
        "mfu": 0.24720301132093792,
        "mfu_percent": 24.72030113209379,
        "flops_achieved": 76632933509490.75,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.958418079098,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63293350949075,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 400,
      "loss": 3.2594070434570312,
      "time_ms": 25384.925603866577,
      "mfu": {
        "mfu": 0.2473189694712628,
        "mfu_percent": 24.73189694712628,
        "flops_achieved": 76668880536091.47,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.379323831282,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.66888053609146,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 410,
      "loss": 3.518632173538208,
      "time_ms": 25406.61358833313,
      "mfu": {
        "mfu": 0.24710784924662016,
        "mfu_percent": 24.710784924662015,
        "flops_achieved": 76603433266452.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.971680515071,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.60343326645226,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 420,
      "loss": 3.440581798553467,
      "time_ms": 25406.728506088257,
      "mfu": {
        "mfu": 0.24710673154745233,
        "mfu_percent": 24.710673154745233,
        "flops_achieved": 76603086779710.22,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.948345852202,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.60308677971022,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 430,
      "loss": 3.2610771656036377,
      "time_ms": 25407.299041748047,
      "mfu": {
        "mfu": 0.2471011826222442,
        "mfu_percent": 24.71011826222442,
        "flops_achieved": 76601366612895.7,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.832498670119,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6013666128957,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 440,
      "loss": 3.5750327110290527,
      "time_ms": 25766.92223548889,
      "mfu": {
        "mfu": 0.24365244646122303,
        "mfu_percent": 24.365244646122303,
        "flops_achieved": 75532258402979.14,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5086.8318226797755,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.53225840297914,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 450,
      "loss": 3.7969768047332764,
      "time_ms": 25383.53943824768,
      "mfu": {
        "mfu": 0.24733247527305277,
        "mfu_percent": 24.733247527305277,
        "flops_achieved": 76673067334646.36,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.661289981567,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.67306733464636,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 460,
      "loss": 3.0240890979766846,
      "time_ms": 25367.196321487427,
      "mfu": {
        "mfu": 0.24749182215044366,
        "mfu_percent": 24.749182215044367,
        "flops_achieved": 76722464866637.53,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5166.988039942543,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.72246486663754,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 470,
      "loss": 3.365525484085083,
      "time_ms": 25774.307250976562,
      "mfu": {
        "mfu": 0.24358263364052515,
        "mfu_percent": 24.358263364052515,
        "flops_achieved": 75510616428562.8,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5085.374311855998,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.5106164285628,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 480,
      "loss": 3.3413476943969727,
      "time_ms": 25429.532766342163,
      "mfu": {
        "mfu": 0.24688513541084697,
        "mfu_percent": 24.688513541084696,
        "flops_achieved": 76534391977362.56,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5154.321992635402,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.53439197736256,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 490,
      "loss": 3.1346960067749023,
      "time_ms": 25392.86184310913,
      "mfu": {
        "mfu": 0.24724167284660217,
        "mfu_percent": 24.72416728466022,
        "flops_achieved": 76644918582446.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.765570569945,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64491858244668,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 500,
      "loss": 3.1805412769317627,
      "time_ms": 25387.994527816772,
      "mfu": {
        "mfu": 0.24728907332850458,
        "mfu_percent": 24.728907332850458,
        "flops_achieved": 76659612731836.42,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.75516982599,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65961273183642,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 510,
      "loss": 3.0656681060791016,
      "time_ms": 25392.947912216187,
      "mfu": {
        "mfu": 0.24724083482377465,
        "mfu_percent": 24.724083482377466,
        "flops_achieved": 76644658795370.14,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.7480748244725,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64465879537013,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 520,
      "loss": 3.059246778488159,
      "time_ms": 25401.421785354614,
      "mfu": {
        "mfu": 0.24715835568199118,
        "mfu_percent": 24.715835568199118,
        "flops_achieved": 76619090261417.27,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.026124032575,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.61909026141727,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 530,
      "loss": 3.1173861026763916,
      "time_ms": 25389.222383499146,
      "mfu": {
        "mfu": 0.24727711410859274,
        "mfu_percent": 24.727711410859275,
        "flops_achieved": 76655905373663.75,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.505492298407,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65590537366376,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 540,
      "loss": 2.9208693504333496,
      "time_ms": 25388.99564743042,
      "mfu": {
        "mfu": 0.24727932241338418,
        "mfu_percent": 24.72793224133842,
        "flops_achieved": 76656589948149.1,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.551595981135,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65658994814909,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 550,
      "loss": 3.0704541206359863,
      "time_ms": 25379.55617904663,
      "mfu": {
        "mfu": 0.24737129349945913,
        "mfu_percent": 24.737129349945914,
        "flops_achieved": 76685100984832.33,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.471713977925,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.68510098483233,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 560,
      "loss": 3.301919460296631,
      "time_ms": 25394.257307052612,
      "mfu": {
        "mfu": 0.24722808643469762,
        "mfu_percent": 24.72280864346976,
        "flops_achieved": 76640706794756.27,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.481921489315,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64070679475627,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 570,
      "loss": 2.8631134033203125,
      "time_ms": 25885.7364654541,
      "mfu": {
        "mfu": 0.242534093972235,
        "mfu_percent": 24.2534093972235,
        "flops_achieved": 75185569131392.84,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5063.483520158778,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.18556913139284,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 580,
      "loss": 3.170853614807129,
      "time_ms": 26080.958127975464,
      "mfu": {
        "mfu": 0.24071867335727742,
        "mfu_percent": 24.07186733572774,
        "flops_achieved": 74622788740756.0,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5025.582241145006,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 74.622788740756,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 590,
      "loss": 2.96789813041687,
      "time_ms": 25382.389307022095,
      "mfu": {
        "mfu": 0.24734368244505983,
        "mfu_percent": 24.734368244505983,
        "flops_achieved": 76676541557968.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.895266697317,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.67654155796855,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 600,
      "loss": 3.0986673831939697,
      "time_ms": 25419.431686401367,
      "mfu": {
        "mfu": 0.24698324171470715,
        "mfu_percent": 24.698324171470716,
        "flops_achieved": 76564804931559.22,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5156.37019808431,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.56480493155922,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 610,
      "loss": 2.938724994659424,
      "time_ms": 25423.91037940979,
      "mfu": {
        "mfu": 0.24693973298212596,
        "mfu_percent": 24.693973298212597,
        "flops_achieved": 76551317224459.05,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5155.461848471274,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.55131722445904,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 620,
      "loss": 3.125304698944092,
      "time_ms": 25401.13377571106,
      "mfu": {
        "mfu": 0.2471611580761894,
        "mfu_percent": 24.71611580761894,
        "flops_achieved": 76619959003618.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.084630762938,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.61995900361872,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 630,
      "loss": 2.9769744873046875,
      "time_ms": 25390.74659347534,
      "mfu": {
        "mfu": 0.24726227002975393,
        "mfu_percent": 24.726227002975392,
        "flops_achieved": 76651303709223.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.195586390578,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65130370922373,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 640,
      "loss": 2.8075921535491943,
      "time_ms": 25496.26612663269,
      "mfu": {
        "mfu": 0.2462389437445881,
        "mfu_percent": 24.62389437445881,
        "flops_achieved": 76334072560822.31,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5140.831184809678,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.3340725608223,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 650,
      "loss": 3.0927419662475586,
      "time_ms": 25394.91295814514,
      "mfu": {
        "mfu": 0.2472217034490484,
        "mfu_percent": 24.72217034490484,
        "flops_achieved": 76638728069205.0,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.348661286122,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.638728069205,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 660,
      "loss": 3.65507173538208,
      "time_ms": 25395.554780960083,
      "mfu": {
        "mfu": 0.2472154554056018,
        "mfu_percent": 24.72154554056018,
        "flops_achieved": 76636791175736.56,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.218218326507,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63679117573656,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 670,
      "loss": 2.712946653366089,
      "time_ms": 25414.32523727417,
      "mfu": {
        "mfu": 0.2470328675594743,
        "mfu_percent": 24.703286755947428,
        "flops_achieved": 76580188943437.03,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5157.406257151458,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.58018894343704,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 680,
      "loss": 2.4956345558166504,
      "time_ms": 25391.091346740723,
      "mfu": {
        "mfu": 0.2472589127705549,
        "mfu_percent": 24.72589127705549,
        "flops_achieved": 76650262958872.02,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.125495516552,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65026295887202,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 690,
      "loss": 2.781557083129883,
      "time_ms": 25387.197732925415,
      "mfu": {
        "mfu": 0.24729683466838895,
        "mfu_percent": 24.729683466838896,
        "flops_achieved": 76662018747200.58,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.917206494548,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.66201874720058,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 700,
      "loss": 3.2431015968322754,
      "time_ms": 25396.13652229309,
      "mfu": {
        "mfu": 0.24720979251871184,
        "mfu_percent": 24.720979251871185,
        "flops_achieved": 76635035680800.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.09999191976,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63503568080067,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 710,
      "loss": 3.260570764541626,
      "time_ms": 25413.870334625244,
      "mfu": {
        "mfu": 0.24703728939307718,
        "mfu_percent": 24.703728939307716,
        "flops_achieved": 76581559711853.92,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5157.498573580914,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.58155971185393,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 720,
      "loss": 2.7611465454101562,
      "time_ms": 25383.197784423828,
      "mfu": {
        "mfu": 0.2473358043290156,
        "mfu_percent": 24.73358043290156,
        "flops_achieved": 76674099341994.84,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.7307920450885,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.67409934199485,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 730,
      "loss": 2.750084638595581,
      "time_ms": 25389.60838317871,
      "mfu": {
        "mfu": 0.24727335474037526,
        "mfu_percent": 24.727335474037528,
        "flops_achieved": 76654739969516.33,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.42700642987,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65473996951633,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 740,
      "loss": 2.9776124954223633,
      "time_ms": 25466.58420562744,
      "mfu": {
        "mfu": 0.24652594120045535,
        "mfu_percent": 24.652594120045535,
        "flops_achieved": 76423041772141.16,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5146.822948129673,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.42304177214116,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 750,
      "loss": 2.957409381866455,
      "time_ms": 25435.290813446045,
      "mfu": {
        "mfu": 0.24682924549594995,
        "mfu_percent": 24.682924549594993,
        "flops_achieved": 76517066103744.48,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5153.1551560130165,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.51706610374448,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 760,
      "loss": 3.3882031440734863,
      "time_ms": 25433.244705200195,
      "mfu": {
        "mfu": 0.246849102944749,
        "mfu_percent": 24.6849102944749,
        "flops_achieved": 76523221912872.19,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5153.569728096881,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.52322191287219,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 770,
      "loss": 2.7317516803741455,
      "time_ms": 25389.962434768677,
      "mfu": {
        "mfu": 0.24726990662482068,
        "mfu_percent": 24.726990662482066,
        "flops_achieved": 76653671053694.4,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.355018710534,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65367105369441,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 780,
      "loss": 2.9971389770507812,
      "time_ms": 25382.622480392456,
      "mfu": {
        "mfu": 0.24734141026218676,
        "mfu_percent": 24.734141026218676,
        "flops_achieved": 76675837181277.89,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.847829405743,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.67583718127788,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 790,
      "loss": 2.7113420963287354,
      "time_ms": 25393.409967422485,
      "mfu": {
        "mfu": 0.2472363360614939,
        "mfu_percent": 24.723633606149388,
        "flops_achieved": 76643264179063.11,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.654152323531,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64326417906311,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 800,
      "loss": 3.052945137023926,
      "time_ms": 25397.222757339478,
      "mfu": {
        "mfu": 0.24719921939648468,
        "mfu_percent": 24.71992193964847,
        "flops_achieved": 76631758012910.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.8792525207045,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63175801291025,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 810,
      "loss": 2.7720305919647217,
      "time_ms": 25380.227088928223,
      "mfu": {
        "mfu": 0.2473647543993695,
        "mfu_percent": 24.73647543993695,
        "flops_achieved": 76683073863804.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.335194509681,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.68307386380455,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 820,
      "loss": 3.071495532989502,
      "time_ms": 25384.658813476562,
      "mfu": {
        "mfu": 0.24732156877050127,
        "mfu_percent": 24.732156877050127,
        "flops_achieved": 76669686318855.39,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.433590465067,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6696863188554,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 830,
      "loss": 2.558474063873291,
      "time_ms": 25843.948125839233,
      "mfu": {
        "mfu": 0.24292625917229446,
        "mfu_percent": 24.292625917229447,
        "flops_achieved": 75307140343411.28,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5071.6709135068995,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.30714034341128,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 840,
      "loss": 2.6478662490844727,
      "time_ms": 25471.933126449585,
      "mfu": {
        "mfu": 0.24647417254459636,
        "mfu_percent": 24.647417254459636,
        "flops_achieved": 76406993488824.88,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5145.742152718564,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.40699348882488,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 850,
      "loss": 2.8936808109283447,
      "time_ms": 25374.576807022095,
      "mfu": {
        "mfu": 0.24741983632671063,
        "mfu_percent": 24.741983632671065,
        "flops_achieved": 76700149261280.3,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.48516244525,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7001492612803,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 860,
      "loss": 2.6709978580474854,
      "time_ms": 25374.772310256958,
      "mfu": {
        "mfu": 0.24741793004838902,
        "mfu_percent": 24.741793004838904,
        "flops_achieved": 76699558315000.6,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.445364292717,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.69955831500059,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 870,
      "loss": 2.91180419921875,
      "time_ms": 25396.090507507324,
      "mfu": {
        "mfu": 0.2472102404343325,
        "mfu_percent": 24.72102404343325,
        "flops_achieved": 76635174534643.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.109343237451,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63517453464308,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 880,
      "loss": 2.7624433040618896,
      "time_ms": 25391.870975494385,
      "mfu": {
        "mfu": 0.24725132096457172,
        "mfu_percent": 24.725132096457173,
        "flops_achieved": 76647909499017.23,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.966998276621,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64790949901723,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 890,
      "loss": 3.38405704498291,
      "time_ms": 25392.688512802124,
      "mfu": {
        "mfu": 0.24724336051646184,
        "mfu_percent": 24.724336051646183,
        "flops_achieved": 76645441760103.17,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.800804744168,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64544176010317,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 900,
      "loss": 3.2097039222717285,
      "time_ms": 25395.32446861267,
      "mfu": {
        "mfu": 0.24721769742349456,
        "mfu_percent": 24.721769742349455,
        "flops_achieved": 76637486201283.31,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.265025851445,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63748620128331,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 910,
      "loss": 2.555624008178711,
      "time_ms": 25401.796102523804,
      "mfu": {
        "mfu": 0.24715471359244495,
        "mfu_percent": 24.715471359244496,
        "flops_achieved": 76617961213657.94,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5159.950086638846,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.61796121365794,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 920,
      "loss": 2.749082088470459,
      "time_ms": 25391.405820846558,
      "mfu": {
        "mfu": 0.24725585045387768,
        "mfu_percent": 24.725585045387767,
        "flops_achieved": 76649313640702.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.061562278241,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64931364070208,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 930,
      "loss": 2.780653953552246,
      "time_ms": 25408.98060798645,
      "mfu": {
        "mfu": 0.2470848294669336,
        "mfu_percent": 24.708482946693362,
        "flops_achieved": 76596297134749.42,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.491087155302,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.59629713474942,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 940,
      "loss": 2.743903875350952,
      "time_ms": 25404.11329269409,
      "mfu": {
        "mfu": 0.2471321698230058,
        "mfu_percent": 24.71321698230058,
        "flops_achieved": 76610972645131.8,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5159.479431139786,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6109726451318,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 950,
      "loss": 2.5765562057495117,
      "time_ms": 25416.6419506073,
      "mfu": {
        "mfu": 0.2470103506455913,
        "mfu_percent": 24.70103506455913,
        "flops_achieved": 76573208700133.3,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5156.936162326833,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.57320870013329,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 960,
      "loss": 2.7574751377105713,
      "time_ms": 25390.522718429565,
      "mfu": {
        "mfu": 0.24726445020747756,
        "mfu_percent": 24.726445020747757,
        "flops_achieved": 76651979564318.05,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.2411028530005,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65197956431804,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 970,
      "loss": 2.5076756477355957,
      "time_ms": 25407.83429145813,
      "mfu": {
        "mfu": 0.24709597710827388,
        "mfu_percent": 24.70959771082739,
        "flops_achieved": 76599752903564.9,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.723821024964,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.5997529035649,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 980,
      "loss": 2.4532055854797363,
      "time_ms": 25364.540338516235,
      "mfu": {
        "mfu": 0.24751773762363447,
        "mfu_percent": 24.751773762363445,
        "flops_achieved": 76730498663326.69,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5167.529087880463,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.73049866332669,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 990,
      "loss": 2.575326919555664,
      "time_ms": 25379.738807678223,
      "mfu": {
        "mfu": 0.24736951345431488,
        "mfu_percent": 24.73695134543149,
        "flops_achieved": 76684549170837.61,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.434551247088,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.68454917083761,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1000,
      "loss": 2.796316146850586,
      "time_ms": 399610.7168197632,
      "mfu": {
        "mfu": 0.015710723902543897,
        "mfu_percent": 1.5710723902543897,
        "flops_achieved": 4870324409788.608,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 327.9992114403617,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 4.8703244097886085,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1010,
      "loss": 2.4246039390563965,
      "time_ms": 25361.005783081055,
      "mfu": {
        "mfu": 0.2475522340932266,
        "mfu_percent": 24.75522340932266,
        "flops_achieved": 76741192568900.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5168.249284791431,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.74119256890025,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1020,
      "loss": 2.6399526596069336,
      "time_ms": 25372.21360206604,
      "mfu": {
        "mfu": 0.24744288137089185,
        "mfu_percent": 24.744288137089185,
        "flops_achieved": 76707293224976.47,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.966283262211,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.70729322497647,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1030,
      "loss": 2.6759302616119385,
      "time_ms": 25389.549493789673,
      "mfu": {
        "mfu": 0.24727392827464728,
        "mfu_percent": 24.72739282746473,
        "flops_achieved": 76654917765140.66,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.438980339546,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65491776514065,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1040,
      "loss": 2.639859914779663,
      "time_ms": 25382.452249526978,
      "mfu": {
        "mfu": 0.2473430690909683,
        "mfu_percent": 24.73430690909683,
        "flops_achieved": 76676351418200.17,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.882461452976,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.67635141820017,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1050,
      "loss": 2.8483824729919434,
      "time_ms": 25368.195295333862,
      "mfu": {
        "mfu": 0.24748207617306306,
        "mfu_percent": 24.748207617306306,
        "flops_achieved": 76719443613649.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5166.784569184901,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.71944361364955,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1060,
      "loss": 1.8747549057006836,
      "time_ms": 25373.06809425354,
      "mfu": {
        "mfu": 0.24743454820407892,
        "mfu_percent": 24.74345482040789,
        "flops_achieved": 76704709943264.47,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.792308328886,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.70470994326448,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1070,
      "loss": 2.2665510177612305,
      "time_ms": 25375.789642333984,
      "mfu": {
        "mfu": 0.24740801090103567,
        "mfu_percent": 24.740801090103567,
        "flops_achieved": 76696483379321.06,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.238278194696,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.69648337932107,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1080,
      "loss": 2.6447572708129883,
      "time_ms": 25352.18119621277,
      "mfu": {
        "mfu": 0.2476384020713302,
        "mfu_percent": 24.76384020713302,
        "flops_achieved": 76767904642112.36,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5170.048248928584,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.76790464211236,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1090,
      "loss": 2.544426918029785,
      "time_ms": 25374.912977218628,
      "mfu": {
        "mfu": 0.24741655847606034,
        "mfu_percent": 24.741655847606033,
        "flops_achieved": 76699133127578.7,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.41672941599,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6991331275787,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1100,
      "loss": 2.5055525302886963,
      "time_ms": 25373.37327003479,
      "mfu": {
        "mfu": 0.2474315722090958,
        "mfu_percent": 24.74315722090958,
        "flops_achieved": 76703787384819.7,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5165.730177263903,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.7037873848197,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1110,
      "loss": 2.4960427284240723,
      "time_ms": 25395.659685134888,
      "mfu": {
        "mfu": 0.24721443421010342,
        "mfu_percent": 24.721443421010342,
        "flops_achieved": 76636474605132.06,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.196898410234,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63647460513207,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1120,
      "loss": 2.4936442375183105,
      "time_ms": 25384.865283966064,
      "mfu": {
        "mfu": 0.2473195571543356,
        "mfu_percent": 24.73195571543356,
        "flops_achieved": 76669062717844.03,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.391593131262,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.66906271784403,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1130,
      "loss": 2.6181116104125977,
      "time_ms": 25391.74175262451,
      "mfu": {
        "mfu": 0.24725257926837715,
        "mfu_percent": 24.725257926837717,
        "flops_achieved": 76648299573196.92,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.993268400041,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.64829957319692,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1140,
      "loss": 2.16070294380188,
      "time_ms": 25385.220289230347,
      "mfu": {
        "mfu": 0.24731609845893135,
        "mfu_percent": 24.731609845893136,
        "flops_achieved": 76667990522268.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.319384532076,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.66799052226872,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1150,
      "loss": 2.7988359928131104,
      "time_ms": 25396.332263946533,
      "mfu": {
        "mfu": 0.24720788715485725,
        "mfu_percent": 24.720788715485725,
        "flops_achieved": 76634445018005.75,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5161.06021285893,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63444501800575,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1160,
      "loss": 1.8831971883773804,
      "time_ms": 25409.586191177368,
      "mfu": {
        "mfu": 0.24707894072800135,
        "mfu_percent": 24.707894072800134,
        "flops_achieved": 76594471625680.42,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5158.368145543054,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.59447162568043,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1170,
      "loss": 2.1465673446655273,
      "time_ms": 25466.816663742065,
      "mfu": {
        "mfu": 0.2465236909405876,
        "mfu_percent": 24.65236909405876,
        "flops_achieved": 76422344191582.16,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5146.775968533651,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.42234419158216,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1180,
      "loss": 2.2966325283050537,
      "time_ms": 26061.009645462036,
      "mfu": {
        "mfu": 0.24090293222949496,
        "mfu_percent": 24.090293222949498,
        "flops_achieved": 74679908991143.44,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5029.429089015489,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 74.67990899114344,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1190,
      "loss": 2.3341972827911377,
      "time_ms": 25976.91059112549,
      "mfu": {
        "mfu": 0.2416828444025123,
        "mfu_percent": 24.16828444025123,
        "flops_achieved": 74921681764778.81,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5045.711634576678,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 74.92168176477881,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1200,
      "loss": 2.303044080734253,
      "time_ms": 25754.606246948242,
      "mfu": {
        "mfu": 0.2437689623461775,
        "mfu_percent": 24.37689623461775,
        "flops_achieved": 75568378327315.03,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5089.264372486036,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.56837832731503,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1210,
      "loss": 2.5457382202148438,
      "time_ms": 25385.8380317688,
      "mfu": {
        "mfu": 0.24731008023435,
        "mfu_percent": 24.731008023435,
        "flops_achieved": 76666124872648.5,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.193739594948,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6661248726485,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1220,
      "loss": 2.6627562046051025,
      "time_ms": 25397.760152816772,
      "mfu": {
        "mfu": 0.2471939888666389,
        "mfu_percent": 24.71939888666389,
        "flops_achieved": 76630136548658.06,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.770052608883,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.63013654865806,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1230,
      "loss": 2.309553623199463,
      "time_ms": 25389.925003051758,
      "mfu": {
        "mfu": 0.24727027116851844,
        "mfu_percent": 24.727027116851843,
        "flops_achieved": 76653784062240.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5162.362629438477,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.65378406224072,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1240,
      "loss": 1.932310700416565,
      "time_ms": 25398.320198059082,
      "mfu": {
        "mfu": 0.24718853811964808,
        "mfu_percent": 24.71885381196481,
        "flops_achieved": 76628446817090.9,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.656255133614,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6284468170909,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1250,
      "loss": 2.6100239753723145,
      "time_ms": 25399.067401885986,
      "mfu": {
        "mfu": 0.2471812661903789,
        "mfu_percent": 24.71812661903789,
        "flops_achieved": 76626192519017.45,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5160.504436090727,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.62619251901745,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1260,
      "loss": 2.442108631134033,
      "time_ms": 25385.706186294556,
      "mfu": {
        "mfu": 0.247311364686103,
        "mfu_percent": 24.731136468610302,
        "flops_achieved": 76666523052691.94,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.22055561977,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.66652305269194,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1270,
      "loss": 2.3328797817230225,
      "time_ms": 25380.306243896484,
      "mfu": {
        "mfu": 0.24736398292919515,
        "mfu_percent": 24.736398292919514,
        "flops_achieved": 76682834708050.5,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5164.319088211179,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6828347080505,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1280,
      "loss": 3.009267568588257,
      "time_ms": 25383.27717781067,
      "mfu": {
        "mfu": 0.24733503071625257,
        "mfu_percent": 24.733503071625258,
        "flops_achieved": 76673859522038.3,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5163.714641014887,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.6738595220383,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1290,
      "loss": 1.701413631439209,
      "time_ms": 25416.92614555359,
      "mfu": {
        "mfu": 0.24700758874224657,
        "mfu_percent": 24.700758874224658,
        "flops_achieved": 76572352510096.44,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5156.8785009405865,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.57235251009644,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1300,
      "loss": 2.704145908355713,
      "time_ms": 25447.102785110474,
      "mfu": {
        "mfu": 0.246714672922468,
        "mfu_percent": 24.6714672922468,
        "flops_achieved": 76481548605965.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5150.763177515533,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.48154860596507,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1310,
      "loss": 2.444512128829956,
      "time_ms": 25665.307998657227,
      "mfu": {
        "mfu": 0.2446171166456066,
        "mfu_percent": 24.46171166456066,
        "flops_achieved": 75831306160138.05,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5106.971636843692,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.83130616013804,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1320,
      "loss": 2.1641502380371094,
      "time_ms": 25724.11799430847,
      "mfu": {
        "mfu": 0.24405787758561895,
        "mfu_percent": 24.405787758561896,
        "flops_achieved": 75657942051541.88,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5095.29617415843,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.65794205154188,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1330,
      "loss": 2.3668675422668457,
      "time_ms": 25683.807134628296,
      "mfu": {
        "mfu": 0.24444092760642103,
        "mfu_percent": 24.444092760642103,
        "flops_achieved": 75776687557990.52,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5103.293266179438,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.77668755799051,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1340,
      "loss": 2.4732160568237305,
      "time_ms": 25695.499897003174,
      "mfu": {
        "mfu": 0.24432969452309306,
        "mfu_percent": 24.432969452309305,
        "flops_achieved": 75742205302158.84,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5100.971007584356,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.74220530215884,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1350,
      "loss": 1.9331493377685547,
      "time_ms": 25663.772583007812,
      "mfu": {
        "mfu": 0.24463175163147227,
        "mfu_percent": 24.463175163147227,
        "flops_achieved": 75835843005756.4,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5107.277177431965,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.8358430057564,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1360,
      "loss": 2.051849603652954,
      "time_ms": 25675.20260810852,
      "mfu": {
        "mfu": 0.24452284705516739,
        "mfu_percent": 24.45228470551674,
        "flops_achieved": 75802082587101.89,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5105.003532030784,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.80208258710189,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1370,
      "loss": 2.5980029106140137,
      "time_ms": 25433.098554611206,
      "mfu": {
        "mfu": 0.24685052145620992,
        "mfu_percent": 24.685052145620993,
        "flops_achieved": 76523661651425.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5153.599342941078,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.52366165142507,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1380,
      "loss": 1.7681806087493896,
      "time_ms": 25422.375917434692,
      "mfu": {
        "mfu": 0.24695463794740666,
        "mfu_percent": 24.695463794740665,
        "flops_achieved": 76555937763696.06,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5155.773025530265,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.55593776369606,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1390,
      "loss": 2.5380280017852783,
      "time_ms": 25413.24520111084,
      "mfu": {
        "mfu": 0.24704336619624354,
        "mfu_percent": 24.704336619624353,
        "flops_achieved": 76583443520835.5,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5157.625441487131,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.5834435208355,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1400,
      "loss": 2.5195021629333496,
      "time_ms": 25638.821363449097,
      "mfu": {
        "mfu": 0.24486982265897636,
        "mfu_percent": 24.486982265897637,
        "flops_achieved": 75909645024282.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5112.247483687267,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.90964502428267,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1410,
      "loss": 2.3557167053222656,
      "time_ms": 25670.095920562744,
      "mfu": {
        "mfu": 0.2445714912745571,
        "mfu_percent": 24.45714912745571,
        "flops_achieved": 75817162295112.7,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5106.019097303265,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.81716229511271,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1420,
      "loss": 2.2817275524139404,
      "time_ms": 25709.017515182495,
      "mfu": {
        "mfu": 0.2442012277110695,
        "mfu_percent": 24.42012277110695,
        "flops_achieved": 75702380590431.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5098.288953383584,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.70238059043155,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1430,
      "loss": 2.4256298542022705,
      "time_ms": 25654.73198890686,
      "mfu": {
        "mfu": 0.24471795858821074,
        "mfu_percent": 24.471795858821075,
        "flops_achieved": 75862567162345.33,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5109.076955342029,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.86256716234533,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1440,
      "loss": 2.255000352859497,
      "time_ms": 25640.650033950806,
      "mfu": {
        "mfu": 0.24485235874051633,
        "mfu_percent": 24.485235874051632,
        "flops_achieved": 75904231209560.06,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.882882315676,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.90423120956007,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1450,
      "loss": 1.9125399589538574,
      "time_ms": 25833.109378814697,
      "mfu": {
        "mfu": 0.2430281832663001,
        "mfu_percent": 24.30281832663001,
        "flops_achieved": 75338736812553.03,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5073.798824522842,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.33873681255304,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1460,
      "loss": 2.419055461883545,
      "time_ms": 25628.25298309326,
      "mfu": {
        "mfu": 0.2449708001787953,
        "mfu_percent": 24.49708001787953,
        "flops_achieved": 75940948055426.55,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5114.355632686593,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.94094805542655,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1470,
      "loss": 2.117662191390991,
      "time_ms": 25659.376859664917,
      "mfu": {
        "mfu": 0.24467365964455226,
        "mfu_percent": 24.467365964455226,
        "flops_achieved": 75848834489811.2,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5108.1521081689925,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.8488344898112,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1480,
      "loss": 2.383938789367676,
      "time_ms": 25727.238655090332,
      "mfu": {
        "mfu": 0.24402827387037782,
        "mfu_percent": 24.40282738703778,
        "flops_achieved": 75648764899817.12,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5094.678125282069,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.64876489981712,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1490,
      "loss": 1.6704225540161133,
      "time_ms": 25757.429838180542,
      "mfu": {
        "mfu": 0.2437422398078998,
        "mfu_percent": 24.37422398078998,
        "flops_achieved": 75560094340448.94,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5088.706475120061,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.56009434044894,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1500,
      "loss": 2.3623948097229004,
      "time_ms": 25745.50461769104,
      "mfu": {
        "mfu": 0.24385514029268257,
        "mfu_percent": 24.385514029268258,
        "flops_achieved": 75595093490731.6,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5091.063544737584,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.5950934907316,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1510,
      "loss": 2.2652106285095215,
      "time_ms": 25484.51066017151,
      "mfu": {
        "mfu": 0.24635252856806095,
        "mfu_percent": 24.635252856806094,
        "flops_achieved": 76369283856098.89,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5143.202541646052,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.36928385609889,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1520,
      "loss": 2.151796817779541,
      "time_ms": 25419.927835464478,
      "mfu": {
        "mfu": 0.2469784210675057,
        "mfu_percent": 24.69784210675057,
        "flops_achieved": 76563310530926.77,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5156.269555460169,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.56331053092677,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1530,
      "loss": 2.3087916374206543,
      "time_ms": 25559.797525405884,
      "mfu": {
        "mfu": 0.24562689255314274,
        "mfu_percent": 24.562689255314275,
        "flops_achieved": 76144336691474.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5128.053141646262,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.14433669147425,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1540,
      "loss": 2.366884469985962,
      "time_ms": 25624.366998672485,
      "mfu": {
        "mfu": 0.24500795047066748,
        "mfu_percent": 24.50079504706675,
        "flops_achieved": 75952464645906.92,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5115.131234531195,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.95246464590693,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1550,
      "loss": 2.053823947906494,
      "time_ms": 25670.303344726562,
      "mfu": {
        "mfu": 0.24456951505961372,
        "mfu_percent": 24.45695150596137,
        "flops_achieved": 75816549668480.25,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5105.977839055262,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.81654966848025,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1560,
      "loss": 2.191774606704712,
      "time_ms": 25688.24291229248,
      "mfu": {
        "mfu": 0.2443987181952678,
        "mfu_percent": 24.43987181952678,
        "flops_achieved": 75763602640533.02,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5102.412043031511,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.76360264053301,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1570,
      "loss": 2.864147663116455,
      "time_ms": 25722.405195236206,
      "mfu": {
        "mfu": 0.24407412886939803,
        "mfu_percent": 24.407412886939802,
        "flops_achieved": 75662979949513.39,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5095.6354588596,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.66297994951339,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1580,
      "loss": 2.4083821773529053,
      "time_ms": 25684.722900390625,
      "mfu": {
        "mfu": 0.24443221228434875,
        "mfu_percent": 24.443221228434876,
        "flops_achieved": 75773985808148.11,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5103.111312834393,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.77398580814811,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1590,
      "loss": 1.7541207075119019,
      "time_ms": 25667.277812957764,
      "mfu": {
        "mfu": 0.24459834370450878,
        "mfu_percent": 24.459834370450878,
        "flops_achieved": 75825486548397.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5106.579706470865,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.82548654839772,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1600,
      "loss": 2.557239532470703,
      "time_ms": 25604.116916656494,
      "mfu": {
        "mfu": 0.2452017252103998,
        "mfu_percent": 24.520172521039978,
        "flops_achieved": 76012534815223.94,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5119.176749061494,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.01253481522394,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1610,
      "loss": 2.2236599922180176,
      "time_ms": 25654.465436935425,
      "mfu": {
        "mfu": 0.24472050122759914,
        "mfu_percent": 24.472050122759914,
        "flops_achieved": 75863355380555.73,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5109.1300390649385,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.86335538055573,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1620,
      "loss": 2.010369062423706,
      "time_ms": 25704.177856445312,
      "mfu": {
        "mfu": 0.24424720664149566,
        "mfu_percent": 24.424720664149564,
        "flops_achieved": 75716634058863.66,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5099.248874327787,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.71663405886366,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1630,
      "loss": 2.5138392448425293,
      "time_ms": 25714.8118019104,
      "mfu": {
        "mfu": 0.24414620215056512,
        "mfu_percent": 24.414620215056512,
        "flops_achieved": 75685322666675.19,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5097.140162241531,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.68532266667519,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1640,
      "loss": 2.631258249282837,
      "time_ms": 25692.274808883667,
      "mfu": {
        "mfu": 0.24436036462922073,
        "mfu_percent": 24.436036462922072,
        "flops_achieved": 75751713035058.42,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5101.611319939602,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.75171303505842,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1650,
      "loss": 2.4770989418029785,
      "time_ms": 25642.107725143433,
      "mfu": {
        "mfu": 0.2448384394819805,
        "mfu_percent": 24.48384394819805,
        "flops_achieved": 75899916239413.95,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.59228425973,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.89991623941395,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1660,
      "loss": 2.1074905395507812,
      "time_ms": 25642.574548721313,
      "mfu": {
        "mfu": 0.24483398219333716,
        "mfu_percent": 24.483398219333715,
        "flops_achieved": 75898534479934.52,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.499227620887,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.89853447993451,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1670,
      "loss": 2.2133772373199463,
      "time_ms": 25686.930894851685,
      "mfu": {
        "mfu": 0.24441120140636421,
        "mfu_percent": 24.44112014063642,
        "flops_achieved": 75767472435972.9,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5102.672660137462,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.7674724359729,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1680,
      "loss": 2.4014835357666016,
      "time_ms": 25689.247608184814,
      "mfu": {
        "mfu": 0.2443891598620691,
        "mfu_percent": 24.43891598620691,
        "flops_achieved": 75760639557241.42,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5102.212489798235,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.76063955724142,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1690,
      "loss": 2.140883684158325,
      "time_ms": 25689.15104866028,
      "mfu": {
        "mfu": 0.2443900784638957,
        "mfu_percent": 24.439007846389572,
        "flops_achieved": 75760924323807.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5102.231667824444,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.76092432380767,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1700,
      "loss": 2.1307804584503174,
      "time_ms": 25830.87682723999,
      "mfu": {
        "mfu": 0.24304918808765705,
        "mfu_percent": 24.304918808765706,
        "flops_achieved": 75345248307173.69,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5074.237350773081,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.34524830717369,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1710,
      "loss": 2.3071985244750977,
      "time_ms": 25644.978046417236,
      "mfu": {
        "mfu": 0.24481103587179928,
        "mfu_percent": 24.481103587179927,
        "flops_achieved": 75891421120257.78,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.020167876945,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.89142112025777,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1720,
      "loss": 2.31813645362854,
      "time_ms": 25623.987197875977,
      "mfu": {
        "mfu": 0.24501158199818976,
        "mfu_percent": 24.501158199818978,
        "flops_achieved": 75953590419438.83,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5115.207051417229,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.95359041943883,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1730,
      "loss": 2.014808416366577,
      "time_ms": 25634.579181671143,
      "mfu": {
        "mfu": 0.24491034535655187,
        "mfu_percent": 24.49103453565519,
        "flops_achieved": 75922207060531.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5113.093492625662,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.92220706053108,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1740,
      "loss": 2.2192015647888184,
      "time_ms": 25659.550428390503,
      "mfu": {
        "mfu": 0.2446720046001505,
        "mfu_percent": 24.46720046001505,
        "flops_achieved": 75848321426046.66,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5108.117555129804,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.84832142604665,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1750,
      "loss": 2.409656047821045,
      "time_ms": 25691.9846534729,
      "mfu": {
        "mfu": 0.2443631243413617,
        "mfu_percent": 24.43631243413617,
        "flops_achieved": 75752568545822.12,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5101.6689355791905,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.75256854582213,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1760,
      "loss": 2.0840601921081543,
      "time_ms": 25615.901470184326,
      "mfu": {
        "mfu": 0.24508892055821052,
        "mfu_percent": 24.50889205582105,
        "flops_achieved": 75977565373045.27,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5116.821680180238,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.97756537304527,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1770,
      "loss": 2.0218403339385986,
      "time_ms": 25720.834255218506,
      "mfu": {
        "mfu": 0.24408903607701507,
        "mfu_percent": 24.408903607701507,
        "flops_achieved": 75667601183874.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5095.946682732764,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.66760118387467,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1780,
      "loss": 2.1874585151672363,
      "time_ms": 25737.979650497437,
      "mfu": {
        "mfu": 0.24392643578500994,
        "mfu_percent": 24.392643578500994,
        "flops_achieved": 75617195093353.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5092.552009903651,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.61719509335308,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1790,
      "loss": 2.030642032623291,
      "time_ms": 25718.8241481781,
      "mfu": {
        "mfu": 0.24410811335236315,
        "mfu_percent": 24.410811335236314,
        "flops_achieved": 75673515139232.58,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5096.344966816262,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.67351513923258,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1800,
      "loss": 2.0590932369232178,
      "time_ms": 25699.559688568115,
      "mfu": {
        "mfu": 0.24429109745587052,
        "mfu_percent": 24.429109745587052,
        "flops_achieved": 75730240211319.86,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5100.165200818771,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.73024021131985,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1810,
      "loss": 2.1461381912231445,
      "time_ms": 25757.283687591553,
      "mfu": {
        "mfu": 0.2437436228369622,
        "mfu_percent": 24.374362283696218,
        "flops_achieved": 75560523079458.28,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5088.735349183707,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.56052307945828,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1820,
      "loss": 2.288496255874634,
      "time_ms": 25687.187433242798,
      "mfu": {
        "mfu": 0.244408760467412,
        "mfu_percent": 24.4408760467412,
        "flops_achieved": 75766715744897.72,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5102.621699656171,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.76671574489772,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1830,
      "loss": 2.604065418243408,
      "time_ms": 25675.845861434937,
      "mfu": {
        "mfu": 0.24451672105894506,
        "mfu_percent": 24.451672105894506,
        "flops_achieved": 75800183528272.97,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5104.8756371010095,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.80018352827297,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1840,
      "loss": 2.0654475688934326,
      "time_ms": 25669.12269592285,
      "mfu": {
        "mfu": 0.24458076401069007,
        "mfu_percent": 24.458076401069007,
        "flops_achieved": 75820036843313.92,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5106.2126880097385,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.82003684331393,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1850,
      "loss": 1.5412548780441284,
      "time_ms": 25619.67444419861,
      "mfu": {
        "mfu": 0.245052826651925,
        "mfu_percent": 24.5052826651925,
        "flops_achieved": 75966376262096.75,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5116.068132929781,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.96637626209674,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1860,
      "loss": 2.3498659133911133,
      "time_ms": 25762.412071228027,
      "mfu": {
        "mfu": 0.24369510211602213,
        "mfu_percent": 24.369510211602215,
        "flops_achieved": 75545481655966.86,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5087.722362238892,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.54548165596685,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1870,
      "loss": 2.347855806350708,
      "time_ms": 25666.894912719727,
      "mfu": {
        "mfu": 0.24460199263689214,
        "mfu_percent": 24.460199263689216,
        "flops_achieved": 75826617717436.56,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5106.655886725306,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.82661771743656,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1880,
      "loss": 2.138373851776123,
      "time_ms": 25623.271942138672,
      "mfu": {
        "mfu": 0.2450184213253501,
        "mfu_percent": 24.50184213253501,
        "flops_achieved": 75955710610858.53,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5115.349838848877,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.95571061085853,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1890,
      "loss": 2.156491756439209,
      "time_ms": 25671.877145767212,
      "mfu": {
        "mfu": 0.2445545218530349,
        "mfu_percent": 24.45545218530349,
        "flops_achieved": 75811901774440.81,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5105.664819746584,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.81190177444081,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1900,
      "loss": 2.242387056350708,
      "time_ms": 25680.195093154907,
      "mfu": {
        "mfu": 0.24447530938448403,
        "mfu_percent": 24.447530938448402,
        "flops_achieved": 75787345909190.05,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5104.011068628424,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.78734590919005,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1910,
      "loss": 2.1452174186706543,
      "time_ms": 25650.9006023407,
      "mfu": {
        "mfu": 0.24475451126577827,
        "mfu_percent": 24.475451126577827,
        "flops_achieved": 75873898492391.27,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5109.84008054826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.87389849239126,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1920,
      "loss": 2.034160614013672,
      "time_ms": 25642.95792579651,
      "mfu": {
        "mfu": 0.24483032178347833,
        "mfu_percent": 24.483032178347834,
        "flops_achieved": 75897399752878.28,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.422807746494,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.89739975287829,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1930,
      "loss": 1.5609902143478394,
      "time_ms": 25613.72208595276,
      "mfu": {
        "mfu": 0.24510977433834466,
        "mfu_percent": 24.510977433834466,
        "flops_achieved": 75984030044886.84,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5117.257053081064,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.98403004488685,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1940,
      "loss": 2.2495875358581543,
      "time_ms": 25641.468048095703,
      "mfu": {
        "mfu": 0.2448445474602696,
        "mfu_percent": 24.48445474602696,
        "flops_achieved": 75901809712683.58,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5111.71980302174,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.90180971268357,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1950,
      "loss": 1.549511432647705,
      "time_ms": 25676.6095161438,
      "mfu": {
        "mfu": 0.24450944882367132,
        "mfu_percent": 24.450944882367132,
        "flops_achieved": 75797929135338.11,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5104.723811669542,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.79792913533811,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1960,
      "loss": 2.409174680709839,
      "time_ms": 25649.219512939453,
      "mfu": {
        "mfu": 0.24477055285388927,
        "mfu_percent": 24.47705528538893,
        "flops_achieved": 75878871384705.67,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5110.174987347164,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.87887138470568,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1970,
      "loss": 2.3247008323669434,
      "time_ms": 25587.558031082153,
      "mfu": {
        "mfu": 0.24536040652361688,
        "mfu_percent": 24.536040652361688,
        "flops_achieved": 76061726022321.23,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5122.489603766878,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.06172602232124,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1980,
      "loss": 2.351907730102539,
      "time_ms": 25679.166078567505,
      "mfu": {
        "mfu": 0.24448510598998308,
        "mfu_percent": 24.448510598998308,
        "flops_achieved": 75790382856894.75,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5104.215596370012,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 75.79038285689475,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 1990,
      "loss": 2.384600877761841,
      "time_ms": 25584.494829177856,
      "mfu": {
        "mfu": 0.2453897832406293,
        "mfu_percent": 24.538978324062928,
        "flops_achieved": 76070832804595.08,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 5123.1029135083345,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 76.07083280459509,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    },
    {
      "iter": 2000,
      "loss": 2.219989776611328,
      "time_ms": 393806.2813282013,
      "mfu": {
        "mfu": 0.01594228923743518,
        "mfu_percent": 1.594228923743518,
        "flops_achieved": 4942109663604.906,
        "flops_per_token": 14848585728.0,
        "tokens_per_sec": 332.83369568898155,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 4.9421096636049064,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 2.223106048,
        "non_attn_gflops": 13.338636288,
        "attn_gflops": 1.50994944,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 180388626432.0,
        "attention_to_ffn_ratio": 1.9047619047619047,
        "architecture": "30L-16H-2048D-GQA8-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 14.420050432,
        "reserved_gb": 44.843401216,
        "max_allocated_gb": 40.85966848,
        "max_reserved_gb": 44.843401216
      }
    }
  ],
  "eval_steps": [
    {
      "iter": 1000,
      "train_loss": 2.590294122695923,
      "val_loss": 2.5646393299102783,
      "timestamp": "2025-11-12T19:30:14.167426",
      "lr": 0.00015007496251874061
    },
    {
      "iter": 2000,
      "train_loss": 2.148232936859131,
      "val_loss": 2.101317882537842,
      "timestamp": "2025-11-13T02:42:56.066608",
      "lr": 0.0003
    }
  ],
  "checkpoints": [
    {
      "iter": 1000,
      "val_loss": 2.5646393299102783,
      "path": "out-llama3-2.2b-chinchilla/ckpt.pt",
      "timestamp": "2025-11-12T19:36:03.596962"
    },
    {
      "iter": 2000,
      "val_loss": 2.101317882537842,
      "path": "out-llama3-2.2b-chinchilla/ckpt.pt",
      "timestamp": "2025-11-13T02:48:39.376909"
    }
  ],
  "metadata": {
    "world_size": 2,
    "device": "cuda:0",
    "dtype": "bfloat16",
    "compile": false,
    "use_zero1": true,
    "use_fsdp": false
  },
  "end_time": "2025-11-13T02:49:04.479950",
  "summary": {
    "total_iterations": 201,
    "final_iter": 2000,
    "final_train_loss": 2.219989776611328,
    "best_val_loss": 2.101317882537842,
    "avg_time_ms": 29198.614967403126,
    "avg_mfu": 24.38398554529069,
    "total_eval_steps": 2,
    "total_checkpoints": 2
  }
}