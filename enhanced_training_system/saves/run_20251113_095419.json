{
  "run_name": "run_20251113_095419",
  "start_time": "2025-11-13T09:54:19.673422",
  "config": {
    "MODULAR_ARCH_AVAILABLE": true,
    "LOGGER_AVAILABLE": true,
    "out_dir": "out-gpt2-1.36b",
    "eval_interval": 1000,
    "log_interval": 10,
    "eval_iters": 50,
    "eval_only": false,
    "eval_at_start": false,
    "always_save_checkpoint": false,
    "init_from": "scratch",
    "save_log_to_json": true,
    "log_save_interval": 10,
    "gradient_log_interval": 10,
    "wandb_log": false,
    "wandb_project": "gpt2-1.36b",
    "wandb_run_name": "run-1",
    "dataset": "slimpajama_6b_gpt2",
    "gradient_accumulation_steps": 32,
    "batch_size": 4,
    "block_size": 2048,
    "n_layer": 18,
    "n_head": 18,
    "n_embd": 2304,
    "dropout": 0.0,
    "bias": false,
    "arch_preset": "gpt2",
    "normalization": "layernorm_nobias",
    "activation": "gelu",
    "attention_backend": "sdpa",
    "position_encoding": "learned_absolute",
    "norm_position": "post",
    "ffn_type": "standard",
    "weight_tying": true,
    "rope_theta": 10000.0,
    "d_ff": null,
    "intermediate_size": 0,
    "learning_rate": 0.0003,
    "max_iters": 2000,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.95,
    "grad_clip": 1.0,
    "decay_lr": true,
    "warmup_iters": 2000,
    "lr_decay_iters": 25000,
    "min_lr": 3e-05,
    "backend": "nccl",
    "use_zero1": true,
    "use_fsdp": false,
    "fsdp_min_num_params": 1000000.0,
    "fsdp_activation_checkpointing": false,
    "device": "cuda",
    "dtype": "bfloat16",
    "compile": true
  },
  "startup_info": {
    "timestamp": "2025-11-13T09:54:19.674967",
    "model": {
      "total_params": 1267213824,
      "trainable_params": 1267213824,
      "non_embedding_params": 1262495232
    },
    "optimizer": {
      "type": "ZeroRedundancyOptimizer",
      "param_groups": 2
    },
    "config": {
      "MODULAR_ARCH_AVAILABLE": true,
      "LOGGER_AVAILABLE": true,
      "out_dir": "out-gpt2-1.36b",
      "eval_interval": 1000,
      "log_interval": 10,
      "eval_iters": 50,
      "eval_only": false,
      "eval_at_start": false,
      "always_save_checkpoint": false,
      "init_from": "scratch",
      "save_log_to_json": true,
      "log_save_interval": 10,
      "gradient_log_interval": 10,
      "wandb_log": false,
      "wandb_project": "gpt2-1.36b",
      "wandb_run_name": "run-1",
      "dataset": "slimpajama_6b_gpt2",
      "gradient_accumulation_steps": 32,
      "batch_size": 4,
      "block_size": 2048,
      "n_layer": 18,
      "n_head": 18,
      "n_embd": 2304,
      "dropout": 0.0,
      "bias": false,
      "arch_preset": "gpt2",
      "normalization": "layernorm_nobias",
      "activation": "gelu",
      "attention_backend": "sdpa",
      "position_encoding": "learned_absolute",
      "norm_position": "post",
      "ffn_type": "standard",
      "weight_tying": true,
      "rope_theta": 10000.0,
      "d_ff": null,
      "intermediate_size": 0,
      "learning_rate": 0.0003,
      "max_iters": 2000,
      "weight_decay": 0.1,
      "beta1": 0.9,
      "beta2": 0.95,
      "grad_clip": 1.0,
      "decay_lr": true,
      "warmup_iters": 2000,
      "lr_decay_iters": 25000,
      "min_lr": 3e-05,
      "backend": "nccl",
      "use_zero1": true,
      "use_fsdp": false,
      "fsdp_min_num_params": 1000000.0,
      "fsdp_activation_checkpointing": false,
      "device": "cuda",
      "dtype": "bfloat16",
      "compile": true
    },
    "hardware": {
      "gpu_name": "NVIDIA RTX A6000",
      "num_gpus": 2,
      "gpu_memory_gb": 51.033931776,
      "precision": "bfloat16",
      "parallelism": "DDP+ZeRO-1"
    }
  },
  "training_iterations": [
    {
      "iter": 0,
      "loss": 11.20173454284668,
      "time_ms": 34886.62099838257,
      "mfu": -100.0
    },
    {
      "iter": 10,
      "loss": 11.026103973388672,
      "time_ms": 11819.014310836792,
      "mfu": {
        "mfu": 0.30744808089675796,
        "mfu_percent": 30.744808089675796,
        "flops_achieved": 95308905077994.97,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11089.926499185365,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.30890507799496,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 20,
      "loss": 10.889175415039062,
      "time_ms": 11826.231241226196,
      "mfu": {
        "mfu": 0.30726046141317703,
        "mfu_percent": 30.726046141317703,
        "flops_achieved": 95250743038084.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11083.158897069721,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.25074303808488,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 30,
      "loss": 9.45221996307373,
      "time_ms": 11837.231636047363,
      "mfu": {
        "mfu": 0.30697492282675737,
        "mfu_percent": 30.697492282675736,
        "flops_achieved": 95162226076294.78,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.859265577994,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.16222607629479,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 40,
      "loss": 10.09040355682373,
      "time_ms": 11824.379682540894,
      "mfu": {
        "mfu": 0.3073085747849778,
        "mfu_percent": 30.730857478497782,
        "flops_achieved": 95265658183343.12,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11084.894389304189,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.26565818334312,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 50,
      "loss": 9.417238235473633,
      "time_ms": 11811.745643615723,
      "mfu": {
        "mfu": 0.3076372771303395,
        "mfu_percent": 30.76372771303395,
        "flops_achieved": 95367555910405.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11096.750976079877,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.36755591040524,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 60,
      "loss": 8.93653678894043,
      "time_ms": 11814.080715179443,
      "mfu": {
        "mfu": 0.30757647214050693,
        "mfu_percent": 30.757647214050692,
        "flops_achieved": 95348706363557.16,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11094.557685863005,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.34870636355716,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 70,
      "loss": 8.907959938049316,
      "time_ms": 11806.174755096436,
      "mfu": {
        "mfu": 0.3077824395568511,
        "mfu_percent": 30.77824395568511,
        "flops_achieved": 95412556262623.84,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11101.987114278436,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.41255626262384,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 80,
      "loss": 9.107263565063477,
      "time_ms": 11809.857368469238,
      "mfu": {
        "mfu": 0.30768646517778264,
        "mfu_percent": 30.768646517778265,
        "flops_achieved": 95382804205112.62,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11098.525232823298,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.38280420511262,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 90,
      "loss": 9.008460998535156,
      "time_ms": 11808.958053588867,
      "mfu": {
        "mfu": 0.3077098971364167,
        "mfu_percent": 30.770989713641672,
        "flops_achieved": 95390068112289.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11099.370444470826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.39006811228919,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 100,
      "loss": 8.678399085998535,
      "time_ms": 11802.114486694336,
      "mfu": {
        "mfu": 0.30788832560934304,
        "mfu_percent": 30.788832560934303,
        "flops_achieved": 95445380938896.34,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11105.806518634447,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.44538093889635,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 110,
      "loss": 8.382787704467773,
      "time_ms": 11806.081056594849,
      "mfu": {
        "mfu": 0.3077848822601718,
        "mfu_percent": 30.77848822601718,
        "flops_achieved": 95413313500653.27,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11102.075224765926,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.41331350065326,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 120,
      "loss": 8.64995002746582,
      "time_ms": 11805.11212348938,
      "mfu": {
        "mfu": 0.30781014444816845,
        "mfu_percent": 30.781014444816847,
        "flops_achieved": 95421144778932.22,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11102.986454418991,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.42114477893222,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 130,
      "loss": 7.984677314758301,
      "time_ms": 11811.82885169983,
      "mfu": {
        "mfu": 0.3076351099885064,
        "mfu_percent": 30.76351099885064,
        "flops_achieved": 95366884096436.98,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11096.672805341024,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.36688409643699,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 140,
      "loss": 7.967390060424805,
      "time_ms": 11810.0004196167,
      "mfu": {
        "mfu": 0.30768273825988784,
        "mfu_percent": 30.768273825988786,
        "flops_achieved": 95381648860565.23,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11098.390799570692,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.38164886056524,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 150,
      "loss": 7.93146276473999,
      "time_ms": 11811.281442642212,
      "mfu": {
        "mfu": 0.30764936773407503,
        "mfu_percent": 30.764936773407502,
        "flops_achieved": 95371303997563.27,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11097.187094940553,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.37130399756326,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 160,
      "loss": 8.135025024414062,
      "time_ms": 11807.797908782959,
      "mfu": {
        "mfu": 0.30774013038072245,
        "mfu_percent": 30.774013038072244,
        "flops_achieved": 95399440418023.97,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11100.46098455878,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.39944041802397,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 170,
      "loss": 7.810211181640625,
      "time_ms": 11812.186002731323,
      "mfu": {
        "mfu": 0.307625808391255,
        "mfu_percent": 30.762580839125498,
        "flops_achieved": 95364000601289.05,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11096.337288431821,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.36400060128905,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 180,
      "loss": 8.006891250610352,
      "time_ms": 11808.732748031616,
      "mfu": {
        "mfu": 0.3077157681093082,
        "mfu_percent": 30.771576810930817,
        "flops_achieved": 95391888113885.53,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11099.582215699498,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.39188811388553,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 190,
      "loss": 7.601027488708496,
      "time_ms": 11813.846111297607,
      "mfu": {
        "mfu": 0.30758258011191997,
        "mfu_percent": 30.758258011191998,
        "flops_achieved": 95350599834695.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11094.778005839737,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.35059983469519,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 200,
      "loss": 6.569042205810547,
      "time_ms": 11819.873094558716,
      "mfu": {
        "mfu": 0.3074257429744217,
        "mfu_percent": 30.74257429744217,
        "flops_achieved": 95301980322070.73,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11089.1207504029,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.30198032207073,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 210,
      "loss": 7.443830490112305,
      "time_ms": 11820.962190628052,
      "mfu": {
        "mfu": 0.30739741903912043,
        "mfu_percent": 30.739741903912044,
        "flops_achieved": 95293199902127.33,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11088.099080793701,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.29319990212733,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 220,
      "loss": 7.0199198722839355,
      "time_ms": 11816.600561141968,
      "mfu": {
        "mfu": 0.3075108826058958,
        "mfu_percent": 30.751088260589583,
        "flops_achieved": 95328373607827.7,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11092.191812848507,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.3283736078277,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 230,
      "loss": 7.421083450317383,
      "time_ms": 11815.818309783936,
      "mfu": {
        "mfu": 0.30753124097627876,
        "mfu_percent": 30.753124097627875,
        "flops_achieved": 95334684702646.42,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11092.926157426398,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.33468470264643,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 240,
      "loss": 7.3385796546936035,
      "time_ms": 11819.47636604309,
      "mfu": {
        "mfu": 0.30743606192213974,
        "mfu_percent": 30.743606192213974,
        "flops_achieved": 95305179195863.31,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11089.492964050836,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.30517919586332,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 250,
      "loss": 7.3680877685546875,
      "time_ms": 11820.531606674194,
      "mfu": {
        "mfu": 0.3074086165385646,
        "mfu_percent": 30.74086165385646,
        "flops_achieved": 95296671126955.03,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11088.502984586004,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.29667112695503,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 260,
      "loss": 7.031739234924316,
      "time_ms": 11822.750568389893,
      "mfu": {
        "mfu": 0.30735092032419986,
        "mfu_percent": 30.735092032419985,
        "flops_achieved": 95278785300501.95,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11086.421830673056,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.27878530050195,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 270,
      "loss": 7.492973327636719,
      "time_ms": 11819.207668304443,
      "mfu": {
        "mfu": 0.3074430511702294,
        "mfu_percent": 30.74430511702294,
        "flops_achieved": 95307345862771.11,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11089.745072463329,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.30734586277111,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 280,
      "loss": 6.891984462738037,
      "time_ms": 11834.580183029175,
      "mfu": {
        "mfu": 0.3070436983619305,
        "mfu_percent": 30.704369836193052,
        "flops_achieved": 95183546492198.45,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11075.340060474444,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.18354649219846,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 290,
      "loss": 7.422941207885742,
      "time_ms": 11832.855701446533,
      "mfu": {
        "mfu": 0.3070884459035427,
        "mfu_percent": 30.708844590354268,
        "flops_achieved": 95197418230098.23,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11076.954144211935,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.19741823009824,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 300,
      "loss": 7.035157680511475,
      "time_ms": 11834.3505859375,
      "mfu": {
        "mfu": 0.3070496552870401,
        "mfu_percent": 30.704965528704008,
        "flops_achieved": 95185393138982.42,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11075.55493207629,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.18539313898242,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 310,
      "loss": 7.193803787231445,
      "time_ms": 11858.356475830078,
      "mfu": {
        "mfu": 0.3064280682879143,
        "mfu_percent": 30.64280682879143,
        "flops_achieved": 94992701169253.44,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11053.133734607605,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.99270116925344,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 320,
      "loss": 6.941917419433594,
      "time_ms": 11836.539268493652,
      "mfu": {
        "mfu": 0.30699287904449524,
        "mfu_percent": 30.699287904449523,
        "flops_achieved": 95167792503793.53,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11073.506962367435,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.16779250379354,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 330,
      "loss": 7.295132637023926,
      "time_ms": 11843.300342559814,
      "mfu": {
        "mfu": 0.3068176237074719,
        "mfu_percent": 30.68176237074719,
        "flops_achieved": 95113463349316.3,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.185346046039,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11346334931629,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 340,
      "loss": 6.775900840759277,
      "time_ms": 11843.58525276184,
      "mfu": {
        "mfu": 0.3068102428790074,
        "mfu_percent": 30.681024287900737,
        "flops_achieved": 95111175292492.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.919112980162,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11117529249228,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 350,
      "loss": 6.869165897369385,
      "time_ms": 11857.078552246094,
      "mfu": {
        "mfu": 0.3064610942692751,
        "mfu_percent": 30.64610942692751,
        "flops_achieved": 95002939223475.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11054.325011212053,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.00293922347528,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 360,
      "loss": 5.471868515014648,
      "time_ms": 11844.31505203247,
      "mfu": {
        "mfu": 0.30679133846026374,
        "mfu_percent": 30.679133846026374,
        "flops_achieved": 95105314922681.77,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.23721373472,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.10531492268177,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 370,
      "loss": 7.042062759399414,
      "time_ms": 11838.222980499268,
      "mfu": {
        "mfu": 0.30694921644437895,
        "mfu_percent": 30.694921644437894,
        "flops_achieved": 95154257097757.47,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11071.932013437387,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15425709775747,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 380,
      "loss": 6.9034423828125,
      "time_ms": 11837.386131286621,
      "mfu": {
        "mfu": 0.3069709163540765,
        "mfu_percent": 30.69709163540765,
        "flops_achieved": 95160984069763.7,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.714748534912,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.1609840697637,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 390,
      "loss": 6.842288970947266,
      "time_ms": 11851.734161376953,
      "mfu": {
        "mfu": 0.30659928905593314,
        "mfu_percent": 30.659928905593315,
        "flops_achieved": 95045779607339.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11059.30982042647,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.04577960733928,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 400,
      "loss": 6.492591857910156,
      "time_ms": 11830.328226089478,
      "mfu": {
        "mfu": 0.3071540534221698,
        "mfu_percent": 30.71540534221698,
        "flops_achieved": 95217756560872.62,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11079.32066592593,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.21775656087263,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 410,
      "loss": 6.261391639709473,
      "time_ms": 11851.462125778198,
      "mfu": {
        "mfu": 0.30660632666195103,
        "mfu_percent": 30.660632666195102,
        "flops_achieved": 95047961265204.81,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11059.563673152561,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.04796126520482,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 420,
      "loss": 6.489388465881348,
      "time_ms": 12059.87286567688,
      "mfu": {
        "mfu": 0.30130775908093627,
        "mfu_percent": 30.130775908093625,
        "flops_achieved": 93405405315090.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10868.439614569965,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.40540531509025,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 430,
      "loss": 6.6971940994262695,
      "time_ms": 11837.675094604492,
      "mfu": {
        "mfu": 0.3069634230470064,
        "mfu_percent": 30.696342304700643,
        "flops_achieved": 95158661144571.98,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.444458265412,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15866114457198,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 440,
      "loss": 6.542629241943359,
      "time_ms": 11831.964015960693,
      "mfu": {
        "mfu": 0.30711158883312834,
        "mfu_percent": 30.711158883312834,
        "flops_achieved": 95204592538269.78,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11077.788930323894,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.20459253826978,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 450,
      "loss": 5.755655288696289,
      "time_ms": 11830.967664718628,
      "mfu": {
        "mfu": 0.30713745239912377,
        "mfu_percent": 30.713745239912377,
        "flops_achieved": 95212610243728.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11078.72185221776,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.21261024372838,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 460,
      "loss": 6.708904266357422,
      "time_ms": 11827.909708023071,
      "mfu": {
        "mfu": 0.30721685890899786,
        "mfu_percent": 30.721685890899785,
        "flops_achieved": 95237226261789.34,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11081.586115853728,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.23722626178935,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 470,
      "loss": 5.219694137573242,
      "time_ms": 11828.728914260864,
      "mfu": {
        "mfu": 0.30719558240760897,
        "mfu_percent": 30.719558240760897,
        "flops_achieved": 95230630546358.78,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11080.818653471544,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.23063054635878,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 480,
      "loss": 6.45275354385376,
      "time_ms": 11832.371234893799,
      "mfu": {
        "mfu": 0.30710101938334805,
        "mfu_percent": 30.710101938334805,
        "flops_achieved": 95201316008837.89,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11077.40768084314,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.20131600883789,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 490,
      "loss": 6.10148286819458,
      "time_ms": 11831.602096557617,
      "mfu": {
        "mfu": 0.3071209831351004,
        "mfu_percent": 30.71209831351004,
        "flops_achieved": 95207504771881.12,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11078.127791175057,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.20750477188112,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 500,
      "loss": 5.529292583465576,
      "time_ms": 11838.241815567017,
      "mfu": {
        "mfu": 0.3069487280771554,
        "mfu_percent": 30.69487280771554,
        "flops_achieved": 95154105703918.17,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11071.914397596045,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15410570391818,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 510,
      "loss": 6.7000885009765625,
      "time_ms": 11829.700469970703,
      "mfu": {
        "mfu": 0.3071703528911996,
        "mfu_percent": 30.71703528911996,
        "flops_achieved": 95222809396271.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11079.90860231177,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.22280939627187,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 520,
      "loss": 6.782349586486816,
      "time_ms": 11842.331647872925,
      "mfu": {
        "mfu": 0.306842721180737,
        "mfu_percent": 30.684272118073704,
        "flops_achieved": 95121243566028.48,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.090634291826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12124356602848,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 530,
      "loss": 6.485640048980713,
      "time_ms": 11829.435586929321,
      "mfu": {
        "mfu": 0.30717723100611033,
        "mfu_percent": 30.717723100611032,
        "flops_achieved": 95224941611894.2,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11080.156702051378,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.22494161189421,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 540,
      "loss": 6.693891525268555,
      "time_ms": 11832.144737243652,
      "mfu": {
        "mfu": 0.30710689808588193,
        "mfu_percent": 30.710689808588192,
        "flops_achieved": 95203138406623.39,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11077.61973088691,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.2031384066234,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 550,
      "loss": 5.860162734985352,
      "time_ms": 11828.510046005249,
      "mfu": {
        "mfu": 0.30720126658600444,
        "mfu_percent": 30.720126658600442,
        "flops_achieved": 95232392641661.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11081.023686856142,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.23239264166138,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 560,
      "loss": 6.370527267456055,
      "time_ms": 11832.891702651978,
      "mfu": {
        "mfu": 0.30708751159648495,
        "mfu_percent": 30.708751159648497,
        "flops_achieved": 95197128594910.34,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11076.92044292303,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.19712859491034,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 570,
      "loss": 6.02476692199707,
      "time_ms": 11827.863931655884,
      "mfu": {
        "mfu": 0.30721804790405405,
        "mfu_percent": 30.721804790405404,
        "flops_achieved": 95237594850256.75,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11081.629003965902,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.23759485025676,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 580,
      "loss": 5.787679195404053,
      "time_ms": 11835.689067840576,
      "mfu": {
        "mfu": 0.3070149314611106,
        "mfu_percent": 30.70149314611106,
        "flops_achieved": 95174628752944.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11074.302412703895,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.17462875294429,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 590,
      "loss": 6.231873512268066,
      "time_ms": 11851.131439208984,
      "mfu": {
        "mfu": 0.30661488201337744,
        "mfu_percent": 30.661488201337743,
        "flops_achieved": 95050613424147.0,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11059.872272309261,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.050613424147,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 600,
      "loss": 6.184642791748047,
      "time_ms": 11841.463088989258,
      "mfu": {
        "mfu": 0.30686522777214115,
        "mfu_percent": 30.686522777214115,
        "flops_achieved": 95128220609363.75,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.902467118007,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12822060936375,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 610,
      "loss": 5.772585868835449,
      "time_ms": 11844.918966293335,
      "mfu": {
        "mfu": 0.30677569667622684,
        "mfu_percent": 30.677569667622684,
        "flops_achieved": 95100465969630.31,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11065.673000633176,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.1004659696303,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 620,
      "loss": 6.454985618591309,
      "time_ms": 11842.434883117676,
      "mfu": {
        "mfu": 0.3068400463099243,
        "mfu_percent": 30.68400463099243,
        "flops_achieved": 95120414356076.53,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.994149315819,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12041435607654,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 630,
      "loss": 6.127507209777832,
      "time_ms": 11841.037273406982,
      "mfu": {
        "mfu": 0.3068762629536566,
        "mfu_percent": 30.68762629536566,
        "flops_achieved": 95131641515633.55,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11069.300515957846,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.13164151563355,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 640,
      "loss": 5.972052574157715,
      "time_ms": 11839.339256286621,
      "mfu": {
        "mfu": 0.30692027564195346,
        "mfu_percent": 30.692027564195346,
        "flops_achieved": 95145285449005.58,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11070.88809288082,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.14528544900558,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 650,
      "loss": 6.339615821838379,
      "time_ms": 11843.583583831787,
      "mfu": {
        "mfu": 0.30681028611295186,
        "mfu_percent": 30.681028611295186,
        "flops_achieved": 95111188695015.08,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.9206724671,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11118869501507,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 660,
      "loss": 5.285557746887207,
      "time_ms": 11843.52993965149,
      "mfu": {
        "mfu": 0.30681167578194324,
        "mfu_percent": 30.681167578194323,
        "flops_achieved": 95111619492402.4,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.970799067103,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11161949240241,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 670,
      "loss": 6.290798187255859,
      "time_ms": 11844.752073287964,
      "mfu": {
        "mfu": 0.3067800191574131,
        "mfu_percent": 30.67800191574131,
        "flops_achieved": 95101805938798.06,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11065.828916384904,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.10180593879807,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 680,
      "loss": 6.418792724609375,
      "time_ms": 11858.771562576294,
      "mfu": {
        "mfu": 0.30641734253700975,
        "mfu_percent": 30.641734253700974,
        "flops_achieved": 94989376186473.02,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11052.74684720589,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.98937618647301,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 690,
      "loss": 6.2618303298950195,
      "time_ms": 11869.430780410767,
      "mfu": {
        "mfu": 0.3061421676560245,
        "mfu_percent": 30.614216765602446,
        "flops_achieved": 94904071973367.6,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11042.82104381285,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.90407197336759,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 700,
      "loss": 6.41489315032959,
      "time_ms": 11845.14307975769,
      "mfu": {
        "mfu": 0.3067698923930958,
        "mfu_percent": 30.67698923930958,
        "flops_achieved": 95098666641859.7,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11065.463634963646,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.09866664185971,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 710,
      "loss": 6.292115211486816,
      "time_ms": 11848.786115646362,
      "mfu": {
        "mfu": 0.30667557271202095,
        "mfu_percent": 30.667557271202096,
        "flops_achieved": 95069427540726.5,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11062.061439941006,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0694275407265,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 720,
      "loss": 6.013501167297363,
      "time_ms": 11843.694686889648,
      "mfu": {
        "mfu": 0.3068074079940986,
        "mfu_percent": 30.68074079940986,
        "flops_achieved": 95110296478170.56,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.816856152993,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11029647817057,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 730,
      "loss": 5.9621992111206055,
      "time_ms": 11847.63240814209,
      "mfu": {
        "mfu": 0.30670543639257974,
        "mfu_percent": 30.670543639257975,
        "flops_achieved": 95078685281699.72,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11063.138649534985,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.07868528169972,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 740,
      "loss": 6.087412357330322,
      "time_ms": 11842.772960662842,
      "mfu": {
        "mfu": 0.30683128689775285,
        "mfu_percent": 30.683128689775284,
        "flops_achieved": 95117698938303.39,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.67818950604,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11769893830339,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 750,
      "loss": 6.030856132507324,
      "time_ms": 11846.276998519897,
      "mfu": {
        "mfu": 0.3067405285569549,
        "mfu_percent": 30.67405285569549,
        "flops_achieved": 95089563852656.02,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11064.404455203643,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.08956385265601,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 760,
      "loss": 6.124932289123535,
      "time_ms": 11843.12391281128,
      "mfu": {
        "mfu": 0.3068221944403795,
        "mfu_percent": 30.68221944403795,
        "flops_achieved": 95114880276517.64,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.350216458775,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11488027651764,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 770,
      "loss": 6.3108696937561035,
      "time_ms": 11849.234819412231,
      "mfu": {
        "mfu": 0.30666395960058607,
        "mfu_percent": 30.66639596005861,
        "flops_achieved": 95065827476181.69,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11061.642544653503,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.06582747618168,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 780,
      "loss": 5.419076919555664,
      "time_ms": 11842.867612838745,
      "mfu": {
        "mfu": 0.30682883459904536,
        "mfu_percent": 30.682883459904538,
        "flops_achieved": 95116938725704.06,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.58973290439,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.11693872570406,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 790,
      "loss": 5.774875640869141,
      "time_ms": 11834.614753723145,
      "mfu": {
        "mfu": 0.30704280144099544,
        "mfu_percent": 30.704280144099545,
        "flops_achieved": 95183268446708.58,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11075.307707736327,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.18326844670858,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 800,
      "loss": 6.237214088439941,
      "time_ms": 11840.2099609375,
      "mfu": {
        "mfu": 0.30689770535710786,
        "mfu_percent": 30.689770535710785,
        "flops_achieved": 95138288660703.44,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11070.07396257539,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.13828866070344,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 810,
      "loss": 5.139606952667236,
      "time_ms": 11837.128162384033,
      "mfu": {
        "mfu": 0.3069776062326798,
        "mfu_percent": 30.69776062326798,
        "flops_achieved": 95163057932130.73,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.956058423017,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.16305793213074,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 820,
      "loss": 5.412445068359375,
      "time_ms": 11844.508409500122,
      "mfu": {
        "mfu": 0.3067863301986921,
        "mfu_percent": 30.67863301986921,
        "flops_achieved": 95103762361594.56,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.056561272826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.10376236159456,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 830,
      "loss": 5.957972526550293,
      "time_ms": 11840.301513671875,
      "mfu": {
        "mfu": 0.30689533233273286,
        "mfu_percent": 30.689533233273288,
        "flops_achieved": 95137553023147.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11069.98836546962,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.13755302314719,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 840,
      "loss": 4.885359764099121,
      "time_ms": 11848.820686340332,
      "mfu": {
        "mfu": 0.3066746779404945,
        "mfu_percent": 30.66746779404945,
        "flops_achieved": 95069150161553.3,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11062.029164733976,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0691501615533,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 850,
      "loss": 4.655833721160889,
      "time_ms": 11848.268032073975,
      "mfu": {
        "mfu": 0.3066889825687059,
        "mfu_percent": 30.66889825687059,
        "flops_achieved": 95073584596298.83,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11062.545145432245,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.07358459629883,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 860,
      "loss": 5.451956272125244,
      "time_ms": 11839.812755584717,
      "mfu": {
        "mfu": 0.30690800124724066,
        "mfu_percent": 30.690800124724067,
        "flops_achieved": 95141480386644.61,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11070.445344515663,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.14148038664462,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 870,
      "loss": 5.711126804351807,
      "time_ms": 11840.480089187622,
      "mfu": {
        "mfu": 0.306890703804849,
        "mfu_percent": 30.6890703804849,
        "flops_achieved": 95136118179503.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11069.821410340539,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.13611817950319,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 880,
      "loss": 5.861025810241699,
      "time_ms": 11843.825101852417,
      "mfu": {
        "mfu": 0.30680402967025927,
        "mfu_percent": 30.680402967025927,
        "flops_achieved": 95109249197780.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.69499699889,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.10924919778037,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 890,
      "loss": 5.509217262268066,
      "time_ms": 11846.288442611694,
      "mfu": {
        "mfu": 0.3067402322306596,
        "mfu_percent": 30.67402322306596,
        "flops_achieved": 95089471991504.48,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11064.393766449872,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.08947199150448,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 900,
      "loss": 5.8010101318359375,
      "time_ms": 12957.738399505615,
      "mfu": {
        "mfu": 0.2804295900970443,
        "mfu_percent": 28.042959009704433,
        "flops_achieved": 86933172930083.73,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10115.345437518701,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.93317293008373,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 910,
      "loss": 5.647893905639648,
      "time_ms": 11866.851568222046,
      "mfu": {
        "mfu": 0.3062087064178654,
        "mfu_percent": 30.620870641786542,
        "flops_achieved": 94924698989538.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11045.221156300171,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.92469898953829,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 920,
      "loss": 5.296248435974121,
      "time_ms": 11948.378562927246,
      "mfu": {
        "mfu": 0.3041193622064029,
        "mfu_percent": 30.41193622064029,
        "flops_achieved": 94277002283984.89,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10969.856647050236,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.27700228398488,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 930,
      "loss": 5.148622512817383,
      "time_ms": 11988.987922668457,
      "mfu": {
        "mfu": 0.3030892425112486,
        "mfu_percent": 30.308924251124857,
        "flops_achieved": 93957665178487.06,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10932.69931085447,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.95766517848706,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 940,
      "loss": 5.497549057006836,
      "time_ms": 12012.382984161377,
      "mfu": {
        "mfu": 0.30249895235185703,
        "mfu_percent": 30.249895235185704,
        "flops_achieved": 93774675229075.67,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10911.407018309495,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.77467522907567,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 950,
      "loss": 4.805819511413574,
      "time_ms": 11962.804555892944,
      "mfu": {
        "mfu": 0.3037526234739071,
        "mfu_percent": 30.37526234739071,
        "flops_achieved": 94163313276911.2,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10956.628053864944,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.1633132769112,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 960,
      "loss": 5.686867713928223,
      "time_ms": 12012.676000595093,
      "mfu": {
        "mfu": 0.3024915737158048,
        "mfu_percent": 30.24915737158048,
        "flops_achieved": 93772387851899.5,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10911.14086432589,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7723878518995,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 970,
      "loss": 5.740520477294922,
      "time_ms": 11884.263515472412,
      "mfu": {
        "mfu": 0.30576007198319394,
        "mfu_percent": 30.576007198319395,
        "flops_achieved": 94785622314790.12,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11029.038512092413,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.78562231479013,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 980,
      "loss": 5.568741798400879,
      "time_ms": 11982.827186584473,
      "mfu": {
        "mfu": 0.3032450699135745,
        "mfu_percent": 30.324506991357453,
        "flops_achieved": 94005971673208.11,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10938.320144243033,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.00597167320811,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 990,
      "loss": 5.475390911102295,
      "time_ms": 11857.243061065674,
      "mfu": {
        "mfu": 0.3064568423911104,
        "mfu_percent": 30.645684239111038,
        "flops_achieved": 95001621141244.22,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11054.171642174286,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.00162114124421,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 28.114419712,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 28.114419712
      }
    },
    {
      "iter": 1000,
      "loss": 5.8704729080200195,
      "time_ms": 231272.90272712708,
      "mfu": {
        "mfu": 0.01571188507218002,
        "mfu_percent": 1.571188507218002,
        "flops_achieved": 4870684372375.807,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 566.7417084077873,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 4.870684372375806,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1010,
      "loss": 5.10884952545166,
      "time_ms": 11837.714433670044,
      "mfu": {
        "mfu": 0.3069624029468605,
        "mfu_percent": 30.69624029468605,
        "flops_achieved": 95158344913526.75,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.407662343294,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15834491352675,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1020,
      "loss": 5.40434455871582,
      "time_ms": 11841.60852432251,
      "mfu": {
        "mfu": 0.30686145893899885,
        "mfu_percent": 30.686145893899884,
        "flops_achieved": 95127052271089.64,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.76652194504,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12705227108964,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1030,
      "loss": 4.877228736877441,
      "time_ms": 11859.134674072266,
      "mfu": {
        "mfu": 0.3064079604309204,
        "mfu_percent": 30.64079604309204,
        "flops_achieved": 94986467733585.31,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11052.408426271093,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.98646773358531,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1040,
      "loss": 4.889911651611328,
      "time_ms": 11885.07056236267,
      "mfu": {
        "mfu": 0.3057393095725743,
        "mfu_percent": 30.573930957257428,
        "flops_achieved": 94779185967498.03,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11028.289593422807,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.77918596749804,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1050,
      "loss": 5.066821098327637,
      "time_ms": 11892.555713653564,
      "mfu": {
        "mfu": 0.30554687784950096,
        "mfu_percent": 30.554687784950097,
        "flops_achieved": 94719532133345.3,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11021.348409536506,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.7195321333453,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1060,
      "loss": 4.585180282592773,
      "time_ms": 11917.274475097656,
      "mfu": {
        "mfu": 0.30491311377875385,
        "mfu_percent": 30.491311377875384,
        "flops_achieved": 94523065271413.69,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10998.487974233381,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.52306527141369,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1070,
      "loss": 4.567850112915039,
      "time_ms": 11908.38885307312,
      "mfu": {
        "mfu": 0.3051406292481251,
        "mfu_percent": 30.514062924812507,
        "flops_achieved": 94593595066918.77,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11006.694660140789,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.59359506691877,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1080,
      "loss": 5.745540618896484,
      "time_ms": 11887.043952941895,
      "mfu": {
        "mfu": 0.30568855321333166,
        "mfu_percent": 30.568855321333167,
        "flops_achieved": 94763451496132.81,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11026.458766273958,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.76345149613282,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1090,
      "loss": 5.475189208984375,
      "time_ms": 11965.886116027832,
      "mfu": {
        "mfu": 0.3036743984292854,
        "mfu_percent": 30.36743984292854,
        "flops_achieved": 94139063513078.47,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10953.806406734411,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.13906351307847,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1100,
      "loss": 5.2630109786987305,
      "time_ms": 12016.329288482666,
      "mfu": {
        "mfu": 0.3023996081266621,
        "mfu_percent": 30.23996081266621,
        "flops_achieved": 93743878519265.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10907.82358349892,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.74387851926525,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1110,
      "loss": 5.428476333618164,
      "time_ms": 12030.189037322998,
      "mfu": {
        "mfu": 0.3020512193685929,
        "mfu_percent": 30.205121936859292,
        "flops_achieved": 93635878004263.8,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10895.25689025803,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.6358780042638,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1120,
      "loss": 4.526187896728516,
      "time_ms": 12027.956485748291,
      "mfu": {
        "mfu": 0.30210728416449095,
        "mfu_percent": 30.210728416449093,
        "flops_achieved": 93653258090992.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10897.279197451775,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.65325809099218,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1130,
      "loss": 4.948567867279053,
      "time_ms": 12028.265237808228,
      "mfu": {
        "mfu": 0.3020995294097974,
        "mfu_percent": 30.20995294097974,
        "flops_achieved": 93650854117037.19,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10896.999476533305,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.65085411703718,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1140,
      "loss": 5.50996208190918,
      "time_ms": 11975.010395050049,
      "mfu": {
        "mfu": 0.3034430157538835,
        "mfu_percent": 30.34430157538835,
        "flops_achieved": 94067334883703.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10945.460227256212,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.06733488370388,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1150,
      "loss": 4.453579902648926,
      "time_ms": 11841.922044754028,
      "mfu": {
        "mfu": 0.3068533346381751,
        "mfu_percent": 30.68533346381751,
        "flops_achieved": 95124533737834.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.473471168043,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12453373783428,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1160,
      "loss": 5.340365409851074,
      "time_ms": 11825.982809066772,
      "mfu": {
        "mfu": 0.30726691613082435,
        "mfu_percent": 30.726691613082437,
        "flops_achieved": 95252744000555.55,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11083.391724492396,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.25274400055555,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1170,
      "loss": 5.821320533752441,
      "time_ms": 11832.71312713623,
      "mfu": {
        "mfu": 0.3070921460628304,
        "mfu_percent": 30.70921460628304,
        "flops_achieved": 95198565279477.44,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11077.087612257716,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.19856527947744,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1180,
      "loss": 5.041683197021484,
      "time_ms": 11833.115339279175,
      "mfu": {
        "mfu": 0.307081707882638,
        "mfu_percent": 30.7081707882638,
        "flops_achieved": 95195329443617.78,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11076.71109778808,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.19532944361778,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1190,
      "loss": 4.878499984741211,
      "time_ms": 11837.857007980347,
      "mfu": {
        "mfu": 0.30695870591344815,
        "mfu_percent": 30.695870591344814,
        "flops_achieved": 95157198833168.92,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11072.274307050628,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15719883316892,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1200,
      "loss": 5.543933868408203,
      "time_ms": 11826.529026031494,
      "mfu": {
        "mfu": 0.3072527247817041,
        "mfu_percent": 30.725272478170414,
        "flops_achieved": 95248344682328.28,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11082.879829871983,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.24834468232828,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1210,
      "loss": 4.727662086486816,
      "time_ms": 11830.329179763794,
      "mfu": {
        "mfu": 0.30715402866166414,
        "mfu_percent": 30.715402866166414,
        "flops_achieved": 95217748885115.89,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11079.319772792409,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.2177488851159,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1220,
      "loss": 5.262688636779785,
      "time_ms": 11830.243110656738,
      "mfu": {
        "mfu": 0.3071562633133723,
        "mfu_percent": 30.71562633133723,
        "flops_achieved": 95218441627145.42,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11079.40037867267,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.21844162714542,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1230,
      "loss": 5.006972312927246,
      "time_ms": 11837.04948425293,
      "mfu": {
        "mfu": 0.30697964664185284,
        "mfu_percent": 30.697964664185285,
        "flops_achieved": 95163690458974.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11073.029657801784,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.16369045897437,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1240,
      "loss": 5.1920552253723145,
      "time_ms": 11842.125654220581,
      "mfu": {
        "mfu": 0.3068480587066743,
        "mfu_percent": 30.684805870667432,
        "flops_achieved": 95122898199069.03,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.283163613065,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12289819906903,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1250,
      "loss": 5.272397994995117,
      "time_ms": 11846.888065338135,
      "mfu": {
        "mfu": 0.3067247067683319,
        "mfu_percent": 30.672470676833193,
        "flops_achieved": 95084659098182.89,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11063.833749176132,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0846590981829,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1260,
      "loss": 5.361639976501465,
      "time_ms": 11850.931882858276,
      "mfu": {
        "mfu": 0.3066200450627926,
        "mfu_percent": 30.66200450627926,
        "flops_achieved": 95052213969465.7,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11060.058508106731,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0522139694657,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1270,
      "loss": 5.3174848556518555,
      "time_ms": 11842.448234558105,
      "mfu": {
        "mfu": 0.3068397003716083,
        "mfu_percent": 30.68397003716083,
        "flops_achieved": 95120307115198.56,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11067.981671012209,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12030711519856,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1280,
      "loss": 5.233241558074951,
      "time_ms": 11855.06820678711,
      "mfu": {
        "mfu": 0.30651306298497316,
        "mfu_percent": 30.651306298497317,
        "flops_achieved": 95019049525341.69,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11056.199569139582,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.01904952534169,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1290,
      "loss": 4.49349308013916,
      "time_ms": 11841.246843338013,
      "mfu": {
        "mfu": 0.30687083176569874,
        "mfu_percent": 30.687083176569875,
        "flops_achieved": 95129957847366.61,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11069.104608163982,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.12995784736661,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1300,
      "loss": 4.575008392333984,
      "time_ms": 11838.16647529602,
      "mfu": {
        "mfu": 0.3069506815553738,
        "mfu_percent": 30.69506815553738,
        "flops_achieved": 95154711282165.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11071.984861297742,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.15471128216588,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1310,
      "loss": 5.310047149658203,
      "time_ms": 11855.912685394287,
      "mfu": {
        "mfu": 0.30649123052623467,
        "mfu_percent": 30.649123052623466,
        "flops_achieved": 95012281463132.75,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11055.412052879925,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.01228146313275,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1320,
      "loss": 4.5998992919921875,
      "time_ms": 11967.175483703613,
      "mfu": {
        "mfu": 0.3036416799357837,
        "mfu_percent": 30.364167993578373,
        "flops_achieved": 94128920780092.95,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10952.626221491299,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.12892078009295,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1330,
      "loss": 4.519414901733398,
      "time_ms": 12003.116846084595,
      "mfu": {
        "mfu": 0.3027324747857812,
        "mfu_percent": 30.27324747857812,
        "flops_achieved": 93847067183592.17,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10919.830380786101,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.84706718359217,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1340,
      "loss": 5.1872053146362305,
      "time_ms": 11940.810680389404,
      "mfu": {
        "mfu": 0.30431210788106977,
        "mfu_percent": 30.431210788106977,
        "flops_achieved": 94336753443131.62,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10976.809155450539,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.33675344313163,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1350,
      "loss": 4.490166187286377,
      "time_ms": 12000.42724609375,
      "mfu": {
        "mfu": 0.30280032480851077,
        "mfu_percent": 30.280032480851077,
        "flops_achieved": 93868100690638.34,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10922.277791623223,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.86810069063834,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1360,
      "loss": 4.8326311111450195,
      "time_ms": 12011.392593383789,
      "mfu": {
        "mfu": 0.30252389468642066,
        "mfu_percent": 30.252389468642065,
        "flops_achieved": 93782407352790.4,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10912.306710563946,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7824073527904,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1370,
      "loss": 4.980451583862305,
      "time_ms": 12018.849849700928,
      "mfu": {
        "mfu": 0.3023361896852809,
        "mfu_percent": 30.23361896852809,
        "flops_achieved": 93724218802437.1,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10905.536023753682,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7242188024371,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1380,
      "loss": 4.41048526763916,
      "time_ms": 12023.882150650024,
      "mfu": {
        "mfu": 0.30220965428887264,
        "mfu_percent": 30.220965428887265,
        "flops_achieved": 93684992829550.52,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10900.9717791449,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.68499282955051,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1390,
      "loss": 4.89534330368042,
      "time_ms": 11992.439270019531,
      "mfu": {
        "mfu": 0.30300201536498356,
        "mfu_percent": 30.300201536498356,
        "flops_achieved": 93930624763144.9,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10929.552949888444,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.9306247631449,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1400,
      "loss": 5.069292068481445,
      "time_ms": 11860.203504562378,
      "mfu": {
        "mfu": 0.3063803472309954,
        "mfu_percent": 30.638034723099537,
        "flops_achieved": 94977907641608.58,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11051.412393520843,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.97790764160858,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1410,
      "loss": 4.403398513793945,
      "time_ms": 11938.708066940308,
      "mfu": {
        "mfu": 0.3043657025185436,
        "mfu_percent": 30.436570251854363,
        "flops_achieved": 94353367780748.52,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10978.742361826724,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.35336778074851,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1420,
      "loss": 4.720643520355225,
      "time_ms": 12002.567291259766,
      "mfu": {
        "mfu": 0.3027463358280161,
        "mfu_percent": 30.27463358280161,
        "flops_achieved": 93851364106684.98,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10920.33036094255,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.85136410668498,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1430,
      "loss": 4.98000431060791,
      "time_ms": 11880.309343338013,
      "mfu": {
        "mfu": 0.30586183936327704,
        "mfu_percent": 30.586183936327703,
        "flops_achieved": 94817170202615.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11032.709352261081,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.81717020261587,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1440,
      "loss": 4.325955390930176,
      "time_ms": 11889.77861404419,
      "mfu": {
        "mfu": 0.3056182445370286,
        "mfu_percent": 30.56182445370286,
        "flops_achieved": 94741655806478.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11023.922669609503,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.74165580647887,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1450,
      "loss": 4.925983428955078,
      "time_ms": 11979.834794998169,
      "mfu": {
        "mfu": 0.30332081619984025,
        "mfu_percent": 30.332081619984024,
        "flops_achieved": 94029453021950.47,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10941.052380348792,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.02945302195047,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1460,
      "loss": 4.890460014343262,
      "time_ms": 11862.480163574219,
      "mfu": {
        "mfu": 0.3063215464095015,
        "mfu_percent": 30.632154640950148,
        "flops_achieved": 94959679386945.45,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11049.291395443515,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.95967938694545,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1470,
      "loss": 4.016759872436523,
      "time_ms": 11844.110488891602,
      "mfu": {
        "mfu": 0.306796637144352,
        "mfu_percent": 30.6796637144352,
        "flops_achieved": 95106957514749.12,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11066.428341995821,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.10695751474913,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1480,
      "loss": 4.77408504486084,
      "time_ms": 11846.463918685913,
      "mfu": {
        "mfu": 0.30673568863249173,
        "mfu_percent": 30.673568863249173,
        "flops_achieved": 95088063476072.44,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11064.229874811399,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.08806347607243,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1490,
      "loss": 4.440621376037598,
      "time_ms": 11872.830629348755,
      "mfu": {
        "mfu": 0.30605450219897623,
        "mfu_percent": 30.605450219897623,
        "flops_achieved": 94876895681682.62,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11039.658872585933,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.87689568168263,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1500,
      "loss": 4.55401086807251,
      "time_ms": 11854.541301727295,
      "mfu": {
        "mfu": 0.3065266867330099,
        "mfu_percent": 30.652668673300994,
        "flops_achieved": 95023272887233.08,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11056.690989882656,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.02327288723308,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1510,
      "loss": 4.852808475494385,
      "time_ms": 11854.731798171997,
      "mfu": {
        "mfu": 0.30652176108433027,
        "mfu_percent": 30.652176108433025,
        "flops_achieved": 95021745936142.39,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11056.513317341463,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0217459361424,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1520,
      "loss": 5.138853549957275,
      "time_ms": 11853.668689727783,
      "mfu": {
        "mfu": 0.3065492518031173,
        "mfu_percent": 30.654925180311732,
        "flops_achieved": 95030268058966.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11057.504932087826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.03026805896637,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1530,
      "loss": 5.109341144561768,
      "time_ms": 11859.99584197998,
      "mfu": {
        "mfu": 0.306385711797303,
        "mfu_percent": 30.6385711797303,
        "flops_achieved": 94979570657163.94,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11051.605898212358,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.97957065716393,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1540,
      "loss": 5.0895795822143555,
      "time_ms": 11990.56601524353,
      "mfu": {
        "mfu": 0.30304935257756377,
        "mfu_percent": 30.304935257756377,
        "flops_achieved": 93945299299044.77,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10931.26044536755,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.94529929904476,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1550,
      "loss": 4.562409400939941,
      "time_ms": 11992.507219314575,
      "mfu": {
        "mfu": 0.30300029856190275,
        "mfu_percent": 30.300029856190275,
        "flops_achieved": 93930092554189.84,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10929.491023270057,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.93009255418984,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1560,
      "loss": 4.87384033203125,
      "time_ms": 11863.766431808472,
      "mfu": {
        "mfu": 0.306288335061581,
        "mfu_percent": 30.6288335061581,
        "flops_achieved": 94949383869090.11,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11048.093432501928,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.94938386909011,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1570,
      "loss": 4.483970642089844,
      "time_ms": 11849.744319915771,
      "mfu": {
        "mfu": 0.3066507740467365,
        "mfu_percent": 30.665077404673653,
        "flops_achieved": 95061739954488.31,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11061.166929965597,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.06173995448832,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1580,
      "loss": 4.853498458862305,
      "time_ms": 11846.494913101196,
      "mfu": {
        "mfu": 0.30673488610875915,
        "mfu_percent": 30.673488610875914,
        "flops_achieved": 95087814693715.34,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11064.200927064572,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.08781469371534,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1590,
      "loss": 4.46720552444458,
      "time_ms": 11849.27487373352,
      "mfu": {
        "mfu": 0.3066629229787762,
        "mfu_percent": 30.666292297877618,
        "flops_achieved": 95065506123420.61,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11061.605152780228,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0655061234206,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1600,
      "loss": 4.181224346160889,
      "time_ms": 11846.287727355957,
      "mfu": {
        "mfu": 0.3067402507510363,
        "mfu_percent": 30.67402507510363,
        "flops_achieved": 95089477732821.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11064.394434496378,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.08947773282125,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1610,
      "loss": 4.721268177032471,
      "time_ms": 11875.47516822815,
      "mfu": {
        "mfu": 0.30598634719727613,
        "mfu_percent": 30.598634719727613,
        "flops_achieved": 94855767631155.61,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11037.200460885328,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.85576763115562,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1620,
      "loss": 4.634127140045166,
      "time_ms": 11886.0182762146,
      "mfu": {
        "mfu": 0.30571493190698207,
        "mfu_percent": 30.571493190698206,
        "flops_achieved": 94771628891164.44,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11027.410269281798,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.77162889116444,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1630,
      "loss": 3.8789844512939453,
      "time_ms": 11994.477033615112,
      "mfu": {
        "mfu": 0.30295053779955344,
        "mfu_percent": 30.295053779955346,
        "flops_achieved": 93914666717861.56,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10927.69610818915,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.91466671786156,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1640,
      "loss": 4.070030212402344,
      "time_ms": 12008.410453796387,
      "mfu": {
        "mfu": 0.3025990227382099,
        "mfu_percent": 30.259902273820988,
        "flops_achieved": 93805697048845.06,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10915.016646400722,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80569704884506,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1650,
      "loss": 4.6137495040893555,
      "time_ms": 12003.181219100952,
      "mfu": {
        "mfu": 0.30273085123264176,
        "mfu_percent": 30.273085123264178,
        "flops_achieved": 93846563882118.95,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10919.77181777627,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.84656388211896,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1660,
      "loss": 4.531791687011719,
      "time_ms": 11995.086908340454,
      "mfu": {
        "mfu": 0.3029351346701351,
        "mfu_percent": 30.29351346701351,
        "flops_achieved": 93909891747741.89,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10927.140503572566,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.9098917477419,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1670,
      "loss": 2.7814788818359375,
      "time_ms": 11851.131439208984,
      "mfu": {
        "mfu": 0.30661488201337744,
        "mfu_percent": 30.661488201337743,
        "flops_achieved": 95050613424147.0,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11059.872272309261,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.050613424147,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1680,
      "loss": 4.053760528564453,
      "time_ms": 11853.379487991333,
      "mfu": {
        "mfu": 0.3065567310689266,
        "mfu_percent": 30.65567310689266,
        "flops_achieved": 95032586631367.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11057.774715876525,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.03258663136725,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1690,
      "loss": 4.587259769439697,
      "time_ms": 11848.999500274658,
      "mfu": {
        "mfu": 0.3066700498952558,
        "mfu_percent": 30.66700498952558,
        "flops_achieved": 95067715467529.3,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11061.862227014337,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0677154675293,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1700,
      "loss": 4.7126383781433105,
      "time_ms": 11842.305421829224,
      "mfu": {
        "mfu": 0.30684340071654775,
        "mfu_percent": 30.684340071654777,
        "flops_achieved": 95121454222129.8,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11068.115145754613,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.1214542221298,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1710,
      "loss": 4.269444942474365,
      "time_ms": 11852.96654701233,
      "mfu": {
        "mfu": 0.3065674110819129,
        "mfu_percent": 30.656741108191287,
        "flops_achieved": 95035897435393.0,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11058.159953470731,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.035897435393,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1720,
      "loss": 5.06351375579834,
      "time_ms": 11848.113059997559,
      "mfu": {
        "mfu": 0.3066929940284381,
        "mfu_percent": 30.66929940284381,
        "flops_achieved": 95074828148815.8,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11062.689842362714,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.0748281488158,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1730,
      "loss": 4.915097236633301,
      "time_ms": 11856.797456741333,
      "mfu": {
        "mfu": 0.3064683597080496,
        "mfu_percent": 30.646835970804958,
        "flops_achieved": 95005191509495.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11054.587082068889,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.00519150949538,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1740,
      "loss": 4.910674571990967,
      "time_ms": 11878.909826278687,
      "mfu": {
        "mfu": 0.30589787456080325,
        "mfu_percent": 30.589787456080327,
        "flops_achieved": 94828341113849.0,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11034.009173976616,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.828341113849,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1750,
      "loss": 4.47672176361084,
      "time_ms": 12018.409967422485,
      "mfu": {
        "mfu": 0.30234725540298696,
        "mfu_percent": 30.234725540298697,
        "flops_achieved": 93727649174925.95,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10905.93517406115,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.72764917492596,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1760,
      "loss": 3.576889991760254,
      "time_ms": 12271.507978439331,
      "mfu": {
        "mfu": 0.2961113886200824,
        "mfu_percent": 29.61113886200824,
        "flops_achieved": 91794530472225.55,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10681.001897263934,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 91.79453047222555,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1770,
      "loss": 3.723506450653076,
      "time_ms": 12069.936990737915,
      "mfu": {
        "mfu": 0.301056523389186,
        "mfu_percent": 30.1056523389186,
        "flops_achieved": 93327522250647.66,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10859.377319084639,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.32752225064766,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1780,
      "loss": 4.715398788452148,
      "time_ms": 12046.597003936768,
      "mfu": {
        "mfu": 0.30163981303355664,
        "mfu_percent": 30.163981303355662,
        "flops_achieved": 93508342040402.56,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10880.41709680886,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50834204040257,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1790,
      "loss": 4.573090553283691,
      "time_ms": 11944.219589233398,
      "mfu": {
        "mfu": 0.3042252564774983,
        "mfu_percent": 30.422525647749833,
        "flops_achieved": 94309829508024.48,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10973.676347858607,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.30982950802448,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1800,
      "loss": 4.323408603668213,
      "time_ms": 11841.158866882324,
      "mfu": {
        "mfu": 0.30687311172903986,
        "mfu_percent": 30.687311172903986,
        "flops_achieved": 95130664636002.36,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11069.186848475258,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.13066463600236,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1810,
      "loss": 3.8170132637023926,
      "time_ms": 11961.811780929565,
      "mfu": {
        "mfu": 0.3037778335344873,
        "mfu_percent": 30.377783353448727,
        "flops_achieved": 94171128395691.06,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10957.537403235603,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.17112839569106,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1820,
      "loss": 4.0694260597229,
      "time_ms": 12122.214555740356,
      "mfu": {
        "mfu": 0.2997582043486742,
        "mfu_percent": 29.975820434867416,
        "flops_achieved": 92925043348089.0,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10812.54579328759,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 92.925043348089,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1830,
      "loss": 3.760239601135254,
      "time_ms": 12096.834659576416,
      "mfu": {
        "mfu": 0.30038711532536805,
        "mfu_percent": 30.038711532536805,
        "flops_achieved": 93120005750864.1,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10835.23117315961,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.1200057508641,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1840,
      "loss": 4.500654220581055,
      "time_ms": 12075.650453567505,
      "mfu": {
        "mfu": 0.3009140817656392,
        "mfu_percent": 30.09140817656392,
        "flops_achieved": 93283365347348.16,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10854.239322675778,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.28336534734815,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1850,
      "loss": 4.659797668457031,
      "time_ms": 12036.635160446167,
      "mfu": {
        "mfu": 0.30188945826811925,
        "mfu_percent": 30.188945826811924,
        "flops_achieved": 93585732063116.97,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10889.422023084855,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.58573206311696,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1860,
      "loss": 4.890393257141113,
      "time_ms": 11868.0739402771,
      "mfu": {
        "mfu": 0.3061771679418142,
        "mfu_percent": 30.617716794181423,
        "flops_achieved": 94914922061962.4,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11044.083535338987,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.9149220619624,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1870,
      "loss": 4.7559661865234375,
      "time_ms": 11845.383644104004,
      "mfu": {
        "mfu": 0.3067636622952915,
        "mfu_percent": 30.67636622952915,
        "flops_achieved": 95096735311540.38,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11065.238909779051,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.09673531154037,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1880,
      "loss": 3.9215729236602783,
      "time_ms": 11857.721328735352,
      "mfu": {
        "mfu": 0.30644448180379313,
        "mfu_percent": 30.644448180379314,
        "flops_achieved": 94997789359175.88,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11053.725784765013,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.99778935917587,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1890,
      "loss": 3.8813018798828125,
      "time_ms": 11855.124950408936,
      "mfu": {
        "mfu": 0.30651159588433924,
        "mfu_percent": 30.651159588433924,
        "flops_achieved": 95018594724145.16,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11056.146649511167,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.01859472414516,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1900,
      "loss": 4.364362716674805,
      "time_ms": 11858.0641746521,
      "mfu": {
        "mfu": 0.30643562173711203,
        "mfu_percent": 30.643562173711203,
        "flops_achieved": 94995042738504.73,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11053.406194257292,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.99504273850474,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1910,
      "loss": 3.991619110107422,
      "time_ms": 11860.984086990356,
      "mfu": {
        "mfu": 0.3063601840545193,
        "mfu_percent": 30.63601840545193,
        "flops_achieved": 94971657056900.98,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11050.685089761268,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.97165705690098,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1920,
      "loss": 3.9593071937561035,
      "time_ms": 12004.25100326538,
      "mfu": {
        "mfu": 0.3027038727338879,
        "mfu_percent": 30.27038727338879,
        "flops_achieved": 93838200547505.25,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10918.798679263367,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.83820054750525,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1930,
      "loss": 4.82370662689209,
      "time_ms": 12126.665353775024,
      "mfu": {
        "mfu": 0.2996481853790837,
        "mfu_percent": 29.96481853790837,
        "flops_achieved": 92890937467515.95,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10808.577310925575,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 92.89093746751595,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1940,
      "loss": 4.6938982009887695,
      "time_ms": 12061.051368713379,
      "mfu": {
        "mfu": 0.3012783178574358,
        "mfu_percent": 30.12783178574358,
        "flops_achieved": 93396278535805.1,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10867.377643378879,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.3962785358051,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1950,
      "loss": 4.554157733917236,
      "time_ms": 12043.254613876343,
      "mfu": {
        "mfu": 0.3017235277722412,
        "mfu_percent": 30.17235277722412,
        "flops_achieved": 93534293609394.77,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10883.436762100646,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.53429360939477,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1960,
      "loss": 3.8979787826538086,
      "time_ms": 12054.70609664917,
      "mfu": {
        "mfu": 0.3014369026357394,
        "mfu_percent": 30.14369026357394,
        "flops_achieved": 93445439817079.22,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 10873.097937778333,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44543981707922,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1970,
      "loss": 4.34737491607666,
      "time_ms": 11858.983516693115,
      "mfu": {
        "mfu": 0.3064118659784898,
        "mfu_percent": 30.64118659784898,
        "flops_achieved": 94987678453331.83,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11052.549302855386,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.98767845333182,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1980,
      "loss": 4.620848178863525,
      "time_ms": 11855.241537094116,
      "mfu": {
        "mfu": 0.3065085815913936,
        "mfu_percent": 30.65085815913936,
        "flops_achieved": 95017660293332.02,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11056.03792127609,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.01766029333201,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 1990,
      "loss": 4.211699485778809,
      "time_ms": 11856.987476348877,
      "mfu": {
        "mfu": 0.30646344825836197,
        "mfu_percent": 30.6463448258362,
        "flops_achieved": 95003668960092.22,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 11054.409921698003,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 95.00366896009221,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    },
    {
      "iter": 2000,
      "loss": 4.233574867248535,
      "time_ms": 225531.1734676361,
      "mfu": {
        "mfu": 0.01611188915522374,
        "mfu_percent": 1.6111889155223742,
        "flops_achieved": 4994685638119.359,
        "flops_per_token": 8594187264.0,
        "tokens_per_sec": 581.1702124576092,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 4.99468563811936,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.262495232,
        "non_attn_gflops": 7.574971392,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-AbsPos-LN-NB-GELU-PostNorm"
      },
      "memory": {
        "allocated_gb": 8.488564736,
        "reserved_gb": 30.643585024,
        "max_allocated_gb": 26.576228352,
        "max_reserved_gb": 30.643585024
      }
    }
  ],
  "eval_steps": [
    {
      "iter": 1000,
      "train_loss": 5.3566060066223145,
      "val_loss": 5.377431869506836,
      "timestamp": "2025-11-13T13:12:44.877440",
      "lr": 0.00015007496251874061
    },
    {
      "iter": 2000,
      "train_loss": 4.481794834136963,
      "val_loss": 4.500303745269775,
      "timestamp": "2025-11-13T16:34:53.477720",
      "lr": 0.0003
    }
  ],
  "checkpoints": [
    {
      "iter": 1000,
      "val_loss": 5.377431869506836,
      "path": "out-gpt2-1.36b/ckpt.pt",
      "timestamp": "2025-11-13T13:15:51.021852"
    },
    {
      "iter": 2000,
      "val_loss": 4.500303745269775,
      "path": "out-gpt2-1.36b/ckpt.pt",
      "timestamp": "2025-11-13T16:38:04.437326"
    }
  ],
  "metadata": {
    "world_size": 2,
    "device": "cuda:0",
    "dtype": "bfloat16",
    "compile": true,
    "use_zero1": true,
    "use_fsdp": false
  },
  "end_time": "2025-11-13T16:38:15.963462",
  "summary": {
    "total_iterations": 201,
    "final_iter": 2000,
    "final_train_loss": 4.233574867248535,
    "best_val_loss": 4.500303745269775,
    "avg_time_ms": 14157.002705246656,
    "avg_mfu": 30.278602583492123,
    "total_eval_steps": 2,
    "total_checkpoints": 2
  }
}