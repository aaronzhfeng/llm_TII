{
  "run_name": "run_20251113_225009",
  "start_time": "2025-11-13T22:50:09.271150",
  "config": {
    "MODULAR_ARCH_AVAILABLE": true,
    "LOGGER_AVAILABLE": true,
    "out_dir": "out-qwen3-1.8b-optimal",
    "eval_interval": 1000,
    "log_interval": 10,
    "eval_iters": 50,
    "eval_only": false,
    "eval_at_start": false,
    "always_save_checkpoint": false,
    "init_from": "scratch",
    "save_log_to_json": true,
    "log_save_interval": 10,
    "gradient_log_interval": 10,
    "wandb_log": false,
    "wandb_project": "qwen3-1.8b",
    "wandb_run_name": "optimal-1.8b",
    "dataset": "slimpajama_6b_qwen3",
    "gradient_accumulation_steps": 96,
    "batch_size": 2,
    "block_size": 2048,
    "n_layer": 24,
    "n_head": 16,
    "n_embd": 2048,
    "dropout": 0.0,
    "bias": false,
    "arch_preset": "custom",
    "normalization": "rmsnorm",
    "activation": "silu",
    "attention_backend": "flash_attn_2",
    "position_encoding": "rope",
    "norm_position": "pre",
    "ffn_type": "swiglu",
    "weight_tying": false,
    "rope_theta": 1000000,
    "d_ff": 6144,
    "intermediate_size": 6144,
    "learning_rate": 0.0003,
    "max_iters": 2000,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.95,
    "grad_clip": 1.0,
    "decay_lr": true,
    "warmup_iters": 2000,
    "lr_decay_iters": 25000,
    "min_lr": 3e-05,
    "backend": "nccl",
    "use_zero1": true,
    "use_fsdp": false,
    "fsdp_min_num_params": 1000000.0,
    "fsdp_activation_checkpointing": false,
    "device": "cuda",
    "dtype": "bfloat16",
    "compile": true
  },
  "startup_info": {
    "timestamp": "2025-11-13T22:50:09.272919",
    "model": {
      "total_params": 1829140480,
      "trainable_params": 1829140480,
      "non_embedding_params": 1829140480
    },
    "optimizer": {
      "type": "ZeroRedundancyOptimizer",
      "param_groups": 2
    },
    "config": {
      "MODULAR_ARCH_AVAILABLE": true,
      "LOGGER_AVAILABLE": true,
      "out_dir": "out-qwen3-1.8b-optimal",
      "eval_interval": 1000,
      "log_interval": 10,
      "eval_iters": 50,
      "eval_only": false,
      "eval_at_start": false,
      "always_save_checkpoint": false,
      "init_from": "scratch",
      "save_log_to_json": true,
      "log_save_interval": 10,
      "gradient_log_interval": 10,
      "wandb_log": false,
      "wandb_project": "qwen3-1.8b",
      "wandb_run_name": "optimal-1.8b",
      "dataset": "slimpajama_6b_qwen3",
      "gradient_accumulation_steps": 96,
      "batch_size": 2,
      "block_size": 2048,
      "n_layer": 24,
      "n_head": 16,
      "n_embd": 2048,
      "dropout": 0.0,
      "bias": false,
      "arch_preset": "custom",
      "normalization": "rmsnorm",
      "activation": "silu",
      "attention_backend": "flash_attn_2",
      "position_encoding": "rope",
      "norm_position": "pre",
      "ffn_type": "swiglu",
      "weight_tying": false,
      "rope_theta": 1000000,
      "d_ff": 6144,
      "intermediate_size": 6144,
      "learning_rate": 0.0003,
      "max_iters": 2000,
      "weight_decay": 0.1,
      "beta1": 0.9,
      "beta2": 0.95,
      "grad_clip": 1.0,
      "decay_lr": true,
      "warmup_iters": 2000,
      "lr_decay_iters": 25000,
      "min_lr": 3e-05,
      "backend": "nccl",
      "use_zero1": true,
      "use_fsdp": false,
      "fsdp_min_num_params": 1000000.0,
      "fsdp_activation_checkpointing": false,
      "device": "cuda",
      "dtype": "bfloat16",
      "compile": true
    },
    "hardware": {
      "gpu_name": "NVIDIA RTX A6000",
      "num_gpus": 2,
      "gpu_memory_gb": 51.033931776,
      "precision": "bfloat16",
      "parallelism": "DDP+ZeRO-1"
    }
  },
  "training_iterations": [
    {
      "iter": 0,
      "loss": 12.17017650604248,
      "time_ms": 67057.17420578003,
      "mfu": -100.0
    },
    {
      "iter": 10,
      "loss": 8.879449367523193,
      "time_ms": 27616.575241088867,
      "mfu": {
        "mfu": 0.2797801331663549,
        "mfu_percent": 27.97801331663549,
        "flops_achieved": 86731841281570.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7119.202807865909,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.73184128157001,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 20,
      "loss": 6.46123480796814,
      "time_ms": 27653.534650802612,
      "mfu": {
        "mfu": 0.27940620235779695,
        "mfu_percent": 27.940620235779694,
        "flops_achieved": 86615922730917.05,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7109.687874721421,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.61592273091705,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 30,
      "loss": 6.925263404846191,
      "time_ms": 27705.242156982422,
      "mfu": {
        "mfu": 0.2788847343318821,
        "mfu_percent": 27.88847343318821,
        "flops_achieved": 86454267642883.45,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7096.41875302829,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.45426764288345,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 40,
      "loss": 5.866743564605713,
      "time_ms": 27757.32469558716,
      "mfu": {
        "mfu": 0.27836144813260294,
        "mfu_percent": 27.836144813260294,
        "flops_achieved": 86292048921106.9,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7083.103366631605,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.29204892110691,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 50,
      "loss": 5.545238971710205,
      "time_ms": 27734.88998413086,
      "mfu": {
        "mfu": 0.27858661429598003,
        "mfu_percent": 27.858661429598,
        "flops_achieved": 86361850431753.81,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7088.832878460801,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.36185043175381,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 60,
      "loss": 4.9614951610565186,
      "time_ms": 27808.894634246826,
      "mfu": {
        "mfu": 0.27784524340766803,
        "mfu_percent": 27.784524340766804,
        "flops_achieved": 86132025456377.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7069.968173343936,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.1320254563771,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 70,
      "loss": 4.531649708747864,
      "time_ms": 27680.57870864868,
      "mfu": {
        "mfu": 0.27913322116117356,
        "mfu_percent": 27.913322116117357,
        "flops_achieved": 86531298559963.8,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7102.74167564895,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.5312985599638,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 80,
      "loss": 4.642507553100586,
      "time_ms": 27624.069452285767,
      "mfu": {
        "mfu": 0.2797042308301598,
        "mfu_percent": 27.970423083015977,
        "flops_achieved": 86708311557349.53,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7117.271419390077,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.70831155734953,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 90,
      "loss": 4.856011390686035,
      "time_ms": 27663.41257095337,
      "mfu": {
        "mfu": 0.27930643331630794,
        "mfu_percent": 27.930643331630794,
        "flops_achieved": 86584994328055.47,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7107.149181097011,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.58499432805547,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 100,
      "loss": 4.723206639289856,
      "time_ms": 27655.797481536865,
      "mfu": {
        "mfu": 0.27938334100503864,
        "mfu_percent": 27.938334100503866,
        "flops_achieved": 86608835711561.98,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7109.10615147715,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.60883571156198,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 110,
      "loss": 5.053463101387024,
      "time_ms": 27706.485271453857,
      "mfu": {
        "mfu": 0.27887222153404023,
        "mfu_percent": 27.887222153404025,
        "flops_achieved": 86450388675552.47,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7096.100356062351,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.45038867555247,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 120,
      "loss": 5.053783535957336,
      "time_ms": 27694.724798202515,
      "mfu": {
        "mfu": 0.2789906437002033,
        "mfu_percent": 27.899064370020334,
        "flops_achieved": 86487099547063.03,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7099.113691599512,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.48709954706302,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 130,
      "loss": 4.630383610725403,
      "time_ms": 27656.967878341675,
      "mfu": {
        "mfu": 0.2793715179674928,
        "mfu_percent": 27.937151796749284,
        "flops_achieved": 86605170569922.78,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7108.805305948408,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.60517056992278,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 140,
      "loss": 4.722321510314941,
      "time_ms": 27631.025075912476,
      "mfu": {
        "mfu": 0.27963382021922123,
        "mfu_percent": 27.963382021922122,
        "flops_achieved": 86686484267958.58,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7115.479771736528,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.68648426795858,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 150,
      "loss": 4.374750137329102,
      "time_ms": 27618.326902389526,
      "mfu": {
        "mfu": 0.2797623884262882,
        "mfu_percent": 27.97623884262882,
        "flops_achieved": 86726340412149.34,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7118.751280440147,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.72634041214934,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 160,
      "loss": 4.921864628791809,
      "time_ms": 27626.237869262695,
      "mfu": {
        "mfu": 0.27968227650523425,
        "mfu_percent": 27.968227650523424,
        "flops_achieved": 86701505716622.62,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7116.712776108707,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.70150571662262,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 170,
      "loss": 4.46223521232605,
      "time_ms": 27609.2586517334,
      "mfu": {
        "mfu": 0.279854276277911,
        "mfu_percent": 27.985427627791097,
        "flops_achieved": 86754825646152.4,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7121.089431630078,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.75482564615241,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 180,
      "loss": 4.284405827522278,
      "time_ms": 27637.048721313477,
      "mfu": {
        "mfu": 0.27957287250399615,
        "mfu_percent": 27.957287250399617,
        "flops_achieved": 86667590476238.81,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7113.928914138269,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.66759047623881,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 190,
      "loss": 4.397523880004883,
      "time_ms": 27634.931564331055,
      "mfu": {
        "mfu": 0.2795942910357461,
        "mfu_percent": 27.959429103574614,
        "flops_achieved": 86674230221081.3,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7114.47392378441,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.6742302210813,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 200,
      "loss": 4.527072072029114,
      "time_ms": 27645.23959159851,
      "mfu": {
        "mfu": 0.2794900392506866,
        "mfu_percent": 27.94900392506866,
        "flops_achieved": 86641912167712.84,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7111.82116358832,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.64191216771285,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 210,
      "loss": 4.046406984329224,
      "time_ms": 27675.14657974243,
      "mfu": {
        "mfu": 0.2791880099455796,
        "mfu_percent": 27.91880099455796,
        "flops_achieved": 86548283083129.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7104.135814908836,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.54828308312968,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 220,
      "loss": 3.0834399461746216,
      "time_ms": 27664.23487663269,
      "mfu": {
        "mfu": 0.27929813107092105,
        "mfu_percent": 27.929813107092105,
        "flops_achieved": 86582420631985.53,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7106.937924607849,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.58242063198553,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 230,
      "loss": 3.96891188621521,
      "time_ms": 27691.04790687561,
      "mfu": {
        "mfu": 0.27902768882329,
        "mfu_percent": 27.902768882329,
        "flops_achieved": 86498583535219.89,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7100.0563308831215,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.4985835352199,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 240,
      "loss": 3.8460216522216797,
      "time_ms": 27646.146535873413,
      "mfu": {
        "mfu": 0.2794808704542085,
        "mfu_percent": 27.94808704542085,
        "flops_achieved": 86639069840804.64,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7111.587857095493,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.63906984080464,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 250,
      "loss": 3.8958005905151367,
      "time_ms": 27672.197103500366,
      "mfu": {
        "mfu": 0.2792177675538867,
        "mfu_percent": 27.92177675538867,
        "flops_achieved": 86557507941704.89,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7104.893018239245,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.55750794170488,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 260,
      "loss": 3.648360013961792,
      "time_ms": 27633.853435516357,
      "mfu": {
        "mfu": 0.27960519934653577,
        "mfu_percent": 27.960519934653576,
        "flops_achieved": 86677611797426.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7114.751493445715,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.6776117974261,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 270,
      "loss": 3.518878698348999,
      "time_ms": 27625.299215316772,
      "mfu": {
        "mfu": 0.27969177956510705,
        "mfu_percent": 27.969177956510705,
        "flops_achieved": 86704451665183.19,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7116.954588169356,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.70445166518319,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 280,
      "loss": 3.579153299331665,
      "time_ms": 27669.99840736389,
      "mfu": {
        "mfu": 0.2792399545817903,
        "mfu_percent": 27.92399545817903,
        "flops_achieved": 86564385920355.0,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7105.45758281201,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.564385920355,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 290,
      "loss": 3.4534817934036255,
      "time_ms": 27689.513206481934,
      "mfu": {
        "mfu": 0.27904315402488783,
        "mfu_percent": 27.904315402488784,
        "flops_achieved": 86503377747715.22,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7100.4498538448615,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.50337774771522,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 300,
      "loss": 3.5793853998184204,
      "time_ms": 27688.371658325195,
      "mfu": {
        "mfu": 0.2790546585366756,
        "mfu_percent": 27.90546585366756,
        "flops_achieved": 86506944146369.44,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7100.742594261043,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.50694414636943,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 310,
      "loss": 3.53841233253479,
      "time_ms": 27713.135719299316,
      "mfu": {
        "mfu": 0.2788052992924129,
        "mfu_percent": 27.880529929241288,
        "flops_achieved": 86429642780647.98,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7094.3974724261525,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.42964278064798,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 320,
      "loss": 3.808188557624817,
      "time_ms": 27839.977264404297,
      "mfu": {
        "mfu": 0.27753503622395403,
        "mfu_percent": 27.753503622395403,
        "flops_achieved": 86035861229425.75,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7062.074732775716,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.03586122942575,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 330,
      "loss": 3.085760235786438,
      "time_ms": 27853.88445854187,
      "mfu": {
        "mfu": 0.2773964654750702,
        "mfu_percent": 27.739646547507018,
        "flops_achieved": 85992904297271.75,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7058.548702341112,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.99290429727175,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 340,
      "loss": 3.9835338592529297,
      "time_ms": 27825.363636016846,
      "mfu": {
        "mfu": 0.2776807951055603,
        "mfu_percent": 27.76807951055603,
        "flops_achieved": 86081046482723.7,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7065.78367031699,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.0810464827237,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 350,
      "loss": 3.038498640060425,
      "time_ms": 27842.67497062683,
      "mfu": {
        "mfu": 0.277508145560791,
        "mfu_percent": 27.7508145560791,
        "flops_achieved": 86027525123845.22,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7061.39048088646,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.02752512384522,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 360,
      "loss": 3.3562281131744385,
      "time_ms": 27849.533319473267,
      "mfu": {
        "mfu": 0.2774398051815053,
        "mfu_percent": 27.74398051815053,
        "flops_achieved": 86006339606266.64,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7059.651511737381,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.00633960626664,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 370,
      "loss": 3.584150791168213,
      "time_ms": 27800.262212753296,
      "mfu": {
        "mfu": 0.27793151875401956,
        "mfu_percent": 27.793151875401957,
        "flops_achieved": 86158770813746.06,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7072.163510378928,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.15877081374606,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 380,
      "loss": 3.027571678161621,
      "time_ms": 27518.747091293335,
      "mfu": {
        "mfu": 0.28077474141237757,
        "mfu_percent": 28.077474141237758,
        "flops_achieved": 87040169837837.05,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7144.51131614945,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.04016983783704,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 390,
      "loss": 3.5355724096298218,
      "time_ms": 27536.816596984863,
      "mfu": {
        "mfu": 0.28059049859076746,
        "mfu_percent": 28.059049859076744,
        "flops_achieved": 86983054563137.9,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7139.823127613362,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.98305456313791,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 400,
      "loss": 3.510550618171692,
      "time_ms": 27528.19871902466,
      "mfu": {
        "mfu": 0.280678339233678,
        "mfu_percent": 28.0678339233678,
        "flops_achieved": 87010285162440.19,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7142.05829472325,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.01028516244018,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 410,
      "loss": 3.105275273323059,
      "time_ms": 27549.351692199707,
      "mfu": {
        "mfu": 0.2804628284860219,
        "mfu_percent": 28.04628284860219,
        "flops_achieved": 86943476830666.78,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.574471756712,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.94347683066678,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 420,
      "loss": 2.9986993074417114,
      "time_ms": 27515.170574188232,
      "mfu": {
        "mfu": 0.28081123748506714,
        "mfu_percent": 28.081123748506716,
        "flops_achieved": 87051483620370.81,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7145.4399844584805,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.05148362037082,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 430,
      "loss": 3.3471533060073853,
      "time_ms": 27497.633934020996,
      "mfu": {
        "mfu": 0.2809903250981508,
        "mfu_percent": 28.099032509815082,
        "flops_achieved": 87107000780426.75,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7149.9969950778195,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.10700078042674,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 440,
      "loss": 3.0418546199798584,
      "time_ms": 27497.16353416443,
      "mfu": {
        "mfu": 0.2809951320597292,
        "mfu_percent": 28.09951320597292,
        "flops_achieved": 87108490938516.05,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7150.119311605368,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.10849093851604,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 450,
      "loss": 2.774450361728668,
      "time_ms": 27500.925302505493,
      "mfu": {
        "mfu": 0.2809566955860415,
        "mfu_percent": 28.095669558604154,
        "flops_achieved": 87096575631672.88,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7149.141268424443,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.09657563167288,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 460,
      "loss": 3.170118570327759,
      "time_ms": 27524.927377700806,
      "mfu": {
        "mfu": 0.28071169789207684,
        "mfu_percent": 28.071169789207683,
        "flops_achieved": 87020626346543.81,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7142.90712931294,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.0206263465438,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 470,
      "loss": 3.0373034477233887,
      "time_ms": 27509.004592895508,
      "mfu": {
        "mfu": 0.28087417966937167,
        "mfu_percent": 28.087417966937167,
        "flops_achieved": 87070995697505.22,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7147.041592729099,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.07099569750523,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 480,
      "loss": 3.1938657760620117,
      "time_ms": 27517.375946044922,
      "mfu": {
        "mfu": 0.2807887319525118,
        "mfu_percent": 28.07887319525118,
        "flops_achieved": 87044506905278.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7144.86731531022,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.04450690527865,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 490,
      "loss": 2.931950569152832,
      "time_ms": 27503.411293029785,
      "mfu": {
        "mfu": 0.28093130034777375,
        "mfu_percent": 28.093130034777374,
        "flops_achieved": 87088703107809.86,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7148.495068676319,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.08870310780986,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 500,
      "loss": 3.0135576725006104,
      "time_ms": 27485.806465148926,
      "mfu": {
        "mfu": 0.2811112385713525,
        "mfu_percent": 28.11112385713525,
        "flops_achieved": 87144483957119.28,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7153.073723679613,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.14448395711928,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 510,
      "loss": 2.5570598244667053,
      "time_ms": 27571.87509536743,
      "mfu": {
        "mfu": 0.2802337190280071,
        "mfu_percent": 28.023371902800708,
        "flops_achieved": 86872452898682.2,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.744620014388,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.8724528986822,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 520,
      "loss": 2.0894322395324707,
      "time_ms": 27982.916831970215,
      "mfu": {
        "mfu": 0.27611735920692054,
        "mfu_percent": 27.611735920692055,
        "flops_achieved": 85596381354145.38,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7026.000941237736,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.59638135414538,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 530,
      "loss": 2.935418486595154,
      "time_ms": 28260.604858398438,
      "mfu": {
        "mfu": 0.27340423664903746,
        "mfu_percent": 27.340423664903746,
        "flops_achieved": 84755313361201.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6956.963624278989,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.75531336120162,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 540,
      "loss": 2.9487987756729126,
      "time_ms": 28056.416273117065,
      "mfu": {
        "mfu": 0.2753940140941629,
        "mfu_percent": 27.539401409416293,
        "flops_achieved": 85372144369190.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7007.594914692819,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.3721443691905,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 550,
      "loss": 3.2319256067276,
      "time_ms": 28259.891986846924,
      "mfu": {
        "mfu": 0.2734111334235354,
        "mfu_percent": 27.34111334235354,
        "flops_achieved": 84757451361295.97,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6957.139117570151,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.75745136129596,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 560,
      "loss": 3.065328598022461,
      "time_ms": 28276.790618896484,
      "mfu": {
        "mfu": 0.2732477388500759,
        "mfu_percent": 27.32477388500759,
        "flops_achieved": 84706799043523.53,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6952.981427411819,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.70679904352353,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 570,
      "loss": 2.7872586250305176,
      "time_ms": 28629.472970962524,
      "mfu": {
        "mfu": 0.2698816393297629,
        "mfu_percent": 26.98816393297629,
        "flops_achieved": 83663308192226.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6867.3286511215165,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.6633081922265,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 580,
      "loss": 3.5036509037017822,
      "time_ms": 28183.168172836304,
      "mfu": {
        "mfu": 0.27415544807334963,
        "mfu_percent": 27.415544807334964,
        "flops_achieved": 84988188902738.39,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6976.078728774577,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.98818890273839,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 590,
      "loss": 2.9962230920791626,
      "time_ms": 28255.083560943604,
      "mfu": {
        "mfu": 0.27345766229588414,
        "mfu_percent": 27.345766229588413,
        "flops_achieved": 84771875311724.08,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6958.323077542302,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.77187531172407,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 600,
      "loss": 2.3401883840560913,
      "time_ms": 27848.67763519287,
      "mfu": {
        "mfu": 0.27744832985485457,
        "mfu_percent": 27.744832985485456,
        "flops_achieved": 86008982255004.92,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7059.868428062917,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.00898225500492,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 610,
      "loss": 2.8459686040878296,
      "time_ms": 29304.715871810913,
      "mfu": {
        "mfu": 0.2636629930946685,
        "mfu_percent": 26.36629930946685,
        "flops_achieved": 81735527859347.23,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6709.0908118690595,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 81.73552785934723,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 620,
      "loss": 2.977013647556305,
      "time_ms": 28092.62251853943,
      "mfu": {
        "mfu": 0.2750390816468429,
        "mfu_percent": 27.503908164684287,
        "flops_achieved": 85262115310521.3,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6998.563408248932,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.2621153105213,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 630,
      "loss": 2.942858576774597,
      "time_ms": 28045.422315597534,
      "mfu": {
        "mfu": 0.2755019700399859,
        "mfu_percent": 27.55019700399859,
        "flops_achieved": 85405610712395.62,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7010.341929871955,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.40561071239563,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 640,
      "loss": 2.964176058769226,
      "time_ms": 28007.97748565674,
      "mfu": {
        "mfu": 0.2758702981144348,
        "mfu_percent": 27.587029811443482,
        "flops_achieved": 85519792415474.8,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7019.714297495619,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.5197924154748,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 650,
      "loss": 3.350395917892456,
      "time_ms": 28275.66957473755,
      "mfu": {
        "mfu": 0.27325857229049266,
        "mfu_percent": 27.325857229049266,
        "flops_achieved": 84710157410052.72,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6953.25709194368,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.71015741005272,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 660,
      "loss": 2.8986766934394836,
      "time_ms": 29959.14649963379,
      "mfu": {
        "mfu": 0.2579035119924044,
        "mfu_percent": 25.79035119924044,
        "flops_achieved": 79950088717645.36,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6562.536753255079,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 79.95008871764536,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 670,
      "loss": 2.830690026283264,
      "time_ms": 28284.887313842773,
      "mfu": {
        "mfu": 0.2731695202748458,
        "mfu_percent": 27.316952027484582,
        "flops_achieved": 84682551285202.2,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6950.9911006001785,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.68255128520221,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 680,
      "loss": 3.011366128921509,
      "time_ms": 28322.030305862427,
      "mfu": {
        "mfu": 0.27281127147693107,
        "mfu_percent": 27.281127147693105,
        "flops_achieved": 84571494157848.62,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6941.875207276498,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.57149415784862,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 690,
      "loss": 3.1153546571731567,
      "time_ms": 27693.087100982666,
      "mfu": {
        "mfu": 0.2790071424820071,
        "mfu_percent": 27.900714248200707,
        "flops_achieved": 86492214169422.2,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7099.53351473854,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.4922141694222,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 700,
      "loss": 2.646790087223053,
      "time_ms": 28192.32153892517,
      "mfu": {
        "mfu": 0.2740664364189526,
        "mfu_percent": 27.40664364189526,
        "flops_achieved": 84960595289875.31,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6973.813764451541,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.96059528987531,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 710,
      "loss": 2.520795464515686,
      "time_ms": 27596.07720375061,
      "mfu": {
        "mfu": 0.2799879505156761,
        "mfu_percent": 27.99879505156761,
        "flops_achieved": 86796264659859.6,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7124.490866885922,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.79626465985959,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 720,
      "loss": 2.7139411568641663,
      "time_ms": 28165.85659980774,
      "mfu": {
        "mfu": 0.27432395216424005,
        "mfu_percent": 27.432395216424005,
        "flops_achieved": 85040425170914.42,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6980.36643420751,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.04042517091442,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 730,
      "loss": 2.7838897705078125,
      "time_ms": 29149.104595184326,
      "mfu": {
        "mfu": 0.2650705469637992,
        "mfu_percent": 26.50705469637992,
        "flops_achieved": 82171869558777.77,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6744.907012769143,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 82.17186955877777,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 740,
      "loss": 2.7040010690689087,
      "time_ms": 28518.229484558105,
      "mfu": {
        "mfu": 0.2709343896238104,
        "mfu_percent": 27.09343896238104,
        "flops_achieved": 83989660783381.22,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6894.116624822667,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.98966078338123,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 750,
      "loss": 2.6640340089797974,
      "time_ms": 29711.4155292511,
      "mfu": {
        "mfu": 0.2600538870638338,
        "mfu_percent": 26.005388706383382,
        "flops_achieved": 80616704989788.48,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6617.254563534277,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 80.61670498978849,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 760,
      "loss": 1.8654947876930237,
      "time_ms": 28353.182315826416,
      "mfu": {
        "mfu": 0.2725115301867763,
        "mfu_percent": 27.25115301867763,
        "flops_achieved": 84478574357900.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6934.248078751136,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.47857435790065,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 770,
      "loss": 2.9308083057403564,
      "time_ms": 28282.69672393799,
      "mfu": {
        "mfu": 0.2731906781721726,
        "mfu_percent": 27.319067817217256,
        "flops_achieved": 84689110233373.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6951.529478219606,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.6891102333735,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 780,
      "loss": 2.8158563375473022,
      "time_ms": 29073.344945907593,
      "mfu": {
        "mfu": 0.2657612707765883,
        "mfu_percent": 26.57612707765883,
        "flops_achieved": 82385993940742.38,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6762.482967329662,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 82.38599394074238,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 790,
      "loss": 2.7105746269226074,
      "time_ms": 28304.956436157227,
      "mfu": {
        "mfu": 0.27297583431997285,
        "mfu_percent": 27.297583431997285,
        "flops_achieved": 84622508639191.58,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6946.062624878293,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.62250863919158,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 800,
      "loss": 2.727719306945801,
      "time_ms": 28349.024295806885,
      "mfu": {
        "mfu": 0.2725515001125927,
        "mfu_percent": 27.25515001125927,
        "flops_achieved": 84490965034903.73,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6935.265141702967,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.49096503490374,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 810,
      "loss": 2.7235084176063538,
      "time_ms": 28296.777725219727,
      "mfu": {
        "mfu": 0.2730547334251468,
        "mfu_percent": 27.30547334251468,
        "flops_achieved": 84646967361795.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6948.07026825431,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.6469673617955,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 820,
      "loss": 2.6947644352912903,
      "time_ms": 28209.87319946289,
      "mfu": {
        "mfu": 0.27389591735909025,
        "mfu_percent": 27.389591735909026,
        "flops_achieved": 84907734381317.98,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6969.474786712029,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.90773438131798,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 830,
      "loss": 2.8355332016944885,
      "time_ms": 30581.881046295166,
      "mfu": {
        "mfu": 0.25265185901592985,
        "mfu_percent": 25.265185901592986,
        "flops_achieved": 78322076294938.25,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6428.904739455784,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 78.32207629493826,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 840,
      "loss": 2.4492015838623047,
      "time_ms": 28190.251111984253,
      "mfu": {
        "mfu": 0.27408656516953767,
        "mfu_percent": 27.408656516953766,
        "flops_achieved": 84966835202556.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6974.3259547063035,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.96683520255667,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 850,
      "loss": 2.681490898132324,
      "time_ms": 30005.537509918213,
      "mfu": {
        "mfu": 0.25750477211069844,
        "mfu_percent": 25.750477211069843,
        "flops_achieved": 79826479354316.52,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6552.390535747343,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 79.82647935431652,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 860,
      "loss": 2.7224221229553223,
      "time_ms": 30252.61378288269,
      "mfu": {
        "mfu": 0.2554017036016337,
        "mfu_percent": 25.540170360163373,
        "flops_achieved": 79174528116506.45,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6498.876474311231,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 79.17452811650645,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 870,
      "loss": 2.6865981817245483,
      "time_ms": 29066.176414489746,
      "mfu": {
        "mfu": 0.2658268149332067,
        "mfu_percent": 26.58268149332067,
        "flops_achieved": 82406312629294.08,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6764.150784620889,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 82.40631262929408,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 880,
      "loss": 2.5982943177223206,
      "time_ms": 27568.05181503296,
      "mfu": {
        "mfu": 0.28027258329285265,
        "mfu_percent": 28.027258329285264,
        "flops_achieved": 86884500820784.33,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.733548643033,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.88450082078433,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 890,
      "loss": 2.518637716770172,
      "time_ms": 27642.078399658203,
      "mfu": {
        "mfu": 0.27952200217498996,
        "mfu_percent": 27.952200217498994,
        "flops_achieved": 86651820674246.89,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7112.634482739586,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.65182067424689,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 900,
      "loss": 2.4859864711761475,
      "time_ms": 28230.671882629395,
      "mfu": {
        "mfu": 0.2736941271066501,
        "mfu_percent": 27.36941271066501,
        "flops_achieved": 84845179403061.53,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6964.340091422861,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.84517940306154,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 910,
      "loss": 2.2373682260513306,
      "time_ms": 27713.167428970337,
      "mfu": {
        "mfu": 0.2788049802807249,
        "mfu_percent": 27.88049802807249,
        "flops_achieved": 86429543887024.73,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7094.389354948766,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.42954388702474,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 920,
      "loss": 2.4351460933685303,
      "time_ms": 28276.63540840149,
      "mfu": {
        "mfu": 0.2732492387073323,
        "mfu_percent": 27.32492387073323,
        "flops_achieved": 84707263999273.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6953.019592337506,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.70726399927301,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 930,
      "loss": 2.2814136743545532,
      "time_ms": 28283.082008361816,
      "mfu": {
        "mfu": 0.27318695665013326,
        "mfu_percent": 27.318695665013326,
        "flops_achieved": 84687956561541.31,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6951.434781466652,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.68795656154131,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 940,
      "loss": 2.5136890411376953,
      "time_ms": 28269.614696502686,
      "mfu": {
        "mfu": 0.2733170997023309,
        "mfu_percent": 27.331709970233092,
        "flops_achieved": 84728300907722.58,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6954.746363215305,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.72830090772258,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 950,
      "loss": 2.4793893098831177,
      "time_ms": 28216.097831726074,
      "mfu": {
        "mfu": 0.27383549435608984,
        "mfu_percent": 27.383549435608984,
        "flops_achieved": 84889003250387.84,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6967.937280786386,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.88900325038784,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 960,
      "loss": 2.5350390672683716,
      "time_ms": 28520.92957496643,
      "mfu": {
        "mfu": 0.2709087401320999,
        "mfu_percent": 27.09087401320999,
        "flops_achieved": 83981709440950.97,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6893.463955416376,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.98170944095097,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 970,
      "loss": 2.5203641653060913,
      "time_ms": 28322.54672050476,
      "mfu": {
        "mfu": 0.272806297216086,
        "mfu_percent": 27.2806297216086,
        "flops_achieved": 84569952136986.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6941.748633701118,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.56995213698666,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 980,
      "loss": 2.648012638092041,
      "time_ms": 28303.0424118042,
      "mfu": {
        "mfu": 0.27299429461081626,
        "mfu_percent": 27.299429461081626,
        "flops_achieved": 84628231329353.05,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6946.532359998222,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.62823132935304,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 990,
      "loss": 2.624989092350006,
      "time_ms": 29519.579887390137,
      "mfu": {
        "mfu": 0.2617438706114872,
        "mfu_percent": 26.174387061148717,
        "flops_achieved": 81140599889561.03,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6660.25738679245,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 81.14059988956103,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1000,
      "loss": 2.651936173439026,
      "time_ms": 345385.0004673004,
      "mfu": {
        "mfu": 0.0223708878153266,
        "mfu_percent": 2.23708878153266,
        "flops_achieved": 6934975222751.246,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 569.2430178901589,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 6.9349752227512464,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1010,
      "loss": 2.4520726203918457,
      "time_ms": 28266.616344451904,
      "mfu": {
        "mfu": 0.2733460915305858,
        "mfu_percent": 27.33460915305858,
        "flops_achieved": 84737288374481.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6955.484080732206,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.73728837448161,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1020,
      "loss": 2.531595289707184,
      "time_ms": 27590.78884124756,
      "mfu": {
        "mfu": 0.28004161617153445,
        "mfu_percent": 28.004161617153443,
        "flops_achieved": 86812901013175.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7125.856427347804,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.81290101317568,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1030,
      "loss": 2.4409672021865845,
      "time_ms": 27597.72300720215,
      "mfu": {
        "mfu": 0.27997125329992295,
        "mfu_percent": 27.997125329992294,
        "flops_achieved": 86791088522976.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7124.065994455102,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.79108852297611,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1040,
      "loss": 2.6549456119537354,
      "time_ms": 27586.54808998108,
      "mfu": {
        "mfu": 0.2800846656619807,
        "mfu_percent": 28.00846656619807,
        "flops_achieved": 86826246355214.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7126.951851993558,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.82624635521401,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1050,
      "loss": 2.2542806267738342,
      "time_ms": 27571.017265319824,
      "mfu": {
        "mfu": 0.28024243807171234,
        "mfu_percent": 28.024243807171235,
        "flops_achieved": 86875155802230.83,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.966482230714,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.87515580223082,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1060,
      "loss": 2.435544490814209,
      "time_ms": 27573.911428451538,
      "mfu": {
        "mfu": 0.28021302377065055,
        "mfu_percent": 28.021302377065055,
        "flops_achieved": 86866037368901.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.218014595286,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.86603736890167,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1070,
      "loss": 2.3510674238204956,
      "time_ms": 27580.544233322144,
      "mfu": {
        "mfu": 0.2801456357491108,
        "mfu_percent": 28.01456357491108,
        "flops_achieved": 86845147082224.34,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7128.50327886072,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.84514708222434,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1080,
      "loss": 2.3014107942581177,
      "time_ms": 27567.233562469482,
      "mfu": {
        "mfu": 0.28028090236336195,
        "mfu_percent": 28.028090236336194,
        "flops_achieved": 86887079732642.2,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.945233259299,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.8870797326422,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1090,
      "loss": 2.1498080492019653,
      "time_ms": 27563.197135925293,
      "mfu": {
        "mfu": 0.2803219474303964,
        "mfu_percent": 28.03219474303964,
        "flops_achieved": 86899803703422.89,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7132.989653937687,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.89980370342289,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1100,
      "loss": 2.3998314142227173,
      "time_ms": 28169.225692749023,
      "mfu": {
        "mfu": 0.27429114249808373,
        "mfu_percent": 27.42911424980837,
        "flops_achieved": 85030254174405.95,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6979.531569112616,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.03025417440595,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1110,
      "loss": 2.494075834751129,
      "time_ms": 28305.973529815674,
      "mfu": {
        "mfu": 0.27296602571933576,
        "mfu_percent": 27.296602571933576,
        "flops_achieved": 84619467972994.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6945.8130381174105,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.61946797299409,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1120,
      "loss": 2.511020064353943,
      "time_ms": 27583.5862159729,
      "mfu": {
        "mfu": 0.28011474063065295,
        "mfu_percent": 28.011474063065293,
        "flops_achieved": 86835569595502.42,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7127.71713078228,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.83556959550242,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1130,
      "loss": 2.5925830006599426,
      "time_ms": 27593.51873397827,
      "mfu": {
        "mfu": 0.28001391098541245,
        "mfu_percent": 28.001391098541244,
        "flops_achieved": 86804312405477.86,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7125.151449347403,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.80431240547786,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1140,
      "loss": 2.415810227394104,
      "time_ms": 27558.940410614014,
      "mfu": {
        "mfu": 0.28036524566723553,
        "mfu_percent": 28.036524566723553,
        "flops_achieved": 86913226156843.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7134.091408110838,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.91322615684301,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1150,
      "loss": 2.2238994240760803,
      "time_ms": 27532.257318496704,
      "mfu": {
        "mfu": 0.2806369637319801,
        "mfu_percent": 28.06369637319801,
        "flops_achieved": 86997458756913.83,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7141.005465901807,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.99745875691383,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1160,
      "loss": 2.273316979408264,
      "time_ms": 27535.32600402832,
      "mfu": {
        "mfu": 0.28060568803217123,
        "mfu_percent": 28.060568803217123,
        "flops_achieved": 86987763289973.08,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7140.20963366248,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.98776328997307,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1170,
      "loss": 2.4874294996261597,
      "time_ms": 27548.837184906006,
      "mfu": {
        "mfu": 0.28046806646285194,
        "mfu_percent": 28.046806646285194,
        "flops_achieved": 86945100603484.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.707755771319,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.9451006034841,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1180,
      "loss": 2.3685665130615234,
      "time_ms": 27571.19131088257,
      "mfu": {
        "mfu": 0.2802406690167488,
        "mfu_percent": 28.024066901674882,
        "flops_achieved": 86874607395192.14,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.921467379513,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.87460739519214,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1190,
      "loss": 2.5501862168312073,
      "time_ms": 27569.8881149292,
      "mfu": {
        "mfu": 0.28025391566121505,
        "mfu_percent": 28.025391566121506,
        "flops_achieved": 86878713854976.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.258537590365,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.87871385497667,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1200,
      "loss": 2.2791815400123596,
      "time_ms": 27529.388189315796,
      "mfu": {
        "mfu": 0.28066621188295054,
        "mfu_percent": 28.066621188295056,
        "flops_achieved": 87006525683714.67,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7141.74970573098,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 87.00652568371467,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1210,
      "loss": 2.3619253635406494,
      "time_ms": 27544.77286338806,
      "mfu": {
        "mfu": 0.2805094504453329,
        "mfu_percent": 28.050945044533286,
        "flops_achieved": 86957929638053.19,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7137.760800392268,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.95792963805319,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1220,
      "loss": 2.263772964477539,
      "time_ms": 27552.459716796875,
      "mfu": {
        "mfu": 0.28043119118835463,
        "mfu_percent": 28.043119118835463,
        "flops_achieved": 86933669268389.94,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7135.769438404854,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.93366926838993,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1230,
      "loss": 2.3287185430526733,
      "time_ms": 27549.423456192017,
      "mfu": {
        "mfu": 0.28046209790331844,
        "mfu_percent": 28.046209790331844,
        "flops_achieved": 86943250350028.72,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.555881564568,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.94325035002872,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1240,
      "loss": 2.4124499559402466,
      "time_ms": 27567.79432296753,
      "mfu": {
        "mfu": 0.2802752011289229,
        "mfu_percent": 28.02752011289229,
        "flops_achieved": 86885312349966.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.80016132811,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.8853123499661,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1250,
      "loss": 2.2976585626602173,
      "time_ms": 27547.984838485718,
      "mfu": {
        "mfu": 0.28047674426465324,
        "mfu_percent": 28.047674426465324,
        "flops_achieved": 86947790722042.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.928568558314,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.9477907220425,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1260,
      "loss": 2.314988136291504,
      "time_ms": 27567.63744354248,
      "mfu": {
        "mfu": 0.2802767960937616,
        "mfu_percent": 28.02767960937616,
        "flops_achieved": 86885806789066.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.840746333306,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.8858067890661,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1270,
      "loss": 2.3455588817596436,
      "time_ms": 27567.046880722046,
      "mfu": {
        "mfu": 0.2802828004023087,
        "mfu_percent": 28.028280040230868,
        "flops_achieved": 86887668124715.69,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.993530198922,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.88766812471569,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1280,
      "loss": 2.265637457370758,
      "time_ms": 27549.549102783203,
      "mfu": {
        "mfu": 0.28046081878595697,
        "mfu_percent": 28.046081878595697,
        "flops_achieved": 86942853823646.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.523333521187,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.94285382364666,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1290,
      "loss": 2.012783110141754,
      "time_ms": 27558.127403259277,
      "mfu": {
        "mfu": 0.2803735168753407,
        "mfu_percent": 28.037351687534066,
        "flops_achieved": 86915790231355.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7134.301874834476,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.91579023135561,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1300,
      "loss": 2.2479663491249084,
      "time_ms": 27550.07529258728,
      "mfu": {
        "mfu": 0.280455462153653,
        "mfu_percent": 28.045546215365302,
        "flops_achieved": 86941193267632.44,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7136.387030234361,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.94119326763244,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1310,
      "loss": 2.3912277817726135,
      "time_ms": 27531.78834915161,
      "mfu": {
        "mfu": 0.2806417440292649,
        "mfu_percent": 28.06417440292649,
        "flops_achieved": 86998940649072.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7141.1271039376,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.99894064907211,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1320,
      "loss": 2.319661259651184,
      "time_ms": 27534.775018692017,
      "mfu": {
        "mfu": 0.28061130309963717,
        "mfu_percent": 28.061130309963715,
        "flops_achieved": 86989503960887.52,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7140.352513014267,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.98950396088752,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1330,
      "loss": 2.0595141649246216,
      "time_ms": 27654.784440994263,
      "mfu": {
        "mfu": 0.27939357527939973,
        "mfu_percent": 27.939357527939972,
        "flops_achieved": 86612008336613.92,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7109.366569806155,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.61200833661393,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1340,
      "loss": 2.459512710571289,
      "time_ms": 28335.60848236084,
      "mfu": {
        "mfu": 0.27268054269455194,
        "mfu_percent": 27.268054269455195,
        "flops_achieved": 84530968235311.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6938.548721210281,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.53096823531111,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1350,
      "loss": 1.9815595149993896,
      "time_ms": 28353.782892227173,
      "mfu": {
        "mfu": 0.2725057579766065,
        "mfu_percent": 27.250575797660648,
        "flops_achieved": 84476784972748.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6934.101200792419,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.47678497274802,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1360,
      "loss": 2.6302456855773926,
      "time_ms": 28393.82529258728,
      "mfu": {
        "mfu": 0.27212145665232584,
        "mfu_percent": 27.212145665232583,
        "flops_achieved": 84357651562221.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6924.322382561397,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.35765156222102,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1370,
      "loss": 2.0290457010269165,
      "time_ms": 28365.379571914673,
      "mfu": {
        "mfu": 0.2723943488561947,
        "mfu_percent": 27.239434885619467,
        "flops_achieved": 84442248145420.34,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6931.266317150464,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.44224814542035,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1380,
      "loss": 2.506033480167389,
      "time_ms": 28357.746601104736,
      "mfu": {
        "mfu": 0.2724676684377136,
        "mfu_percent": 27.246766843771358,
        "flops_achieved": 84464977215691.2,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6933.131985612027,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.4649772156912,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1390,
      "loss": 2.0721487998962402,
      "time_ms": 28377.65407562256,
      "mfu": {
        "mfu": 0.27227652708572225,
        "mfu_percent": 27.227652708572226,
        "flops_achieved": 84405723396573.9,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6928.268259105092,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.4057233965739,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1400,
      "loss": 2.342210054397583,
      "time_ms": 28378.925561904907,
      "mfu": {
        "mfu": 0.272264328037931,
        "mfu_percent": 27.2264328037931,
        "flops_achieved": 84401941691758.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6927.957845730467,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.40194169175861,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1410,
      "loss": 2.2436288595199585,
      "time_ms": 28402.623176574707,
      "mfu": {
        "mfu": 0.27203716538840866,
        "mfu_percent": 27.203716538840865,
        "flops_achieved": 84331521270406.69,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6922.177531903251,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.33152127040668,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1420,
      "loss": 2.128342866897583,
      "time_ms": 27952.620267868042,
      "mfu": {
        "mfu": 0.2764166301587229,
        "mfu_percent": 27.641663015872293,
        "flops_achieved": 85689155349204.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7033.616101672009,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.68915534920411,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1430,
      "loss": 1.8909780979156494,
      "time_ms": 28380.850791931152,
      "mfu": {
        "mfu": 0.2722458588432174,
        "mfu_percent": 27.22458588432174,
        "flops_achieved": 84396216241397.39,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6927.487884045282,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.3962162413974,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1440,
      "loss": 1.9908837676048279,
      "time_ms": 28363.9075756073,
      "mfu": {
        "mfu": 0.2724084852538189,
        "mfu_percent": 27.24084852538189,
        "flops_achieved": 84446630428683.86,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6931.626027757934,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.44663042868386,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1450,
      "loss": 2.316544532775879,
      "time_ms": 28394.226789474487,
      "mfu": {
        "mfu": 0.272117608830774,
        "mfu_percent": 27.211760883077403,
        "flops_achieved": 84356458737539.95,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6924.224472028272,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.35645873753995,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1460,
      "loss": 2.14321768283844,
      "time_ms": 28400.70676803589,
      "mfu": {
        "mfu": 0.2720555217747791,
        "mfu_percent": 27.20555217747791,
        "flops_achieved": 84337211750181.52,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6922.644623100584,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.33721175018151,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1470,
      "loss": 2.1667596101760864,
      "time_ms": 28357.301473617554,
      "mfu": {
        "mfu": 0.27247194539081865,
        "mfu_percent": 27.247194539081864,
        "flops_achieved": 84466303071153.78,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6933.240815700177,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.46630307115377,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1480,
      "loss": 1.6502578854560852,
      "time_ms": 28363.836765289307,
      "mfu": {
        "mfu": 0.27240916532159765,
        "mfu_percent": 27.240916532159765,
        "flops_achieved": 84446841249695.27,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6931.643332562192,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.44684124969527,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1490,
      "loss": 2.3134735822677612,
      "time_ms": 28368.22772026062,
      "mfu": {
        "mfu": 0.27236700067209973,
        "mfu_percent": 27.236700067209973,
        "flops_achieved": 84433770208350.92,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6930.570423318421,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.43377020835092,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1500,
      "loss": 2.20566987991333,
      "time_ms": 28379.5325756073,
      "mfu": {
        "mfu": 0.27225850453899386,
        "mfu_percent": 27.225850453899387,
        "flops_achieved": 84400136407088.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6927.809662692894,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.4001364070881,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1510,
      "loss": 2.3788567185401917,
      "time_ms": 28316.978216171265,
      "mfu": {
        "mfu": 0.27285994429087823,
        "mfu_percent": 27.285994429087822,
        "flops_achieved": 84586582730172.25,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6943.11372135467,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.58658273017225,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1520,
      "loss": 2.207487165927887,
      "time_ms": 27557.8134059906,
      "mfu": {
        "mfu": 0.28037671148723575,
        "mfu_percent": 28.037671148723575,
        "flops_achieved": 86916780561043.08,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7134.383163987197,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.91678056104308,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1530,
      "loss": 1.982273519039154,
      "time_ms": 27572.882413864136,
      "mfu": {
        "mfu": 0.2802234812659792,
        "mfu_percent": 28.02234812659792,
        "flops_achieved": 86869279192453.55,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.484112939241,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.86927919245355,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1540,
      "loss": 2.08430814743042,
      "time_ms": 27822.946071624756,
      "mfu": {
        "mfu": 0.2777049230753622,
        "mfu_percent": 27.77049230753622,
        "flops_achieved": 86088526153362.28,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7066.397623525237,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.08852615336228,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1550,
      "loss": 2.094231605529785,
      "time_ms": 27595.46184539795,
      "mfu": {
        "mfu": 0.27999419403951925,
        "mfu_percent": 27.999419403951926,
        "flops_achieved": 86798200152250.97,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7124.649737753456,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.79820015225097,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1560,
      "loss": 2.2455273270606995,
      "time_ms": 27572.680234909058,
      "mfu": {
        "mfu": 0.2802255360277995,
        "mfu_percent": 28.022553602779947,
        "flops_achieved": 86869916168617.84,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.536397802913,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.86991616861785,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1570,
      "loss": 1.4836462140083313,
      "time_ms": 27558.7899684906,
      "mfu": {
        "mfu": 0.28036677616777417,
        "mfu_percent": 28.036677616777418,
        "flops_achieved": 86913700612009.98,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7134.130352776452,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.91370061200999,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1580,
      "loss": 2.351062774658203,
      "time_ms": 27569.772005081177,
      "mfu": {
        "mfu": 0.28025509594807235,
        "mfu_percent": 28.025509594807236,
        "flops_achieved": 86879079743902.42,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.288570821865,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.87907974390242,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1590,
      "loss": 2.314329743385315,
      "time_ms": 27604.602098464966,
      "mfu": {
        "mfu": 0.27990148421592936,
        "mfu_percent": 27.990148421592938,
        "flops_achieved": 86769460106938.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7122.290670907115,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.76946010693811,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1600,
      "loss": 1.98904287815094,
      "time_ms": 27575.533628463745,
      "mfu": {
        "mfu": 0.28019653953586815,
        "mfu_percent": 28.019653953586815,
        "flops_achieved": 86860927256119.12,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7129.7985616154765,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.86092725611913,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1610,
      "loss": 2.337785840034485,
      "time_ms": 27932.465076446533,
      "mfu": {
        "mfu": 0.27661608373640356,
        "mfu_percent": 27.661608373640355,
        "flops_achieved": 85750985958285.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7038.691338623943,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.75098595828511,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1620,
      "loss": 1.8179081082344055,
      "time_ms": 27573.005199432373,
      "mfu": {
        "mfu": 0.2802222334005713,
        "mfu_percent": 28.022223340057128,
        "flops_achieved": 86868892354177.1,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7130.452360123859,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.86889235417709,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1630,
      "loss": 1.7069388628005981,
      "time_ms": 27570.09506225586,
      "mfu": {
        "mfu": 0.28025181201237015,
        "mfu_percent": 28.025181201237015,
        "flops_achieved": 86878061723834.75,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.2050087618745,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.87806172383475,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1640,
      "loss": 2.099101424217224,
      "time_ms": 27582.96537399292,
      "mfu": {
        "mfu": 0.2801210454999024,
        "mfu_percent": 28.01210454999024,
        "flops_achieved": 86837524104969.75,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7127.877562626943,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.83752410496975,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1650,
      "loss": 2.2860379815101624,
      "time_ms": 27582.622289657593,
      "mfu": {
        "mfu": 0.28012452976407776,
        "mfu_percent": 28.012452976407776,
        "flops_achieved": 86838604226864.11,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7127.966222186218,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.8386042268641,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1660,
      "loss": 2.2856959104537964,
      "time_ms": 27569.460153579712,
      "mfu": {
        "mfu": 0.2802582660490456,
        "mfu_percent": 28.02582660490456,
        "flops_achieved": 86880062475204.12,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7131.369236276893,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.88006247520413,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1670,
      "loss": 2.155651330947876,
      "time_ms": 28301.120042800903,
      "mfu": {
        "mfu": 0.2730128378970623,
        "mfu_percent": 27.301283789706233,
        "flops_achieved": 84633979748089.31,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6947.004206994704,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.63397974808932,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1680,
      "loss": 2.1776397228240967,
      "time_ms": 28368.970155715942,
      "mfu": {
        "mfu": 0.2723598726404141,
        "mfu_percent": 27.235987264041412,
        "flops_achieved": 84431560518528.38,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6930.38904552502,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.43156051852837,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1690,
      "loss": 2.0405430793762207,
      "time_ms": 27593.854665756226,
      "mfu": {
        "mfu": 0.28001050205352856,
        "mfu_percent": 28.001050205352858,
        "flops_achieved": 86803255636593.86,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7125.064706671414,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.80325563659386,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1700,
      "loss": 2.4336209893226624,
      "time_ms": 28420.936107635498,
      "mfu": {
        "mfu": 0.27186187918964083,
        "mfu_percent": 27.186187918964084,
        "flops_achieved": 84277182548788.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6917.717250952188,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.27718254878866,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1710,
      "loss": 2.3249078392982483,
      "time_ms": 28312.983512878418,
      "mfu": {
        "mfu": 0.27289844233604,
        "mfu_percent": 27.289844233604,
        "flops_achieved": 84598517124172.4,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6944.09333126518,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.5985171241724,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1720,
      "loss": 2.1643930077552795,
      "time_ms": 27584.13529396057,
      "mfu": {
        "mfu": 0.2801091647865504,
        "mfu_percent": 28.01091647865504,
        "flops_achieved": 86833841083830.62,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7127.575249496636,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.83384108383062,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1730,
      "loss": 2.215452790260315,
      "time_ms": 27627.59566307068,
      "mfu": {
        "mfu": 0.27966853115917256,
        "mfu_percent": 27.966853115917257,
        "flops_achieved": 86697244659343.5,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7116.36301608404,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.6972446593435,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1740,
      "loss": 2.300983965396881,
      "time_ms": 28537.799835205078,
      "mfu": {
        "mfu": 0.27074859110262517,
        "mfu_percent": 27.074859110262516,
        "flops_achieved": 83932063241813.8,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6889.388850413708,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.9320632418138,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1750,
      "loss": 1.8283921480178833,
      "time_ms": 27600.059747695923,
      "mfu": {
        "mfu": 0.27994754972208075,
        "mfu_percent": 27.994754972208074,
        "flops_achieved": 86783740413845.03,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7123.462840199576,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.78374041384504,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1760,
      "loss": 1.994323968887329,
      "time_ms": 27600.3839969635,
      "mfu": {
        "mfu": 0.279944260898709,
        "mfu_percent": 27.9944260898709,
        "flops_achieved": 86782720878599.8,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7123.379153769387,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.7827208785998,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1770,
      "loss": 1.7624019384384155,
      "time_ms": 28299.63994026184,
      "mfu": {
        "mfu": 0.27302711677111935,
        "mfu_percent": 27.302711677111937,
        "flops_achieved": 84638406199047.0,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6947.367543015493,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.638406199047,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1780,
      "loss": 2.219957649707794,
      "time_ms": 28561.7573261261,
      "mfu": {
        "mfu": 0.2705214882377994,
        "mfu_percent": 27.05214882377994,
        "flops_achieved": 83861661353717.83,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6883.610057850261,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.86166135371782,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1790,
      "loss": 1.9123170375823975,
      "time_ms": 28328.1147480011,
      "mfu": {
        "mfu": 0.27275267582343116,
        "mfu_percent": 27.275267582343115,
        "flops_achieved": 84553329505263.66,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6940.384199547665,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.55332950526366,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1800,
      "loss": 2.0161476731300354,
      "time_ms": 28310.768604278564,
      "mfu": {
        "mfu": 0.2729197926962251,
        "mfu_percent": 27.291979269622512,
        "flops_achieved": 84605135735829.78,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6944.636606237774,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.60513573582978,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1810,
      "loss": 2.2156434059143066,
      "time_ms": 28340.620279312134,
      "mfu": {
        "mfu": 0.2726323214665377,
        "mfu_percent": 27.26323214665377,
        "flops_achieved": 84516019654626.69,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6937.32169805466,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.51601965462669,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1820,
      "loss": 2.1575549840927124,
      "time_ms": 27588.07349205017,
      "mfu": {
        "mfu": 0.2800691791971993,
        "mfu_percent": 28.006917919719932,
        "flops_achieved": 86821445551131.78,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7126.557787975116,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.82144555113179,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1830,
      "loss": 2.2472053170204163,
      "time_ms": 27532.705545425415,
      "mfu": {
        "mfu": 0.28063239501845033,
        "mfu_percent": 28.063239501845032,
        "flops_achieved": 86996042455719.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7140.889211763883,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.9960424557196,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1840,
      "loss": 2.0323569774627686,
      "time_ms": 27551.578283309937,
      "mfu": {
        "mfu": 0.28044016277757366,
        "mfu_percent": 28.044016277757365,
        "flops_achieved": 86936450461047.84,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7135.997726819892,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.93645046104784,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1850,
      "loss": 2.250145733356476,
      "time_ms": 27608.25538635254,
      "mfu": {
        "mfu": 0.27986444599356836,
        "mfu_percent": 27.986444599356837,
        "flops_achieved": 86757978258006.19,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7121.348207217336,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.75797825800619,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1860,
      "loss": 1.9737274646759033,
      "time_ms": 28306.862831115723,
      "mfu": {
        "mfu": 0.27295745009430133,
        "mfu_percent": 27.295745009430135,
        "flops_achieved": 84616809529233.4,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6945.594825290311,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.61680952923341,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1870,
      "loss": 1.9063430428504944,
      "time_ms": 28274.75380897522,
      "mfu": {
        "mfu": 0.27326742261847276,
        "mfu_percent": 27.326742261847276,
        "flops_achieved": 84712901011726.55,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6953.482294780971,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.71290101172654,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1880,
      "loss": 2.2828913927078247,
      "time_ms": 27540.682315826416,
      "mfu": {
        "mfu": 0.28055111380121417,
        "mfu_percent": 28.055111380121417,
        "flops_achieved": 86970845278376.39,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7138.8209538664205,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.97084527837639,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1890,
      "loss": 1.5803056955337524,
      "time_ms": 28056.68544769287,
      "mfu": {
        "mfu": 0.27539137197640234,
        "mfu_percent": 27.539137197640233,
        "flops_achieved": 85371325312684.73,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7007.527684143006,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 85.37132531268473,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1900,
      "loss": 1.851170003414154,
      "time_ms": 28326.87783241272,
      "mfu": {
        "mfu": 0.2727645857853583,
        "mfu_percent": 27.276458578535827,
        "flops_achieved": 84557021593461.06,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6940.687256928592,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.55702159346106,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1910,
      "loss": 1.8333560228347778,
      "time_ms": 28311.10715866089,
      "mfu": {
        "mfu": 0.2729165290233732,
        "mfu_percent": 27.291652902337322,
        "flops_achieved": 84604123997245.69,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6944.5535597803,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.6041239972457,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1920,
      "loss": 1.8123550415039062,
      "time_ms": 29176.942825317383,
      "mfu": {
        "mfu": 0.2648176385308612,
        "mfu_percent": 26.48176385308612,
        "flops_achieved": 82093467944566.97,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6738.471579325286,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 82.09346794456697,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1930,
      "loss": 2.1198769211769104,
      "time_ms": 28332.265377044678,
      "mfu": {
        "mfu": 0.272712718016919,
        "mfu_percent": 27.2712718016919,
        "flops_achieved": 84540942585244.89,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6939.3674449800765,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.54094258524489,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1940,
      "loss": 2.2861061096191406,
      "time_ms": 27607.586145401,
      "mfu": {
        "mfu": 0.2798712302429103,
        "mfu_percent": 27.987123024291026,
        "flops_achieved": 86760081375302.19,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7121.5208372265415,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.76008137530219,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1950,
      "loss": 2.1904191970825195,
      "time_ms": 28667.218685150146,
      "mfu": {
        "mfu": 0.26952629006011414,
        "mfu_percent": 26.952629006011414,
        "flops_achieved": 83553149918635.39,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6858.286538339506,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 83.55314991863538,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1960,
      "loss": 2.245172381401062,
      "time_ms": 28318.840742111206,
      "mfu": {
        "mfu": 0.27284199833296136,
        "mfu_percent": 27.284199833296135,
        "flops_achieved": 84581019483218.02,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6942.6570737987995,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.58101948321801,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1970,
      "loss": 2.431563377380371,
      "time_ms": 28355.976819992065,
      "mfu": {
        "mfu": 0.2724846739578011,
        "mfu_percent": 27.248467395780114,
        "flops_achieved": 84470248926918.34,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6933.564703064072,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.47024892691834,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1980,
      "loss": 1.9971249103546143,
      "time_ms": 27740.381002426147,
      "mfu": {
        "mfu": 0.27853146998502815,
        "mfu_percent": 27.853146998502815,
        "flops_achieved": 86344755695358.72,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 7087.429692577216,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 86.34475569535871,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 1990,
      "loss": 2.2782622575759888,
      "time_ms": 28304.66341972351,
      "mfu": {
        "mfu": 0.2729786602290563,
        "mfu_percent": 27.29786602290563,
        "flops_achieved": 84623384671007.45,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 6946.1345321279405,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 84.62338467100746,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    },
    {
      "iter": 2000,
      "loss": 1.8775582909584045,
      "time_ms": 327182.41024017334,
      "mfu": {
        "mfu": 0.02361547826754713,
        "mfu_percent": 2.361547826754713,
        "flops_achieved": 7320798262939.61,
        "flops_per_token": 12182802432.0,
        "tokens_per_sec": 600.9124997143851,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 7.3207982629396104,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.82914048,
        "non_attn_gflops": 10.97484288,
        "attn_gflops": 1.207959552,
        "attention_flops_per_layer": 343597383680.0,
        "ffn_flops_per_layer": 137430564864.0,
        "attention_to_ffn_ratio": 2.500152597204419,
        "architecture": "24L-16H-2048D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 12.247712256,
        "reserved_gb": 31.398559744,
        "max_allocated_gb": 28.705547776,
        "max_reserved_gb": 31.398559744
      }
    }
  ],
  "eval_steps": [
    {
      "iter": 1000,
      "train_loss": 2.4179091453552246,
      "val_loss": 2.3609988689422607,
      "timestamp": "2025-11-14T06:38:59.186700",
      "lr": 0.00015007496251874061
    },
    {
      "iter": 2000,
      "train_loss": 2.0744056701660156,
      "val_loss": 2.015394926071167,
      "timestamp": "2025-11-14T14:29:17.947758",
      "lr": 0.0003
    }
  ],
  "checkpoints": [
    {
      "iter": 1000,
      "val_loss": 2.3609988689422607,
      "path": "out-qwen3-1.8b-optimal/ckpt.pt",
      "timestamp": "2025-11-14T06:43:37.561667"
    },
    {
      "iter": 2000,
      "val_loss": 2.015394926071167,
      "path": "out-qwen3-1.8b-optimal/ckpt.pt",
      "timestamp": "2025-11-14T14:33:58.074772"
    }
  ],
  "metadata": {
    "world_size": 2,
    "device": "cuda:0",
    "dtype": "bfloat16",
    "compile": true,
    "use_zero1": true,
    "use_fsdp": false
  },
  "end_time": "2025-11-14T14:34:25.025009",
  "summary": {
    "total_iterations": 201,
    "final_iter": 2000,
    "final_train_loss": 1.8775582909584045,
    "best_val_loss": 2.015394926071167,
    "avg_time_ms": 31244.14208042088,
    "avg_mfu": 27.369168800612297,
    "total_eval_steps": 2,
    "total_checkpoints": 2
  }
}