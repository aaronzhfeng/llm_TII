{
  "run_name": "run_20251110_174040",
  "start_time": "2025-11-10T17:40:40.744600",
  "config": {
    "MODULAR_ARCH_AVAILABLE": true,
    "LOGGER_AVAILABLE": true,
    "out_dir": "out-llama-1.36b",
    "eval_interval": 1000,
    "log_interval": 10,
    "eval_iters": 50,
    "eval_only": false,
    "eval_at_start": false,
    "always_save_checkpoint": true,
    "init_from": "scratch",
    "save_log_to_json": true,
    "log_save_interval": 10,
    "gradient_log_interval": 10,
    "wandb_log": false,
    "wandb_project": "llama-1.36b",
    "wandb_run_name": "run-1",
    "dataset": "slimpajama_6b_llama",
    "gradient_accumulation_steps": 16,
    "batch_size": 5,
    "block_size": 2048,
    "n_layer": 18,
    "n_head": 18,
    "n_embd": 2304,
    "dropout": 0.0,
    "bias": false,
    "arch_preset": "llama",
    "normalization": "layernorm_nobias",
    "activation": "gelu",
    "attention_backend": "flash_attn_2",
    "position_encoding": "learned_absolute",
    "norm_position": "post",
    "ffn_type": "standard",
    "weight_tying": true,
    "rope_theta": 10000.0,
    "d_ff": 6144,
    "intermediate_size": 6144,
    "learning_rate": 0.0003,
    "max_iters": 2000,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.95,
    "grad_clip": 1.0,
    "decay_lr": true,
    "warmup_iters": 2000,
    "lr_decay_iters": 25000,
    "min_lr": 3e-05,
    "backend": "nccl",
    "use_zero1": true,
    "use_fsdp": false,
    "fsdp_min_num_params": 1000000.0,
    "fsdp_activation_checkpointing": false,
    "device": "cuda",
    "dtype": "bfloat16",
    "compile": true
  },
  "startup_info": {
    "timestamp": "2025-11-10T17:40:40.746211",
    "model": {
      "total_params": 1294159104,
      "trainable_params": 1294159104,
      "non_embedding_params": 1294159104
    },
    "optimizer": {
      "type": "ZeroRedundancyOptimizer",
      "param_groups": 2
    },
    "config": {
      "MODULAR_ARCH_AVAILABLE": true,
      "LOGGER_AVAILABLE": true,
      "out_dir": "out-llama-1.36b",
      "eval_interval": 1000,
      "log_interval": 10,
      "eval_iters": 50,
      "eval_only": false,
      "eval_at_start": false,
      "always_save_checkpoint": true,
      "init_from": "scratch",
      "save_log_to_json": true,
      "log_save_interval": 10,
      "gradient_log_interval": 10,
      "wandb_log": false,
      "wandb_project": "llama-1.36b",
      "wandb_run_name": "run-1",
      "dataset": "slimpajama_6b_llama",
      "gradient_accumulation_steps": 16,
      "batch_size": 5,
      "block_size": 2048,
      "n_layer": 18,
      "n_head": 18,
      "n_embd": 2304,
      "dropout": 0.0,
      "bias": false,
      "arch_preset": "llama",
      "normalization": "layernorm_nobias",
      "activation": "gelu",
      "attention_backend": "flash_attn_2",
      "position_encoding": "learned_absolute",
      "norm_position": "post",
      "ffn_type": "standard",
      "weight_tying": true,
      "rope_theta": 10000.0,
      "d_ff": 6144,
      "intermediate_size": 6144,
      "learning_rate": 0.0003,
      "max_iters": 2000,
      "weight_decay": 0.1,
      "beta1": 0.9,
      "beta2": 0.95,
      "grad_clip": 1.0,
      "decay_lr": true,
      "warmup_iters": 2000,
      "lr_decay_iters": 25000,
      "min_lr": 3e-05,
      "backend": "nccl",
      "use_zero1": true,
      "use_fsdp": false,
      "fsdp_min_num_params": 1000000.0,
      "fsdp_activation_checkpointing": false,
      "device": "cuda",
      "dtype": "bfloat16",
      "compile": true
    },
    "hardware": {
      "gpu_name": "NVIDIA RTX A6000",
      "num_gpus": 2,
      "gpu_memory_gb": 51.033931776,
      "precision": "bfloat16",
      "parallelism": "DDP+ZeRO-1"
    }
  },
  "training_iterations": [
    {
      "iter": 0,
      "loss": 10.860885620117188,
      "time_ms": 35614.51697349548,
      "mfu": -100.0
    },
    {
      "iter": 10,
      "loss": 10.802600860595703,
      "time_ms": 7612.497568130493,
      "mfu": {
        "mfu": 0.3049311836066328,
        "mfu_percent": 30.493118360663278,
        "flops_achieved": 94528666918056.17,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10761.251385216303,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.52866691805617,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 20,
      "loss": 10.495339393615723,
      "time_ms": 7665.270805358887,
      "mfu": {
        "mfu": 0.3028318180265469,
        "mfu_percent": 30.283181802654692,
        "flops_achieved": 93877863588229.55,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10687.16319098977,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.87786358822954,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 30,
      "loss": 9.451326370239258,
      "time_ms": 7670.962810516357,
      "mfu": {
        "mfu": 0.30260711086622927,
        "mfu_percent": 30.260711086622926,
        "flops_achieved": 93808204368531.08,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10679.233105874711,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80820436853108,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 40,
      "loss": 8.877449989318848,
      "time_ms": 7673.27356338501,
      "mfu": {
        "mfu": 0.30251598284326325,
        "mfu_percent": 30.251598284326324,
        "flops_achieved": 93779954681411.61,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10676.01712923442,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.77995468141161,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 50,
      "loss": 8.616602897644043,
      "time_ms": 7674.601078033447,
      "mfu": {
        "mfu": 0.30246365512035917,
        "mfu_percent": 30.246365512035915,
        "flops_achieved": 93763733087311.34,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10674.170444438438,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.76373308731134,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 60,
      "loss": 8.341612815856934,
      "time_ms": 7665.628910064697,
      "mfu": {
        "mfu": 0.3028176710465174,
        "mfu_percent": 30.281767104651742,
        "flops_achieved": 93873478024420.39,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10686.663933397815,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.8734780244204,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 70,
      "loss": 8.559901237487793,
      "time_ms": 7678.101301193237,
      "mfu": {
        "mfu": 0.30232577073343625,
        "mfu_percent": 30.232577073343624,
        "flops_achieved": 93720988927365.23,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10669.304400460174,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.72098892736524,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 80,
      "loss": 8.176912307739258,
      "time_ms": 7661.41676902771,
      "mfu": {
        "mfu": 0.30298415601625517,
        "mfu_percent": 30.298415601625518,
        "flops_achieved": 93925088365039.1,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10692.53931350823,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.92508836503909,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 90,
      "loss": 8.334858894348145,
      "time_ms": 7701.315402984619,
      "mfu": {
        "mfu": 0.30141446911173614,
        "mfu_percent": 30.141446911173613,
        "flops_achieved": 93438485424638.2,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.14387911605,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.4384854246382,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 100,
      "loss": 7.780569553375244,
      "time_ms": 7669.643402099609,
      "mfu": {
        "mfu": 0.302659168354187,
        "mfu_percent": 30.2659168354187,
        "flops_achieved": 93824342189797.97,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10681.07025387568,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.82434218979797,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 110,
      "loss": 7.919502258300781,
      "time_ms": 7661.027669906616,
      "mfu": {
        "mfu": 0.30299954440458776,
        "mfu_percent": 30.299954440458777,
        "flops_achieved": 93929858765422.2,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10693.082381335213,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.9298587654222,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 120,
      "loss": 7.473707675933838,
      "time_ms": 7702.383041381836,
      "mfu": {
        "mfu": 0.30137268961843233,
        "mfu_percent": 30.137268961843233,
        "flops_achieved": 93425533781714.03,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10635.669449295949,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.42553378171404,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 130,
      "loss": 7.559391975402832,
      "time_ms": 7697.040557861328,
      "mfu": {
        "mfu": 0.3015818711364085,
        "mfu_percent": 30.15818711364085,
        "flops_achieved": 93490380052286.64,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10643.051622786563,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.49038005228664,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 140,
      "loss": 7.6922478675842285,
      "time_ms": 7685.939073562622,
      "mfu": {
        "mfu": 0.30201747261270845,
        "mfu_percent": 30.201747261270846,
        "flops_achieved": 93625416509939.62,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10658.424327325309,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.62541650993963,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 150,
      "loss": 7.2794084548950195,
      "time_ms": 7668.915510177612,
      "mfu": {
        "mfu": 0.3026878951231116,
        "mfu_percent": 30.268789512311162,
        "flops_achieved": 93833247488164.61,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10682.084043210789,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.83324748816462,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 160,
      "loss": 6.975630283355713,
      "time_ms": 7675.125360488892,
      "mfu": {
        "mfu": 0.30244299403922487,
        "mfu_percent": 30.244299403922486,
        "flops_achieved": 93757328152159.7,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10673.441299306915,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7573281521597,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 170,
      "loss": 7.060530185699463,
      "time_ms": 7679.935932159424,
      "mfu": {
        "mfu": 0.3022535492688611,
        "mfu_percent": 30.225354926886112,
        "flops_achieved": 93698600273346.94,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10666.755650520896,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.69860027334694,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 180,
      "loss": 7.4656219482421875,
      "time_ms": 7696.074724197388,
      "mfu": {
        "mfu": 0.30161971873197074,
        "mfu_percent": 30.161971873197075,
        "flops_achieved": 93502112806910.92,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.387292970745,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50211280691092,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 190,
      "loss": 6.747688293457031,
      "time_ms": 7680.587291717529,
      "mfu": {
        "mfu": 0.30222791636726,
        "mfu_percent": 30.222791636725997,
        "flops_achieved": 93690654073850.6,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.851046096384,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.6906540738506,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 200,
      "loss": 6.755859375,
      "time_ms": 7719.947099685669,
      "mfu": {
        "mfu": 0.30068702073711884,
        "mfu_percent": 30.068702073711883,
        "flops_achieved": 93212976428506.84,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10611.471677485395,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.21297642850685,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 210,
      "loss": 6.583184242248535,
      "time_ms": 7693.685531616211,
      "mfu": {
        "mfu": 0.30171338354207633,
        "mfu_percent": 30.171338354207634,
        "flops_achieved": 93531148898043.66,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10647.692794741908,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.53114889804365,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 220,
      "loss": 6.9404754638671875,
      "time_ms": 7689.105987548828,
      "mfu": {
        "mfu": 0.3018930806015118,
        "mfu_percent": 30.18930806015118,
        "flops_achieved": 93586854986468.66,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10654.03443945957,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.58685498646865,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 230,
      "loss": 6.363495349884033,
      "time_ms": 7696.213722229004,
      "mfu": {
        "mfu": 0.30161427130694934,
        "mfu_percent": 30.161427130694936,
        "flops_achieved": 93500424105154.3,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.195049234424,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.5004241051543,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 240,
      "loss": 6.4479079246521,
      "time_ms": 7708.754539489746,
      "mfu": {
        "mfu": 0.30112359678354667,
        "mfu_percent": 30.112359678354668,
        "flops_achieved": 93348315002899.47,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10626.878775338773,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.34831500289947,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 250,
      "loss": 6.373876571655273,
      "time_ms": 7684.6489906311035,
      "mfu": {
        "mfu": 0.302068174679506,
        "mfu_percent": 30.206817467950597,
        "flops_achieved": 93641134150646.84,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10660.213641491555,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.64113415064685,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 260,
      "loss": 6.389260292053223,
      "time_ms": 7673.7260818481445,
      "mfu": {
        "mfu": 0.30249814352163906,
        "mfu_percent": 30.249814352163906,
        "flops_achieved": 93774424491708.11,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10675.387566123592,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7744244917081,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 270,
      "loss": 6.307857513427734,
      "time_ms": 7690.219879150391,
      "mfu": {
        "mfu": 0.30184935283139125,
        "mfu_percent": 30.184935283139126,
        "flops_achieved": 93573299377731.28,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.491253481618,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.57329937773127,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 280,
      "loss": 6.313107967376709,
      "time_ms": 7685.640811920166,
      "mfu": {
        "mfu": 0.30202919320044297,
        "mfu_percent": 30.2029193200443,
        "flops_achieved": 93629049892137.31,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10658.837955703691,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.62904989213732,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 290,
      "loss": 6.086931228637695,
      "time_ms": 7701.715469360352,
      "mfu": {
        "mfu": 0.30139881210717256,
        "mfu_percent": 30.139881210717256,
        "flops_achieved": 93433631753223.5,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10636.591331619744,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.4336317532235,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 300,
      "loss": 6.012209415435791,
      "time_ms": 7694.464683532715,
      "mfu": {
        "mfu": 0.3016828316361686,
        "mfu_percent": 30.16828316361686,
        "flops_achieved": 93521677807212.27,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10646.614594946526,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.52167780721227,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 310,
      "loss": 5.8769145011901855,
      "time_ms": 7682.875394821167,
      "mfu": {
        "mfu": 0.3021379072758836,
        "mfu_percent": 30.213790727588357,
        "flops_achieved": 93662751255523.9,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10662.674557395556,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.6627512555239,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 320,
      "loss": 5.864589691162109,
      "time_ms": 7706.836938858032,
      "mfu": {
        "mfu": 0.3011985217889668,
        "mfu_percent": 30.11985217889668,
        "flops_achieved": 93371541754579.7,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10629.52293527292,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.37154175457971,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 330,
      "loss": 5.705782413482666,
      "time_ms": 7710.575103759766,
      "mfu": {
        "mfu": 0.3010524976951146,
        "mfu_percent": 30.10524976951146,
        "flops_achieved": 93326274285485.53,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10624.369634899847,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.32627428548552,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 340,
      "loss": 5.870142459869385,
      "time_ms": 7690.979957580566,
      "mfu": {
        "mfu": 0.3018195218887136,
        "mfu_percent": 30.181952188871357,
        "flops_achieved": 93564051785501.2,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10651.438497022224,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.5640517855012,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 350,
      "loss": 5.571564674377441,
      "time_ms": 7690.519094467163,
      "mfu": {
        "mfu": 0.301837608767224,
        "mfu_percent": 30.1837608767224,
        "flops_achieved": 93569658717839.44,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.076796602221,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56965871783943,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 360,
      "loss": 5.441626071929932,
      "time_ms": 7673.90251159668,
      "mfu": {
        "mfu": 0.3024911888240373,
        "mfu_percent": 30.24911888240373,
        "flops_achieved": 93772268535451.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10675.142129601438,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.77226853545156,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 370,
      "loss": 5.4737067222595215,
      "time_ms": 7690.549612045288,
      "mfu": {
        "mfu": 0.3018364110176129,
        "mfu_percent": 30.18364110176129,
        "flops_achieved": 93569287415460.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.034527115355,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56928741546,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 380,
      "loss": 5.835301876068115,
      "time_ms": 7699.153423309326,
      "mfu": {
        "mfu": 0.3014991085415838,
        "mfu_percent": 30.149910854158378,
        "flops_achieved": 93464723647890.98,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10640.130868412847,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.46472364789098,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 390,
      "loss": 5.718082427978516,
      "time_ms": 7694.861173629761,
      "mfu": {
        "mfu": 0.3016672869430944,
        "mfu_percent": 30.16672869430944,
        "flops_achieved": 93516858952359.27,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10646.06601100736,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.51685895235927,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 400,
      "loss": 5.58052396774292,
      "time_ms": 7696.056365966797,
      "mfu": {
        "mfu": 0.3016204382179105,
        "mfu_percent": 30.162043821791052,
        "flops_achieved": 93502335847552.27,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.412684172046,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50233584755226,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 410,
      "loss": 5.640834331512451,
      "time_ms": 7717.990875244141,
      "mfu": {
        "mfu": 0.30076323374497593,
        "mfu_percent": 30.076323374497594,
        "flops_achieved": 93236602460942.55,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10614.161291996688,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.23660246094255,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 420,
      "loss": 5.527043342590332,
      "time_ms": 7719.550848007202,
      "mfu": {
        "mfu": 0.3007024552797504,
        "mfu_percent": 30.070245527975043,
        "flops_achieved": 93217761136722.62,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10612.016374132389,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.21776113672263,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 430,
      "loss": 5.385039806365967,
      "time_ms": 7690.8605098724365,
      "mfu": {
        "mfu": 0.3018242094851291,
        "mfu_percent": 30.182420948512913,
        "flops_achieved": 93565504940390.03,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10651.60392583414,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56550494039003,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 440,
      "loss": 5.3064494132995605,
      "time_ms": 7722.291469573975,
      "mfu": {
        "mfu": 0.3005957367445374,
        "mfu_percent": 30.059573674453738,
        "flops_achieved": 93184678390806.6,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10608.250196559777,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.1846783908066,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 450,
      "loss": 5.618447780609131,
      "time_ms": 7699.936628341675,
      "mfu": {
        "mfu": 0.30146844132567596,
        "mfu_percent": 30.146844132567598,
        "flops_achieved": 93455216810959.55,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10639.048599240616,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.45521681095954,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 460,
      "loss": 5.334929943084717,
      "time_ms": 7675.968408584595,
      "mfu": {
        "mfu": 0.3024097768636697,
        "mfu_percent": 30.24097768636697,
        "flops_achieved": 93747030827737.61,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10672.269040136081,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.7470308277376,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 470,
      "loss": 5.218414306640625,
      "time_ms": 7707.047462463379,
      "mfu": {
        "mfu": 0.3011902943323382,
        "mfu_percent": 30.11902943323382,
        "flops_achieved": 93368991243024.83,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10629.232582125058,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.36899124302482,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 480,
      "loss": 5.18153190612793,
      "time_ms": 7700.929403305054,
      "mfu": {
        "mfu": 0.30142957714381907,
        "mfu_percent": 30.142957714381907,
        "flops_achieved": 93443168914583.9,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.677052959596,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44316891458391,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 490,
      "loss": 5.311305522918701,
      "time_ms": 7697.758436203003,
      "mfu": {
        "mfu": 0.30155374618349856,
        "mfu_percent": 30.155374618349857,
        "flops_achieved": 93481661316884.55,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10642.059071992375,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.48166131688454,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 500,
      "loss": 5.100747585296631,
      "time_ms": 7686.265707015991,
      "mfu": {
        "mfu": 0.30200463816047673,
        "mfu_percent": 30.200463816047673,
        "flops_achieved": 93621437829747.78,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10657.971389828974,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.62143782974778,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 510,
      "loss": 5.177337646484375,
      "time_ms": 7681.856393814087,
      "mfu": {
        "mfu": 0.3021779859776983,
        "mfu_percent": 30.21779859776983,
        "flops_achieved": 93675175653086.47,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10664.088965001627,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.67517565308647,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 520,
      "loss": 4.913947582244873,
      "time_ms": 7685.259103775024,
      "mfu": {
        "mfu": 0.3020441942565633,
        "mfu_percent": 30.204419425656333,
        "flops_achieved": 93633700219534.62,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10659.367354284857,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.63370021953463,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 530,
      "loss": 5.262187957763672,
      "time_ms": 7686.559677124023,
      "mfu": {
        "mfu": 0.3019930880860825,
        "mfu_percent": 30.19930880860825,
        "flops_achieved": 93617857306685.58,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10657.563778994936,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.61785730668558,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 540,
      "loss": 5.020594120025635,
      "time_ms": 7699.609994888306,
      "mfu": {
        "mfu": 0.30148123024331425,
        "mfu_percent": 30.148123024331426,
        "flops_achieved": 93459181375427.42,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10639.499929786816,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.45918137542742,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 550,
      "loss": 4.975106239318848,
      "time_ms": 7721.099376678467,
      "mfu": {
        "mfu": 0.3006421469802708,
        "mfu_percent": 30.06421469802708,
        "flops_achieved": 93199065563883.95,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10609.888048771767,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.19906556388395,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 560,
      "loss": 4.874137878417969,
      "time_ms": 7698.396921157837,
      "mfu": {
        "mfu": 0.3015287361025708,
        "mfu_percent": 30.152873610257082,
        "flops_achieved": 93473908191796.95,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10641.176447379028,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.47390819179695,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 570,
      "loss": 4.855454921722412,
      "time_ms": 7686.6185665130615,
      "mfu": {
        "mfu": 0.30199077443043576,
        "mfu_percent": 30.199077443043578,
        "flops_achieved": 93617140073435.08,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10657.482128342683,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.61714007343508,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 580,
      "loss": 5.106820583343506,
      "time_ms": 7662.792205810547,
      "mfu": {
        "mfu": 0.3029297717211302,
        "mfu_percent": 30.292977172113023,
        "flops_achieved": 93908229233550.38,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10690.620050727939,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.90822923355037,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 590,
      "loss": 5.093485355377197,
      "time_ms": 7709.029197692871,
      "mfu": {
        "mfu": 0.30111286831646084,
        "mfu_percent": 30.111286831646083,
        "flops_achieved": 93344989178102.86,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10626.500159646133,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.34498917810286,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 600,
      "loss": 4.861051559448242,
      "time_ms": 7685.282945632935,
      "mfu": {
        "mfu": 0.3020432572325379,
        "mfu_percent": 30.20432572325379,
        "flops_achieved": 93633409742086.75,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10659.334286000492,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.63340974208676,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 610,
      "loss": 5.022087097167969,
      "time_ms": 7667.346239089966,
      "mfu": {
        "mfu": 0.3027498460703606,
        "mfu_percent": 30.27498460703606,
        "flops_achieved": 93852452281811.78,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10684.270338849736,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.85245228181178,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 620,
      "loss": 4.9185991287231445,
      "time_ms": 7686.962366104126,
      "mfu": {
        "mfu": 0.30197726788522194,
        "mfu_percent": 30.197726788522196,
        "flops_achieved": 93612953044418.8,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10657.005472178258,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.6129530444188,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 630,
      "loss": 4.81209135055542,
      "time_ms": 7685.705900192261,
      "mfu": {
        "mfu": 0.302026635392668,
        "mfu_percent": 30.2026635392668,
        "flops_achieved": 93628256971727.08,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10658.747688738746,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.62825697172708,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 640,
      "loss": 4.397986888885498,
      "time_ms": 7699.482679367065,
      "mfu": {
        "mfu": 0.3014862154146006,
        "mfu_percent": 30.14862154146006,
        "flops_achieved": 93460726778526.19,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10639.675860240292,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.4607267785262,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 650,
      "loss": 4.784566402435303,
      "time_ms": 7709.557056427002,
      "mfu": {
        "mfu": 0.3010922516900662,
        "mfu_percent": 30.10922516900662,
        "flops_achieved": 93338598023920.53,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10625.772583355892,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.33859802392053,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 660,
      "loss": 4.933363437652588,
      "time_ms": 7712.19801902771,
      "mfu": {
        "mfu": 0.3009891457565678,
        "mfu_percent": 30.09891457565678,
        "flops_achieved": 93306635184536.03,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10622.133897221664,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.30663518453603,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 670,
      "loss": 4.765735149383545,
      "time_ms": 7711.497068405151,
      "mfu": {
        "mfu": 0.301016504715176,
        "mfu_percent": 30.101650471517598,
        "flops_achieved": 93315116461704.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10623.099415499388,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.31511646170456,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 680,
      "loss": 4.79919958114624,
      "time_ms": 7682.938814163208,
      "mfu": {
        "mfu": 0.30213541325793697,
        "mfu_percent": 30.213541325793695,
        "flops_achieved": 93661978109960.47,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10662.586541621751,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.66197810996047,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 690,
      "loss": 5.1692328453063965,
      "time_ms": 7681.788444519043,
      "mfu": {
        "mfu": 0.3021806588944641,
        "mfu_percent": 30.21806588944641,
        "flops_achieved": 93676004257283.88,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10664.183294249653,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.67600425728388,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 700,
      "loss": 4.762387752532959,
      "time_ms": 7681.050777435303,
      "mfu": {
        "mfu": 0.30220967949748684,
        "mfu_percent": 30.220967949748683,
        "flops_achieved": 93685000644220.92,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.207453211633,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.68500064422092,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 710,
      "loss": 4.5777812004089355,
      "time_ms": 7716.068506240845,
      "mfu": {
        "mfu": 0.30083816541742225,
        "mfu_percent": 30.083816541742227,
        "flops_achieved": 93259831279400.89,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10616.805687215214,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.25983127940088,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 720,
      "loss": 4.963666915893555,
      "time_ms": 7698.8654136657715,
      "mfu": {
        "mfu": 0.3015103874308899,
        "mfu_percent": 30.151038743088993,
        "flops_achieved": 93468220103575.88,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10640.528908920653,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.46822010357587,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 730,
      "loss": 4.786726474761963,
      "time_ms": 7703.5205364227295,
      "mfu": {
        "mfu": 0.3013281891931682,
        "mfu_percent": 30.132818919316822,
        "flops_achieved": 93411738649882.16,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10634.098995735403,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.41173864988215,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 740,
      "loss": 4.443315029144287,
      "time_ms": 7690.524339675903,
      "mfu": {
        "mfu": 0.30183740290333305,
        "mfu_percent": 30.183740290333304,
        "flops_achieved": 93569594900033.25,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.069531510293,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56959490003325,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 750,
      "loss": 4.781023979187012,
      "time_ms": 7710.626840591431,
      "mfu": {
        "mfu": 0.30105047769042276,
        "mfu_percent": 30.105047769042276,
        "flops_achieved": 93325648084031.05,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10624.298347411202,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.32564808403104,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 760,
      "loss": 4.516653537750244,
      "time_ms": 7710.429906845093,
      "mfu": {
        "mfu": 0.3010581668853346,
        "mfu_percent": 30.10581668853346,
        "flops_achieved": 93328031734453.73,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10624.569704897238,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.32803173445373,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 770,
      "loss": 4.575562953948975,
      "time_ms": 7660.866260528564,
      "mfu": {
        "mfu": 0.30300592840430124,
        "mfu_percent": 30.300592840430124,
        "flops_achieved": 93931837805333.39,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10693.30767749859,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.93183780533339,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 780,
      "loss": 4.538966655731201,
      "time_ms": 7695.1398849487305,
      "mfu": {
        "mfu": 0.3016563608145651,
        "mfu_percent": 30.16563608145651,
        "flops_achieved": 93513471852515.17,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10645.680419693344,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.51347185251517,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 790,
      "loss": 4.626158237457275,
      "time_ms": 7699.494123458862,
      "mfu": {
        "mfu": 0.30148576730257276,
        "mfu_percent": 30.148576730257275,
        "flops_achieved": 93460587863797.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10639.660046028957,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.46058786379756,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 800,
      "loss": 4.451152801513672,
      "time_ms": 7678.2066822052,
      "mfu": {
        "mfu": 0.3023216214057376,
        "mfu_percent": 30.23216214057376,
        "flops_achieved": 93719702635778.66,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10669.157967557128,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.71970263577866,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 810,
      "loss": 4.581548690795898,
      "time_ms": 7691.504716873169,
      "mfu": {
        "mfu": 0.3017989300013482,
        "mfu_percent": 30.179893000134822,
        "flops_achieved": 93557668300417.95,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10650.711793790979,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.55766830041796,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 820,
      "loss": 4.580964088439941,
      "time_ms": 7675.718307495117,
      "mfu": {
        "mfu": 0.30241963040592235,
        "mfu_percent": 30.241963040592236,
        "flops_achieved": 93750085425835.92,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10672.616779071672,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.75008542583592,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 830,
      "loss": 4.130216598510742,
      "time_ms": 7705.010175704956,
      "mfu": {
        "mfu": 0.30126993225421184,
        "mfu_percent": 30.126993225421185,
        "flops_achieved": 93393678998805.67,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10632.043064434354,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.39367899880567,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 840,
      "loss": 4.524116039276123,
      "time_ms": 7712.206840515137,
      "mfu": {
        "mfu": 0.30098880147483115,
        "mfu_percent": 30.098880147483115,
        "flops_achieved": 93306528457197.66,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10622.121747259589,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.30652845719766,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 850,
      "loss": 4.566056728363037,
      "time_ms": 7698.160409927368,
      "mfu": {
        "mfu": 0.3015379999953192,
        "mfu_percent": 30.153799999531923,
        "flops_achieved": 93476779998548.95,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10641.50337713902,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.47677999854895,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 860,
      "loss": 4.376221656799316,
      "time_ms": 7697.767734527588,
      "mfu": {
        "mfu": 0.3015533819292487,
        "mfu_percent": 30.15533819292487,
        "flops_achieved": 93481548398067.1,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10642.046217185252,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.48154839806709,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 870,
      "loss": 4.102181911468506,
      "time_ms": 7700.927257537842,
      "mfu": {
        "mfu": 0.3014296611334065,
        "mfu_percent": 30.142966113340652,
        "flops_achieved": 93443194951356.02,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.680017015464,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44319495135602,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 880,
      "loss": 3.6732444763183594,
      "time_ms": 7688.79508972168,
      "mfu": {
        "mfu": 0.30190528770310504,
        "mfu_percent": 30.190528770310504,
        "flops_achieved": 93590639187962.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10654.465237278857,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.59063918796257,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 890,
      "loss": 4.375781536102295,
      "time_ms": 7688.96484375,
      "mfu": {
        "mfu": 0.3018986223535034,
        "mfu_percent": 30.18986223535034,
        "flops_achieved": 93588572929586.05,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10654.230012065791,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.58857292958605,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 900,
      "loss": 3.9580345153808594,
      "time_ms": 7674.667119979858,
      "mfu": {
        "mfu": 0.3024610523640193,
        "mfu_percent": 30.24610523640193,
        "flops_achieved": 93762926232845.98,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10674.078591204747,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.76292623284598,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 910,
      "loss": 4.2833170890808105,
      "time_ms": 7687.176704406738,
      "mfu": {
        "mfu": 0.3019688479805528,
        "mfu_percent": 30.19688479805528,
        "flops_achieved": 93610342873971.36,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10656.70832739394,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.61034287397136,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 920,
      "loss": 3.837420701980591,
      "time_ms": 7704.107046127319,
      "mfu": {
        "mfu": 0.3013052492332001,
        "mfu_percent": 30.130524923320014,
        "flops_achieved": 93404627262292.03,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10633.289427251577,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.40462726229202,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 930,
      "loss": 4.573126316070557,
      "time_ms": 7711.195707321167,
      "mfu": {
        "mfu": 0.30102826873512845,
        "mfu_percent": 30.102826873512846,
        "flops_achieved": 93318763307889.81,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10623.514576633488,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.31876330788981,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 940,
      "loss": 4.105754852294922,
      "time_ms": 7703.092813491821,
      "mfu": {
        "mfu": 0.30134492078129366,
        "mfu_percent": 30.134492078129366,
        "flops_achieved": 93416925442201.03,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10634.689466095835,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.41692544220103,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 950,
      "loss": 4.17803430557251,
      "time_ms": 7716.304063796997,
      "mfu": {
        "mfu": 0.3008289816550332,
        "mfu_percent": 30.08289816550332,
        "flops_achieved": 93256984313060.3,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10616.481585316022,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.2569843130603,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 960,
      "loss": 4.403513431549072,
      "time_ms": 7688.9708042144775,
      "mfu": {
        "mfu": 0.3018983883227,
        "mfu_percent": 30.18983883227,
        "flops_achieved": 93588500380037.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10654.221752942281,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.588500380037,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 970,
      "loss": 4.147204399108887,
      "time_ms": 7671.143054962158,
      "mfu": {
        "mfu": 0.3026000006806151,
        "mfu_percent": 30.260000068061508,
        "flops_achieved": 93806000210990.69,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10678.982182063362,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80600021099069,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 980,
      "loss": 4.777564525604248,
      "time_ms": 7684.885501861572,
      "mfu": {
        "mfu": 0.30205887818241933,
        "mfu_percent": 30.205887818241933,
        "flops_achieved": 93638252236550.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10659.885561099883,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.63825223655,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 990,
      "loss": 4.236446380615234,
      "time_ms": 7694.456577301025,
      "mfu": {
        "mfu": 0.3016831494638027,
        "mfu_percent": 30.16831494638027,
        "flops_achieved": 93521776333778.84,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10646.625811323374,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.52177633377885,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 36.435918848,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 36.435918848
      }
    },
    {
      "iter": 1000,
      "loss": 4.153626918792725,
      "time_ms": 272147.24683761597,
      "mfu": {
        "mfu": 0.008529529218561982,
        "mfu_percent": 0.8529529218561982,
        "flops_achieved": 2644154057754.2144,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 301.01351732167177,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 2.644154057754214,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1010,
      "loss": 4.185768127441406,
      "time_ms": 7663.962364196777,
      "mfu": {
        "mfu": 0.30288351943073877,
        "mfu_percent": 30.288351943073877,
        "flops_achieved": 93893891023529.02,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10688.987772526156,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.89389102352902,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1020,
      "loss": 4.225730895996094,
      "time_ms": 7687.463760375977,
      "mfu": {
        "mfu": 0.30195757222524017,
        "mfu_percent": 30.195757222524016,
        "flops_achieved": 93606847389824.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10656.310397487126,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.60684738982445,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1030,
      "loss": 4.0382256507873535,
      "time_ms": 7698.008060455322,
      "mfu": {
        "mfu": 0.30154396766315483,
        "mfu_percent": 30.154396766315482,
        "flops_achieved": 93478629975578.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10641.713980636518,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.478629975578,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1040,
      "loss": 4.183394908905029,
      "time_ms": 7690.002202987671,
      "mfu": {
        "mfu": 0.30185789709537314,
        "mfu_percent": 30.185789709537314,
        "flops_achieved": 93575948099565.67,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.792786999848,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.57594809956568,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1050,
      "loss": 4.007138729095459,
      "time_ms": 7677.961826324463,
      "mfu": {
        "mfu": 0.30233126266582067,
        "mfu_percent": 30.233126266582065,
        "flops_achieved": 93722691426404.4,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10669.498214894895,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.72269142640441,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1060,
      "loss": 4.18347692489624,
      "time_ms": 7680.188179016113,
      "mfu": {
        "mfu": 0.3022436221022411,
        "mfu_percent": 30.224362210224108,
        "flops_achieved": 93695522851694.73,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10666.40531332587,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.69552285169473,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1070,
      "loss": 4.256051540374756,
      "time_ms": 7704.937934875488,
      "mfu": {
        "mfu": 0.3012727569349534,
        "mfu_percent": 30.127275693495342,
        "flops_achieved": 93394554649835.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10632.14274954751,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.39455464983556,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1080,
      "loss": 3.485028028488159,
      "time_ms": 7662.354946136475,
      "mfu": {
        "mfu": 0.30294705870067906,
        "mfu_percent": 30.294705870067908,
        "flops_achieved": 93913588197210.52,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10691.230121270464,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.91358819721052,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1090,
      "loss": 3.9246697425842285,
      "time_ms": 7680.919170379639,
      "mfu": {
        "mfu": 0.3022148576441682,
        "mfu_percent": 30.221485764416823,
        "flops_achieved": 93686605869692.14,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.3901939123,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.68660586969214,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1100,
      "loss": 4.185784339904785,
      "time_ms": 7707.368850708008,
      "mfu": {
        "mfu": 0.3011777350501928,
        "mfu_percent": 30.11777350501928,
        "flops_achieved": 93365097865559.77,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10628.789355588546,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.36509786555976,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1110,
      "loss": 4.030068874359131,
      "time_ms": 7707.284688949585,
      "mfu": {
        "mfu": 0.3011810238410968,
        "mfu_percent": 30.11810238410968,
        "flops_achieved": 93366117390740.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10628.905419499271,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.36611739074,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1120,
      "loss": 4.043554306030273,
      "time_ms": 7676.879167556763,
      "mfu": {
        "mfu": 0.30237389999084957,
        "mfu_percent": 30.237389999084957,
        "flops_achieved": 93735908997163.38,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10671.002918243377,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.73590899716338,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1130,
      "loss": 4.450652599334717,
      "time_ms": 7650.49147605896,
      "mfu": {
        "mfu": 0.30341683288148996,
        "mfu_percent": 30.341683288148996,
        "flops_achieved": 94059218193261.89,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10707.808806317356,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 94.05921819326188,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1140,
      "loss": 3.6866042613983154,
      "time_ms": 7671.7798709869385,
      "mfu": {
        "mfu": 0.3025748825811946,
        "mfu_percent": 30.25748825811946,
        "flops_achieved": 93798213600170.33,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10678.09574539596,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.79821360017033,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1150,
      "loss": 4.1294450759887695,
      "time_ms": 7691.648244857788,
      "mfu": {
        "mfu": 0.30179329836157426,
        "mfu_percent": 30.179329836157425,
        "flops_achieved": 93555922492088.02,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10650.513048977142,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.55592249208802,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1160,
      "loss": 3.990788221359253,
      "time_ms": 7684.332609176636,
      "mfu": {
        "mfu": 0.30208061151342686,
        "mfu_percent": 30.208061151342687,
        "flops_achieved": 93644989569162.33,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10660.652546737902,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.64498956916233,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1170,
      "loss": 3.9340438842773438,
      "time_ms": 7695.496082305908,
      "mfu": {
        "mfu": 0.3016423982061317,
        "mfu_percent": 30.16423982061317,
        "flops_achieved": 93509143443900.83,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10645.187668714032,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50914344390083,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1180,
      "loss": 4.121340274810791,
      "time_ms": 7671.677589416504,
      "mfu": {
        "mfu": 0.3025789166185748,
        "mfu_percent": 30.257891661857478,
        "flops_achieved": 93799464151758.19,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10678.238109616741,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.79946415175819,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1190,
      "loss": 3.8546974658966064,
      "time_ms": 7674.247741699219,
      "mfu": {
        "mfu": 0.30247758109756695,
        "mfu_percent": 30.247758109756695,
        "flops_achieved": 93768050140245.75,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10674.66190267418,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.76805014024575,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1200,
      "loss": 4.413578033447266,
      "time_ms": 7689.551115036011,
      "mfu": {
        "mfu": 0.30187560482089004,
        "mfu_percent": 30.187560482089005,
        "flops_achieved": 93581437494475.9,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10653.417705984826,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.5814374944759,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1210,
      "loss": 3.8864142894744873,
      "time_ms": 7687.870979309082,
      "mfu": {
        "mfu": 0.3019415778308577,
        "mfu_percent": 30.194157783085767,
        "flops_achieved": 93601889127565.88,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10655.74594324972,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.60188912756587,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1220,
      "loss": 3.8522164821624756,
      "time_ms": 7674.31902885437,
      "mfu": {
        "mfu": 0.30247477136732603,
        "mfu_percent": 30.247477136732602,
        "flops_achieved": 93767179123871.08,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10674.562745175463,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.76717912387107,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1230,
      "loss": 3.7877004146575928,
      "time_ms": 7661.830186843872,
      "mfu": {
        "mfu": 0.30296780756620373,
        "mfu_percent": 30.29678075662037,
        "flops_achieved": 93920020345523.16,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10691.962364379313,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.92002034552316,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1240,
      "loss": 3.717486619949341,
      "time_ms": 7672.8105545043945,
      "mfu": {
        "mfu": 0.30253423789929385,
        "mfu_percent": 30.253423789929386,
        "flops_achieved": 93785613748781.1,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10676.661363925909,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.78561374878109,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1250,
      "loss": 3.8740203380584717,
      "time_ms": 7691.380023956299,
      "mfu": {
        "mfu": 0.3018038227759573,
        "mfu_percent": 30.18038227759573,
        "flops_achieved": 93559185060546.77,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10650.884463496048,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.55918506054677,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1260,
      "loss": 3.983158588409424,
      "time_ms": 7696.6845989227295,
      "mfu": {
        "mfu": 0.3015958187993757,
        "mfu_percent": 30.15958187993757,
        "flops_achieved": 93494703827806.47,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10643.543846329103,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.49470382780648,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1270,
      "loss": 3.7931134700775146,
      "time_ms": 7672.893762588501,
      "mfu": {
        "mfu": 0.30253095709089334,
        "mfu_percent": 30.253095709089333,
        "flops_achieved": 93784596698176.94,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10676.545581723753,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.78459669817694,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1280,
      "loss": 3.7927749156951904,
      "time_ms": 7681.020975112915,
      "mfu": {
        "mfu": 0.30221085206950904,
        "mfu_percent": 30.221085206950903,
        "flops_achieved": 93685364141547.8,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.248834162405,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.6853641415478,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1290,
      "loss": 3.4508092403411865,
      "time_ms": 7683.464527130127,
      "mfu": {
        "mfu": 0.30211474074699424,
        "mfu_percent": 30.211474074699424,
        "flops_achieved": 93655569631568.22,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10661.85699312367,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.65556963156821,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1300,
      "loss": 3.302994966506958,
      "time_ms": 7702.275276184082,
      "mfu": {
        "mfu": 0.3013769062279834,
        "mfu_percent": 30.137690622798342,
        "flops_achieved": 93426840930674.86,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10635.818256626295,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.42684093067486,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1310,
      "loss": 3.831713914871216,
      "time_ms": 7705.7785987854,
      "mfu": {
        "mfu": 0.30123988950558883,
        "mfu_percent": 30.123988950558882,
        "flops_achieved": 93384365746732.53,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10630.982833183449,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.38436574673253,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1320,
      "loss": 3.5389862060546875,
      "time_ms": 7705.606698989868,
      "mfu": {
        "mfu": 0.30124660968706646,
        "mfu_percent": 30.124660968706646,
        "flops_achieved": 93386449002990.6,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10631.219993454757,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.38644900299059,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1330,
      "loss": 3.723125457763672,
      "time_ms": 7709.244966506958,
      "mfu": {
        "mfu": 0.3011044406731851,
        "mfu_percent": 30.11044406731851,
        "flops_achieved": 93342376608687.38,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10626.202741760555,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.34237660868737,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1340,
      "loss": 3.934626817703247,
      "time_ms": 7682.255744934082,
      "mfu": {
        "mfu": 0.30216227768561005,
        "mfu_percent": 30.216227768561005,
        "flops_achieved": 93670306082539.11,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10663.534607529904,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.67030608253911,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1350,
      "loss": 3.707345724105835,
      "time_ms": 7671.879529953003,
      "mfu": {
        "mfu": 0.30257095208413226,
        "mfu_percent": 30.257095208413226,
        "flops_achieved": 93796995146081.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10677.957035191066,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.796995146081,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1360,
      "loss": 3.666954755783081,
      "time_ms": 7695.245027542114,
      "mfu": {
        "mfu": 0.301652239187252,
        "mfu_percent": 30.165223918725196,
        "flops_achieved": 93512194148048.11,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10645.53496435779,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.51219414804811,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1370,
      "loss": 3.8456437587738037,
      "time_ms": 7664.337158203125,
      "mfu": {
        "mfu": 0.3028687081136789,
        "mfu_percent": 30.286870811367887,
        "flops_achieved": 93889299515240.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10688.465069979495,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.88929951524045,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1380,
      "loss": 2.816383123397827,
      "time_ms": 7671.605110168457,
      "mfu": {
        "mfu": 0.3025817753022579,
        "mfu_percent": 30.258177530225787,
        "flops_achieved": 93800350343699.94,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10678.338994719341,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80035034369993,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1390,
      "loss": 4.131997585296631,
      "time_ms": 7668.258905410767,
      "mfu": {
        "mfu": 0.30271381317273094,
        "mfu_percent": 30.271381317273093,
        "flops_achieved": 93841282083546.6,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10682.998710723863,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.84128208354659,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1400,
      "loss": 3.2957749366760254,
      "time_ms": 7674.450874328613,
      "mfu": {
        "mfu": 0.302469574913491,
        "mfu_percent": 30.246957491349104,
        "flops_achieved": 93765568223182.22,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10674.37935840154,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.76556822318221,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1410,
      "loss": 3.946143388748169,
      "time_ms": 7705.523729324341,
      "mfu": {
        "mfu": 0.30124985337709,
        "mfu_percent": 30.124985337709,
        "flops_achieved": 93387454546897.89,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10631.334465721406,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.38745454689789,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1420,
      "loss": 3.6761815547943115,
      "time_ms": 7699.103593826294,
      "mfu": {
        "mfu": 0.30150105987845444,
        "mfu_percent": 30.150105987845443,
        "flops_achieved": 93465328562320.88,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10640.199732562303,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.46532856232088,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1430,
      "loss": 3.580839157104492,
      "time_ms": 7690.931558609009,
      "mfu": {
        "mfu": 0.3018214212365811,
        "mfu_percent": 30.182142123658114,
        "flops_achieved": 93564640583340.16,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10651.505526440564,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56464058334015,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1440,
      "loss": 3.299915313720703,
      "time_ms": 7707.2789669036865,
      "mfu": {
        "mfu": 0.30118124744421915,
        "mfu_percent": 30.118124744421916,
        "flops_achieved": 93366186707707.94,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10628.913310622056,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.36618670770794,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1450,
      "loss": 3.699554443359375,
      "time_ms": 7676.682472229004,
      "mfu": {
        "mfu": 0.3023816475476333,
        "mfu_percent": 30.238164754763332,
        "flops_achieved": 93738310739766.33,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10671.276335363873,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.73831073976633,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1460,
      "loss": 4.1462016105651855,
      "time_ms": 7677.522420883179,
      "mfu": {
        "mfu": 0.302348565904366,
        "mfu_percent": 30.2348565904366,
        "flops_achieved": 93728055430353.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10670.108859229666,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.72805543035345,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1470,
      "loss": 3.1476967334747314,
      "time_ms": 7669.388055801392,
      "mfu": {
        "mfu": 0.3026692451553214,
        "mfu_percent": 30.26692451553214,
        "flops_achieved": 93827465998149.64,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10681.425871785541,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.82746599814963,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1480,
      "loss": 3.6322524547576904,
      "time_ms": 7698.598861694336,
      "mfu": {
        "mfu": 0.3015208267575286,
        "mfu_percent": 30.15208267575286,
        "flops_achieved": 93471456294833.88,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10640.897320628905,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.47145629483387,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1490,
      "loss": 3.6209380626678467,
      "time_ms": 7672.454833984375,
      "mfu": {
        "mfu": 0.3025482643926076,
        "mfu_percent": 30.25482643926076,
        "flops_achieved": 93789961961708.36,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10677.15636945082,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.78996196170836,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1500,
      "loss": 3.286681890487671,
      "time_ms": 7684.234142303467,
      "mfu": {
        "mfu": 0.3020844824175027,
        "mfu_percent": 30.20844824175027,
        "flops_achieved": 93646189549425.83,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10660.789153861366,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.64618954942583,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1510,
      "loss": 3.3072280883789062,
      "time_ms": 7662.520408630371,
      "mfu": {
        "mfu": 0.3029405169398513,
        "mfu_percent": 30.29405169398513,
        "flops_achieved": 93911560251353.9,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10690.999257598414,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.9115602513539,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1520,
      "loss": 4.122924327850342,
      "time_ms": 7691.20192527771,
      "mfu": {
        "mfu": 0.3018108113926328,
        "mfu_percent": 30.181081139263284,
        "flops_achieved": 93561351531716.17,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10651.131097047888,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.56135153171617,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1530,
      "loss": 3.6077656745910645,
      "time_ms": 7672.680377960205,
      "mfu": {
        "mfu": 0.30253937076807613,
        "mfu_percent": 30.253937076807613,
        "flops_achieved": 93787204938103.61,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10676.84250673538,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.78720493810361,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1540,
      "loss": 3.7038657665252686,
      "time_ms": 7695.814609527588,
      "mfu": {
        "mfu": 0.30162991332702316,
        "mfu_percent": 30.162991332702315,
        "flops_achieved": 93505273131377.19,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.747067916791,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.5052731313772,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1550,
      "loss": 3.465890645980835,
      "time_ms": 7673.6931800842285,
      "mfu": {
        "mfu": 0.30249944051413924,
        "mfu_percent": 30.249944051413923,
        "flops_achieved": 93774826559383.17,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10675.433337966846,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.77482655938317,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1560,
      "loss": 4.082411289215088,
      "time_ms": 7680.1629066467285,
      "mfu": {
        "mfu": 0.30224461666610053,
        "mfu_percent": 30.224461666610054,
        "flops_achieved": 93695831166491.17,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10666.440412234364,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.69583116649117,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1570,
      "loss": 3.660841464996338,
      "time_ms": 7707.176923751831,
      "mfu": {
        "mfu": 0.3011852350889914,
        "mfu_percent": 30.11852350889914,
        "flops_achieved": 93367422877587.34,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10629.054037612723,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.36742287758734,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1580,
      "loss": 3.6603481769561768,
      "time_ms": 7671.091079711914,
      "mfu": {
        "mfu": 0.30260205093795084,
        "mfu_percent": 30.260205093795083,
        "flops_achieved": 93806635790764.77,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10679.05453719062,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80663579076476,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1590,
      "loss": 3.202685594558716,
      "time_ms": 7700.212717056274,
      "mfu": {
        "mfu": 0.3014576322691581,
        "mfu_percent": 30.14576322691581,
        "flops_achieved": 93451866003439.02,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10638.667139486157,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.45186600343902,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1600,
      "loss": 3.722188711166382,
      "time_ms": 7660.991907119751,
      "mfu": {
        "mfu": 0.3030009588569535,
        "mfu_percent": 30.30009588569535,
        "flops_achieved": 93930297245655.58,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10693.132298425686,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.93029724565558,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1610,
      "loss": 3.681185245513916,
      "time_ms": 7712.245225906372,
      "mfu": {
        "mfu": 0.30098730339320073,
        "mfu_percent": 30.098730339320074,
        "flops_achieved": 93306064051892.23,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10622.06887882931,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.30606405189224,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1620,
      "loss": 3.688342809677124,
      "time_ms": 7679.812908172607,
      "mfu": {
        "mfu": 0.3022583911103363,
        "mfu_percent": 30.225839111033633,
        "flops_achieved": 93700101244204.25,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10666.926522757267,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.70010124420425,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1630,
      "loss": 3.345600128173828,
      "time_ms": 7682.178258895874,
      "mfu": {
        "mfu": 0.3021653254354805,
        "mfu_percent": 30.216532543548052,
        "flops_achieved": 93671250884998.97,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10663.642164920813,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.67125088499897,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1640,
      "loss": 3.4263360500335693,
      "time_ms": 7687.7336502075195,
      "mfu": {
        "mfu": 0.30194697153562094,
        "mfu_percent": 30.194697153562096,
        "flops_achieved": 93603561176042.48,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10655.936291157626,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.60356117604249,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1650,
      "loss": 3.869126081466675,
      "time_ms": 7676.055192947388,
      "mfu": {
        "mfu": 0.302406357862747,
        "mfu_percent": 30.2406357862747,
        "flops_achieved": 93745970937451.56,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10672.148381015619,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.74597093745156,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1660,
      "loss": 3.520098924636841,
      "time_ms": 7680.902719497681,
      "mfu": {
        "mfu": 0.30221550492498017,
        "mfu_percent": 30.221550492498018,
        "flops_achieved": 93686806526743.86,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.4130369402,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.68680652674387,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1670,
      "loss": 3.7701218128204346,
      "time_ms": 7688.655138015747,
      "mfu": {
        "mfu": 0.3019107830932982,
        "mfu_percent": 30.19107830932982,
        "flops_achieved": 93592342758922.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10654.659173742255,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.59234275892246,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1680,
      "loss": 3.5692596435546875,
      "time_ms": 7706.801414489746,
      "mfu": {
        "mfu": 0.30119991015836156,
        "mfu_percent": 30.119991015836156,
        "flops_achieved": 93371972149092.08,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10629.57193187568,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.37197214909207,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1690,
      "loss": 3.390911817550659,
      "time_ms": 7686.357021331787,
      "mfu": {
        "mfu": 0.30200105032987967,
        "mfu_percent": 30.200105032987967,
        "flops_achieved": 93620325602262.7,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10657.844772582008,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.62032560226271,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1700,
      "loss": 3.037954330444336,
      "time_ms": 7720.651626586914,
      "mfu": {
        "mfu": 0.30065958236724927,
        "mfu_percent": 30.065958236724928,
        "flops_achieved": 93204470533847.27,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10610.50335672438,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.20447053384727,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1710,
      "loss": 3.556891679763794,
      "time_ms": 7689.5012855529785,
      "mfu": {
        "mfu": 0.3018775610342737,
        "mfu_percent": 30.187756103427372,
        "flops_achieved": 93582043920624.84,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10653.486742230105,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.58204392062484,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1720,
      "loss": 3.2817254066467285,
      "time_ms": 7700.76847076416,
      "mfu": {
        "mfu": 0.3014358764927651,
        "mfu_percent": 30.14358764927651,
        "flops_achieved": 93445121712757.19,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.899361733562,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44512171275719,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1730,
      "loss": 3.244964599609375,
      "time_ms": 7700.81901550293,
      "mfu": {
        "mfu": 0.30143389800221726,
        "mfu_percent": 30.143389800221726,
        "flops_achieved": 93444508380687.34,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.829539310362,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44450838068734,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1740,
      "loss": 3.7066311836242676,
      "time_ms": 7696.418285369873,
      "mfu": {
        "mfu": 0.3016062546996936,
        "mfu_percent": 30.16062546996936,
        "flops_achieved": 93497938956905.02,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10643.91213711991,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.49793895690502,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1750,
      "loss": 2.9624416828155518,
      "time_ms": 7685.244798660278,
      "mfu": {
        "mfu": 0.3020447564737692,
        "mfu_percent": 30.20447564737692,
        "flops_achieved": 93633874506868.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10659.38719535396,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.63387450686845,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1760,
      "loss": 3.4155449867248535,
      "time_ms": 7705.864667892456,
      "mfu": {
        "mfu": 0.3012365248671716,
        "mfu_percent": 30.123652486717162,
        "flops_achieved": 93383322708823.2,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10630.864092556794,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.3833227088232,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1770,
      "loss": 3.5670621395111084,
      "time_ms": 7694.1399574279785,
      "mfu": {
        "mfu": 0.301695563961201,
        "mfu_percent": 30.1695563961201,
        "flops_achieved": 93525624827972.31,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10647.063928296993,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.52562482797231,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1780,
      "loss": 3.698463201522827,
      "time_ms": 7676.172494888306,
      "mfu": {
        "mfu": 0.3024017366986516,
        "mfu_percent": 30.24017366986516,
        "flops_achieved": 93744538376582.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10671.985296650371,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.744538376582,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1790,
      "loss": 3.924459934234619,
      "time_ms": 7693.426609039307,
      "mfu": {
        "mfu": 0.3017235377179362,
        "mfu_percent": 30.17235377179362,
        "flops_achieved": 93534296692560.22,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10648.051143264172,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.53429669256022,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1800,
      "loss": 3.3855769634246826,
      "time_ms": 7700.788259506226,
      "mfu": {
        "mfu": 0.30143510189195694,
        "mfu_percent": 30.143510189195695,
        "flops_achieved": 93444881586506.66,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10637.872025487,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.44488158650665,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1810,
      "loss": 3.5859615802764893,
      "time_ms": 7696.022748947144,
      "mfu": {
        "mfu": 0.3016217557270357,
        "mfu_percent": 30.16217557270357,
        "flops_achieved": 93502744275381.06,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.459180062466,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50274427538106,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1820,
      "loss": 3.22021222114563,
      "time_ms": 7689.871072769165,
      "mfu": {
        "mfu": 0.3018630444758207,
        "mfu_percent": 30.186304447582067,
        "flops_achieved": 93577543787504.4,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.974441936925,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.57754378750441,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1830,
      "loss": 3.3805038928985596,
      "time_ms": 7694.41819190979,
      "mfu": {
        "mfu": 0.30168465448022275,
        "mfu_percent": 30.168465448022275,
        "flops_achieved": 93522242888869.05,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10646.678924487607,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.52224288886904,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1840,
      "loss": 3.256986379623413,
      "time_ms": 7673.0897426605225,
      "mfu": {
        "mfu": 0.30252323008120785,
        "mfu_percent": 30.252323008120786,
        "flops_achieved": 93782201325174.44,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10676.272889725846,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.78220132517444,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1850,
      "loss": 3.6177964210510254,
      "time_ms": 7696.24662399292,
      "mfu": {
        "mfu": 0.3016129818938064,
        "mfu_percent": 30.16129818938064,
        "flops_achieved": 93500024387079.98,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10644.149544872402,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.50002438707999,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1860,
      "loss": 2.7276341915130615,
      "time_ms": 7659.046411514282,
      "mfu": {
        "mfu": 0.3030779249702575,
        "mfu_percent": 30.307792497025748,
        "flops_achieved": 93954156740779.81,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10695.848490596034,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.95415674077981,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1870,
      "loss": 3.887815475463867,
      "time_ms": 7680.076122283936,
      "mfu": {
        "mfu": 0.30224803201069445,
        "mfu_percent": 30.224803201069445,
        "flops_achieved": 93696889923315.28,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10666.560942320226,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.69688992331528,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1880,
      "loss": 3.7167203426361084,
      "time_ms": 7687.988519668579,
      "mfu": {
        "mfu": 0.3019369614970124,
        "mfu_percent": 30.19369614970124,
        "flops_achieved": 93600458064073.84,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10655.583029347641,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.60045806407385,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1890,
      "loss": 3.519097089767456,
      "time_ms": 7705.327987670898,
      "mfu": {
        "mfu": 0.30125750615248037,
        "mfu_percent": 30.125750615248037,
        "flops_achieved": 93389826907268.92,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10631.604537935067,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.38982690726893,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1900,
      "loss": 3.136730909347534,
      "time_ms": 7689.149856567383,
      "mfu": {
        "mfu": 0.30189135820652646,
        "mfu_percent": 30.189135820652645,
        "flops_achieved": 93586321044023.2,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10653.973654841864,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.58632104402321,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1910,
      "loss": 3.636345624923706,
      "time_ms": 7706.217527389526,
      "mfu": {
        "mfu": 0.3012227315673736,
        "mfu_percent": 30.12227315673736,
        "flops_achieved": 93379046785885.83,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10630.377316606882,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.37904678588583,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1920,
      "loss": 4.002242565155029,
      "time_ms": 7705.09934425354,
      "mfu": {
        "mfu": 0.3012664457576216,
        "mfu_percent": 30.126644575762164,
        "flops_achieved": 93392598184862.7,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10631.920023340894,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.3925981848627,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1930,
      "loss": 3.548698902130127,
      "time_ms": 7661.211729049683,
      "mfu": {
        "mfu": 0.30299226489862124,
        "mfu_percent": 30.299226489862125,
        "flops_achieved": 93927602118572.58,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10692.825482080963,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.92760211857258,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1940,
      "loss": 3.7160568237304688,
      "time_ms": 7693.422317504883,
      "mfu": {
        "mfu": 0.30172370602495163,
        "mfu_percent": 30.172370602495164,
        "flops_achieved": 93534348867735.0,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10648.05708294565,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.534348867735,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1950,
      "loss": 3.578688144683838,
      "time_ms": 7671.3831424713135,
      "mfu": {
        "mfu": 0.30259053035706535,
        "mfu_percent": 30.259053035706536,
        "flops_achieved": 93803064410690.25,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10678.647967204739,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.80306441069025,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1960,
      "loss": 3.6916253566741943,
      "time_ms": 7690.450668334961,
      "mfu": {
        "mfu": 0.3018402943809821,
        "mfu_percent": 30.184029438098207,
        "flops_achieved": 93570491258104.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10652.17157393668,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.57049125810445,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1970,
      "loss": 3.6487202644348145,
      "time_ms": 7677.16121673584,
      "mfu": {
        "mfu": 0.30236279115675596,
        "mfu_percent": 30.236279115675597,
        "flops_achieved": 93732465258594.34,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10670.610879112239,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.73246525859435,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1980,
      "loss": 3.362874746322632,
      "time_ms": 7673.752784729004,
      "mfu": {
        "mfu": 0.30249709089822086,
        "mfu_percent": 30.249709089822087,
        "flops_achieved": 93774098178448.47,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10675.350418249494,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.77409817844847,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 1990,
      "loss": 3.3847153186798096,
      "time_ms": 7680.808305740356,
      "mfu": {
        "mfu": 0.3022192198076079,
        "mfu_percent": 30.22192198076079,
        "flops_achieved": 93687958140358.45,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 10665.544137949124,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 93.68795814035845,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    },
    {
      "iter": 2000,
      "loss": 3.399492025375366,
      "time_ms": 227364.00318145752,
      "mfu": {
        "mfu": 0.010209566427276717,
        "mfu_percent": 1.0209566427276717,
        "flops_achieved": 3164965592455.782,
        "flops_per_token": 8784170496.0,
        "tokens_per_sec": 360.3032971521893,
        "hardware_peak_flops": 310000000000000.0,
        "hardware_peak_tflops": 310.0,
        "achieved_tflops": 3.1649655924557822,
        "gpu_name": "A6000",
        "precision": "bf16",
        "num_gpus": 2.0,
        "model_params_billion": 1.294159104,
        "non_attn_gflops": 7.764954624,
        "attn_gflops": 1.019215872,
        "attention_flops_per_layer": 434865438720.0,
        "ffn_flops_per_layer": 173946175488.0,
        "attention_to_ffn_ratio": 2.5,
        "architecture": "18L-18H-2304D-RoPE-RMS-SwiGLU-PreNorm"
      },
      "memory": {
        "allocated_gb": 8.448795136,
        "reserved_gb": 39.025901568,
        "max_allocated_gb": 31.881979904,
        "max_reserved_gb": 39.025901568
      }
    }
  ],
  "eval_steps": [
    {
      "iter": 1000,
      "train_loss": 4.093095779418945,
      "val_loss": 4.120126724243164,
      "timestamp": "2025-11-10T19:50:06.502197",
      "lr": 0.00015007496251874061
    },
    {
      "iter": 2000,
      "train_loss": 3.3941164016723633,
      "val_loss": 3.3995847702026367,
      "timestamp": "2025-11-10T22:02:19.506002",
      "lr": 0.0003
    }
  ],
  "checkpoints": [
    {
      "iter": 1000,
      "val_loss": 4.120126724243164,
      "path": "out-llama-1.36b/ckpt.pt",
      "timestamp": "2025-11-10T19:53:44.786307"
    },
    {
      "iter": 2000,
      "val_loss": 3.3995847702026367,
      "path": "out-llama-1.36b/ckpt.pt",
      "timestamp": "2025-11-10T22:05:30.106695"
    }
  ],
  "metadata": {
    "world_size": 2,
    "device": "cuda:0",
    "dtype": "bfloat16",
    "compile": true,
    "use_zero1": true,
    "use_fsdp": false
  },
  "end_time": "2025-11-10T22:05:37.564510",
  "summary": {
    "total_iterations": 201,
    "final_iter": 2000,
    "final_train_loss": 3.399492025375366,
    "best_val_loss": 3.3995847702026367,
    "avg_time_ms": 10236.142702956698,
    "avg_mfu": 29.898956533628052,
    "total_eval_steps": 2,
    "total_checkpoints": 2
  }
}