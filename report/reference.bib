@article{dao2023flashattn2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Greg and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Mike and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@misc{rajbhandari2020zero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.02054}, 
}

@article{zhao2023fsdp,
  title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
  author={Zhao, Michael and Zheng, Le and Zhu, Eddie and Xu, Shibo and Fan, Xingyu and Zheng, Wei and Jain, Ashish and Lian, Chen and others},
  journal={arXiv preprint arXiv:2307.03797},
  year={2023}
}

@article{shazeer2020glu,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{zhang2019rmsnorm,
  title={Root Mean Square Layer Normalization},
  author={Zhang, Biao and Sennrich, Rico},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{su2021rope,
  title={Roformer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Miao, Bo and et al.},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{tillet2019triton,
  title={Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  author={Tillet, Philippe and Kung, Jeremy and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  year={2019}
}

@inproceedings{shoeybi2019megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{rasley2020deepspeed,
  title={DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Rasley, Jeff and et al.},
  journal={arXiv preprint arXiv:2007.03039},
  year={2020}
}

@article{micikevicius2018mixed,
  title={Mixed Precision Training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Greg and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@misc{nvidia_amp_guide,
  title={NVIDIA Apex and Automatic Mixed Precision (AMP) Documentation},
  author={{NVIDIA Corporation}},
  year={2023},
  howpublished={\url{https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html}}
}

@misc{chen2016training,
      title={Training Deep Nets with Sublinear Memory Cost}, 
      author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
      year={2016},
      eprint={1604.06174},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1604.06174}, 
}

@inproceedings{gruslys2016memory,
  title={Memory-efficient Backpropagation Through Time},
  author={Gruslys, Audrunas and Munos, Remi and Danihelka, Ivo and Lanctot, Marc and Graves, Alex},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2016}
}


@inproceedings{huang2019gpipe,
  title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia and Chen, Dehao and Zhou, Mohammad and Wu, Yonghui},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@article{hoffmann2022chinchilla,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}



@misc{zhao2023pytorchfsdp,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}


@misc{karpathy2022nanogpt,
  author = {Karpathy, Andrej},
  title = {nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{jang2022transformer,
  author = {Jang, Insu},
  title = {Analysis of Transformer Model},
  year = {2022},
  howpublished = {\url{https://insujang.github.io/2022-07-30/analysis-of-transformer-model/}},
}

@misc{epochai2024backward,
  author = {Epoch AI},
  title = {What's the backward-forward FLOP ratio for neural networks?},
  year = {2024},
  howpublished = {\url{https://epoch.ai/blog/backward-forward-FLOP-ratio}},
}

@article{dao2022flashattn,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@manual{pytorch_profiler,
  title        = {torch.profiler â€” PyTorch Profiler},
  author       = {{PyTorch Developers}},
  year         = {2025},
  note         = {Online; accessed YYYY-MM-DD},
  url          = {https://docs.pytorch.org/docs/stable/profiler.html}
}

@manual{nsight_systems,
  title        = {Profiling Deep Learning with Nsight Systems},
  author       = {Horowitz, Daniel},
  year         = {2022},
  note         = {Argonne Leadership Computing Facility Technical Report; accessed YYYY-MM-DD},
  url          = {https://www.alcf.anl.gov/sites/default/files/2024-07/Nsight-Systems-DL-Profiling-2022-06-30.pdf}
}

@inproceedings{distributed_training,
  title        = {Drilling down into performance for distributed training},
  author       = {Srivastava, Ankur and Hugues, Maxime},
  booktitle    = {AWS re:Invent 2024 CMP335},
  year         = {2024},
  note         = {Amazon Web Services, Inc.; slides accessed YYYY-MM-DD},
  url          = {https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/cmp/CMP335_Drilling-down-into-performance-for-distributed-training.pdf}
}
